{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1QaThQfGpU61pgKsVxsFFZmC1PEYWV9LE","timestamp":1691007827931},{"file_id":"1lF7Hn9Ya9o_nnMMpk7pMZhtCRsl1diZ7","timestamp":1690996738552}],"collapsed_sections":["Y5rqbKwYb-3D"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ny18gSkgXn0R"},"source":["**Copyright: Â© NexStream Technical Education, LLC**.\n","All rights reserved"]},{"cell_type":"markdown","metadata":{"id":"NOLfZLgL0y_y"},"source":["# Building a Deep Neural Network with a Custom Framework\n","In this project, you will implement a deep neural network from scratch using standard Python and Numpy libraries.  Please complete the following steps in your Colab Script. The reference script below provides template code and hints to help with each step. You will be turning in code and screenshots of your console output in an accompanying assignment.\n","\n","The following instructions are identified as Steps in the text cells preceding their corresponding code cell. Read through the instructions and write/fill-in the appropriate code in the cells."]},{"cell_type":"markdown","metadata":{"id":"AZo3WZvRGgFd"},"source":["##Part A:  Dataset Creation and Preprocessing\n"]},{"cell_type":"markdown","metadata":{"id":"uDv7kyCwGw7B"},"source":["\n","In this section, you will set up your drive, create a synthetic dataset, and preprocess the data in preparation for deep network algorithms.  Please follow the steps outlined in the following cells and fill in your code where prompted.\n","<br>\n","\n","\n","**Step A-1:**\n","- Mount your Google drive.\n","- Upload the utility files indicated in the code cell below from the materials folder provided with this course and copy them to your project directory.\n","- Import the required libraries"]},{"cell_type":"code","metadata":{"id":"7Ur2QWYlFGQm","executionInfo":{"status":"ok","timestamp":1691007865581,"user_tz":420,"elapsed":14,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["from google.colab import drive\n","#drive.mount('/content/drive', force_remount=True)\n","\n","#See the reference cp commands below.  Update this to your own drive path.\n","#!cp drive/MyDrive/MachineLearning/DNN/tf_image_utils_v2.py .\n","#!cp drive/MyDrive/MachineLearning/DNN/h5py_image_utils.py .\n","#!cp drive/MyDrive/MachineLearning/DNN/train_catvnocat.h5 .\n","#!cp drive/MyDrive/MachineLearning/DNN/test_catvnocat.h5 ."],"execution_count":1,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive/', force_remount=True)\n","# cd (change directory) to the folder which contains the dataset\n","# YOUR CODE HERE...\n","%cd /content/drive/MyDrive/Colab Notebooks/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ygWVw2HIsdp8","executionInfo":{"status":"ok","timestamp":1691007891252,"user_tz":420,"elapsed":23980,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"ef3636e2-9ebd-42bf-a1fb-1e06833f58ab"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","/content/drive/MyDrive/Colab Notebooks\n"]}]},{"cell_type":"code","metadata":{"id":"VbezJ-FyOLW5","executionInfo":{"status":"ok","timestamp":1691007957140,"user_tz":420,"elapsed":10111,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["import numpy as np\n","\n","import h5py\n","import h5py_image_utils\n","from h5py_image_utils import *\n","import matplotlib.pyplot as plt\n","\n","from sklearn.datasets import make_circles\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import make_moons\n","import seaborn as sns\n","from matplotlib import cm\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","from google.colab import files\n","\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import tf_image_utils_v2\n","from tf_image_utils_v2 import *\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F6dDkIvmJkcA"},"source":["**Step A-2:** Create a Synthetic Dataset\n","\n","- Use the following code cells to generate synthetic data and create the train and test datasets.\n","- Print the shapes of the datasets.  Your shapes output should be the following:\n","  - X is of shape:  (10, 5)\n","  - Y is of shape:  (1, 5)\n","- Verify the doctest modules included in the cells run without any errors"]},{"cell_type":"code","metadata":{"id":"U2i7CCeDu_Kp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691007967855,"user_tz":420,"elapsed":222,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"f736c0fd-4410-44b9-ef8a-b8bb8be09bdf"},"source":["# Example data.\n","np.random.seed(0) #do not change - for grading purposes\n","\n","x_example = np.random.randn(10, 5)\n","y_example = np.array([1, 0, 0, 1, 1])\n","y_example = y_example.reshape(1, y_example.shape[0])\n","\n","print (\"X is of shape: \", x_example.shape)\n","print (\"Y is of shape: \", y_example.shape)\n","print(x_example)\n","\n","import doctest\n","\"\"\"\n","  >>> print(x_example.shape)\n","  (10, 5)\n","  >>> print(y_example.shape)\n","  (1, 5)\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["\n","PYDEV DEBUGGER WARNING:\n","sys.settrace() should not be used when the debugger is being used.\n","This may cause the debugger to stop working correctly.\n","If this is needed, please check: \n","http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n","to see how to restore the debug tracing back correctly.\n","Call Location:\n","  File \"/usr/lib/python3.10/doctest.py\", line 1501, in run\n","    sys.settrace(save_trace)\n","\n"]},{"output_type":"stream","name":"stdout","text":["X is of shape:  (10, 5)\n","Y is of shape:  (1, 5)\n","[[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799]\n"," [-0.97727788  0.95008842 -0.15135721 -0.10321885  0.4105985 ]\n"," [ 0.14404357  1.45427351  0.76103773  0.12167502  0.44386323]\n"," [ 0.33367433  1.49407907 -0.20515826  0.3130677  -0.85409574]\n"," [-2.55298982  0.6536186   0.8644362  -0.74216502  2.26975462]\n"," [-1.45436567  0.04575852 -0.18718385  1.53277921  1.46935877]\n"," [ 0.15494743  0.37816252 -0.88778575 -1.98079647 -0.34791215]\n"," [ 0.15634897  1.23029068  1.20237985 -0.38732682 -0.30230275]\n"," [-1.04855297 -1.42001794 -1.70627019  1.9507754  -0.50965218]\n"," [-0.4380743  -1.25279536  0.77749036 -1.61389785 -0.21274028]]\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=2)"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"0hfys8TKMcrd"},"source":["## Part 1:  The Framework"]},{"cell_type":"markdown","metadata":{"id":"mHIA3o8bxGW4"},"source":["### 1.1  Layer Class\n","In this section you will create a Layer class.  Test your code using the example data in the following code cell.  Complete the following steps and refer to the previous lecture units for guidance.\n","- Step 1-1a:  Create the Layer class.\n","- Step 1-1b:  Write the class constructor.\n","- Step 1-1c:  Write the sigmoid, relu, and tanh activation functions.\n","- Step 1-1d:  Write the activation_backward function to compute the derivatives of the activations.\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1Fpx-NYPbNeaRiCg6ZBHZrRu754mcurX-)\n"]},{"cell_type":"code","metadata":{"id":"EUy1ZdNYTobL","executionInfo":{"status":"ok","timestamp":1691008002580,"user_tz":420,"elapsed":204,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["class Layer:\n","\n","  #Constructor parameter order: output_units, input_units, activation, initialization\n","  #   set initialization default to 'he'\n","  #def __init__(self, None, None, None, None='None'):\n","  def __init__(self, output_units, input_units, activation, initialization='he'):\n","\n","    # He initialization\n","    if initialization == 'he':\n","      self.weights = np.random.randn(output_units, input_units)  * np.sqrt((2/input_units))\n","\n","    # Random initialization\n","    if initialization == 'random':\n","      self.weights = np.random.randn(output_units, input_units) * 0.01\n","\n","    #self.biases = None\n","    self.biases = np.zeros((output_units, 1))\n","\n","    # Create an instance variable for the activation function\n","    #self.activation = None\n","    self.activation = activation\n","\n","  #Sigmoid activation:  return the activation of Z applied to the sigmoid function\n","  def sigmoid(self, Z):\n","    #return None\n","    return 1 / (1 + np.exp(-Z))\n","\n","  #ReLu activation:  return the activation of Z applied to the ReLu function\n","  def relu(self, Z):\n","    #return None\n","    return np.maximum(0, Z)\n","\n","  #Tanh activation:  return the activation of Z applied to the tanh function\n","  #Note, use the numpy tanh function here\n","  def tanh(self, Z):\n","    #return None\n","    return np.tanh(Z)\n","\n","  # Backward activations\n","  # Computes the derivatives of activation functions.\n","  #def activation_backwards(self, dA):\n","\n","    # Check the activation function then calc the activiation gradient\n","    #       (e.g. sigmoid_grad, relu_grad, tanh_grad)\n","    #Sigmoid\n","    #if self.activation == 'sigmoid':\n","      #Calculate the sigmoid gradient\n","      #None = None\n","      #Calculate dZ by multiplying dA and activation_grad\n","      #dZ = None\n","      #return dZ\n","\n","    #ReLu\n","    #if None == 'None':\n","      #Calculate the ReLu gradient\n","      #None = None\n","      #Calculate dZ by multiplying dA and activation_grad\n","      #dZ = None\n","      #return dZ\n","\n","    #Tanh\n","    #if None == 'None':\n","      #Calculate the tanh gradient\n","      #None = None\n","      #Calculate dZ by multiplying dA and activation_grad\n","      #dZ = None\n","      #return dZ\n","  #def tanh(self, Z):\n","    #return np.tanh(Z)\n","\n","  def activation_backwards(self, dA):\n","    if self.activation == 'sigmoid':\n","    # Calculate the sigmoid gradient using stored Z\n","     sigmoid_grad = self.sigmoid(self.Z) * (1 - self.sigmoid(self.Z))\n","    # Calculate dZ by multiplying dA and activation_grad\n","     dZ = dA * sigmoid_grad\n","     return dZ\n","\n","    if self.activation == 'relu':\n","    # Calculate the ReLu gradient using stored Z\n","     relu_grad = np.where(self.Z > 0, 1, 0)\n","    # Calculate dZ by multiplying dA and activation_grad\n","     dZ = dA * relu_grad\n","     return dZ\n","\n","    if self.activation == 'tanh':\n","    # Calculate the tanh gradient using stored Z\n","     tanh_grad = 1 - np.power(np.tanh(self.Z), 2)\n","    # Calculate dZ by multiplying dA and activation_grad\n","     dZ = dA * tanh_grad\n","\n","     return dZ\n","\n","\n","   #Abstract methods - will complete in child classes\n","  def forward(self):\n","    pass\n","\n","  def backward(self):\n","    pass\n","\n","  def update(self):\n","    pass"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Da03EeubLNGb"},"source":["**Step 1.1e:**  Layer Class Instantiation\n","Instantiate a layer with input units equal to the number of features in x_example, 5 output units, and relu activation. Print out the weights and biases.\n"]},{"cell_type":"code","metadata":{"id":"Bz-Od7dkpe7V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691008009038,"user_tz":420,"elapsed":1011,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"f18b1d40-cfac-4eb8-d7e3-33449405c0da"},"source":["# instantiate a layer.\n","np.random.seed(0) #do not change - for grading purposes\n","\n","#Create a layer reference by instantiating a Layer object with:\n","#  num of output units =5,\n","#  num of input units = shape of x_example (from the dataset creation section),\n","#  activation = relu\n","#layer = None\n","layer = Layer(5, x_example.shape[0], 'relu')\n","\n","#print (\"Weights: \\n\", layer.weights)\n","#print (\"Biases: \\n\", layer.biases)\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(layer.weights[0][0], 3))\n","  0.789\n","  >>> print(np.round(layer.weights[4][0], 3))\n","  -0.469\n","  >>> print(np.round(layer.biases[0][0], 1))\n","  0.0\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=3)"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"K7orZpTKM1mJ"},"source":["**Step 1.1f:**  Layer Class FeedForward - forward step\n","Compute forward step for a single layer.\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1TRGzgXctY_aN9aDWO2EhVe1JkfgLhb1S)\n"]},{"cell_type":"code","metadata":{"id":"MixaiG-tqjLW","executionInfo":{"status":"ok","timestamp":1691008013562,"user_tz":420,"elapsed":191,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Compute forward step for a single layer.\n","def forward(layer, input):\n","\n","  # Create an instance variable for the input.\n","  #layer.input = None\n","  layer.input = input\n","\n","\n","  # Calculate Z using the weights, biases, and input of this layer,\n","  # then store Z in an instance variable\n","  #Z = None\n","  #layer.Z = None\n","  Z = np.dot(layer.weights, input) + layer.biases\n","  layer.Z = Z\n","\n","\n","  # Check which activation function was set in your layer instance\n","  # (see previous test cell), then call the correct method.\n","  # e.g. if layer.activation == \"sigmoid\": A = layer.sigmoid(Z)\n","\n"," # if None == \"None\":\n","    #A = layer.sigmoid(Z)\n","\n","  #elif None == \"None\":\n","    #A = None\n","\n","  #elif None == 'None':\n","    #A = None\n","\n","  if layer.activation == \"sigmoid\":\n","    A = layer.sigmoid(Z)\n","\n","  elif layer.activation == \"relu\":\n","    A = layer.relu(Z)\n","\n","  elif layer.activation == 'tanh':\n","    A = layer.tanh(Z)\n","\n","  # Store the activation result as an instance variable and return it.\n","  #layer.A = None\n","  layer.A = A\n","  return A"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kcXNMJ8TSi9V"},"source":["**Step 1.1g:**  Layer Class FeedForward - forward method call for the layer\n","Calculate A for your example data with the layer you previously instantiated."]},{"cell_type":"code","metadata":{"id":"PSDWighxsZC1","outputId":"0b65e234-569d-47a2-8073-9911a71a68d9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691008017317,"user_tz":420,"elapsed":195,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Calculate A for your example data with the layer you previously instantiated.\n","# by calling the forward method and pass to it your layer instance and the dataset (x_example)\n","\n","#A = None\n","A = forward(layer, x_example)\n","\n","print(layer.Z.shape)\n","print(layer.A.shape)\n","print (\"A: \\n\", A)\n","\n","assert (layer.Z.shape == (layer.weights.shape[0], layer.A.shape[1]))\n","assert (layer.A.shape == (layer.weights.shape[0], layer.input.shape[1]))\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(layer.Z.shape)\n","  (5, 5)\n","  >>> print(layer.Z.shape)\n","  (5, 5)\n","  >>> print(np.round(layer.A[0][0], 3))\n","  0.141\n","  >>> print(np.round(layer.A[4][2], 3))\n","  0.79\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["(5, 5)\n","(5, 5)\n","A: \n"," [[0.14082542 3.05836426 1.43926944 0.         1.99575878]\n"," [0.         1.77602343 0.         0.         0.96692557]\n"," [0.         0.         0.         0.         0.        ]\n"," [0.30989993 0.         0.12274639 0.         0.29186782]\n"," [1.60744178 0.60481148 0.79011514 0.         0.        ]]\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=4)"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"Yy80HEj2q5PD"},"source":["**Step 1.1h:**  Layer Class Back Propagation - backward method call for the layer\n","Calculate the backward step.\n","\n","\n","![alt text](https://docs.google.com/uc?export=download&id=12sXyoW-HZ0BKSqENnFBuf4GXJviBqYxL)\n","\n","\n","<br>\n","\n","Where *weights* = $W^T$\n","See the lecture material for more details on these calculations.\n","Regarding dA_prev, recall:\n","\n","$$\\frac{\\partial E}{\\partial A^{[L-1]}}=\\frac{\\partial E}{\\partial X^{[L]}}$$\n","and\n","$$\\frac{\\partial E}{\\partial X}=W^T\\cdot\\frac{\\partial E}{\\partial Z}$$\n","\n","<br>\n","\n","Hints:\n","- For *da_prev*,  don't forget to tranpose the weights matrix\n","- For *dB*,  use np.sum with axis=1 and keepdims=True"]},{"cell_type":"code","metadata":{"id":"URQBBKxrrVBh","executionInfo":{"status":"ok","timestamp":1691008022324,"user_tz":420,"elapsed":198,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["def backward(layer, dA_prev):\n","\n","  # Calculate dZ by calling the activation_backwards() method from your Layer class\n","  # and pass to it dA_prev\n","  #dZ = None\n","  dZ = layer.activation_backwards(dA_prev)\n","\n","  # Extract the number of examples from the shape of the input instance variable\n","  # that you created in your forward method.\n","  #m = None\n","  m = layer.input.shape[1]\n","\n","  # Calculate dW and db using the input to this layer (ie the activation of the previous layer).\n","  # For db,\n","  #layer.dW = None\n","  #layer.db = None\n","  layer.dW = 1/m * np.dot(dZ, layer.input.T)\n","  layer.db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n","\n","  # Calculate dW and db using the input to this layer (ie the activation of the previous layer).\n","  #dA_prev = None\n","  dA_prev = np.dot(layer.weights.T, dZ)\n","\n","\n","  return dA_prev"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uDw0nKBEsR3U"},"source":["**Step 1.1i:**  Layer Class Back Propagation - Calculate the derivative of the error (cost) function between the output layer activation and the label Y.\n","dAL is the derivative of the cost function with respect to the output of the final layer.\n","\n","For the cost, use the binary cross entropy loss function:\n","\n","$$cost=\\frac{-1}{m}*\\sum [Y*log(AL) + (1-Y)*log(1-AL)]$$\n","\n","And the derivative of the cost is:\n","$$dAL = -\\bigl(\\frac{Y}{AL+\\epsilon}-\\frac{1-Y}{1-AL+\\epsilon}\\bigr)$$\n","\n","\n","Confirm your outputs pass the doctests in the cell."]},{"cell_type":"code","metadata":{"id":"3Lrz8yXGtA5u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691008026207,"user_tz":420,"elapsed":257,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"9a518a4f-60b6-426c-f35e-deef31dab9d4"},"source":["# Calculate the derivative of the output.\n","\n","#dAL = None\n","Y=y_example\n","#dAL = - (np.divide(y_example, A) - np.divide(1 - y_example, 1 - A))\n","# prevent divide by zero errors, as we deal with  binary cross-entropy loss with a sigmoid activation. Sigmoid function can sometimes output exactly 0 or 1, causing division by zero in the gradient computation.\n","\n","epsilon = 1e-8\n","\n","dAL = - (np.divide(Y, A + epsilon) - np.divide(1 - Y, 1 - A + epsilon))\n","\n","print (\"dAL: \\n\", dAL)\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(dAL[0][0], 3))\n","  -7.101\n","  >>> print(np.round(dAL[0][2], 3))\n","  -2.277\n","  >>> print(np.round(dAL[3][2], 3))\n","  1.14\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["dAL: \n"," [[-7.10099043e+00 -4.85822663e-01 -2.27650715e+00 -1.00000000e+08\n","  -5.01062555e-01]\n"," [-1.00000000e+08 -1.28862090e+00  9.99999990e-01 -1.00000000e+08\n","  -1.03420576e+00]\n"," [-1.00000000e+08  9.99999990e-01  9.99999990e-01 -1.00000000e+08\n","  -1.00000000e+08]\n"," [-3.22684795e+00  9.99999990e-01  1.13992120e+00 -1.00000000e+08\n","  -3.42620841e+00]\n"," [-6.22106508e-01  2.53043781e+00  4.76451693e+00 -1.00000000e+08\n","  -1.00000000e+08]]\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=3)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"4mRbkDJhug7X"},"source":["**Step 1.1j:**  Layer Class Back Propagation - derivative of previous layer\n","Calculate dA_prev, the derivative of the activation of the previous layer using the backward function.\n"]},{"cell_type":"code","metadata":{"id":"tw89iI1bwJ6y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691008031022,"user_tz":420,"elapsed":195,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"b5163f2f-9d06-44a7-fc02-db902e3cf296"},"source":["# Calculate the derivative of the activation of the previous layer using the backward function.\n","\n","# Call the backward method and pass to it the layer instance and dAL\n","#dA_prev = None\n","\n","dA_prev = backward(layer, dAL)\n","\n","\n","print (\"dA_prev: \\n\", dA_prev)\n","assert (dA_prev.shape == layer.input.shape)\n","assert (layer.dW.shape == layer.weights.shape)\n","assert (layer.db.shape == layer.biases.shape)\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(dA_prev[0][0], 3))\n","  -5.534\n","  >>> print(np.round(dA_prev[0][2], 3))\n","  -3.951\n","  >>> print(np.round(dA_prev[6][2], 3))\n","  -3.009\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["dA_prev: \n"," [[-5.53391002 -1.65287114 -3.95117607  0.         -0.6993318 ]\n"," [-1.42141612 -2.53198016 -3.24032402  0.         -1.34172371]\n"," [-1.35227447 -2.58211865 -5.08466788  0.          0.78900126]\n"," [-4.80057955  1.65059658  0.86542143  0.          2.47664959]\n"," [-5.28686694 -1.23829714 -3.16463903  0.         -0.09069034]\n"," [ 2.99975469 -0.47570814  0.14122699  0.         -0.17490287]\n"," [-4.44403197 -2.4851645  -3.00948514  0.         -2.7890347 ]\n"," [-1.4707928   1.03095926  2.42369645  0.         -1.71353854]\n"," [ 1.33574525 -1.98435276 -3.53119284  0.          0.47181262]\n"," [-0.80848343  0.16224977 -1.02543233  0.          0.76622332]]\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=3)"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"criAEC5WvYzd"},"source":["**Step 1.1k:**  Layer Class Back Propagation - update\n","Update the layer weights and biases.\n","\n","\n","![alt text](https://docs.google.com/uc?export=download&id=18cw_sLHVkAs_1ekztijXzj0GcCtaF0Gy)\n"]},{"cell_type":"code","metadata":{"id":"Y2v_40t1rVPj","executionInfo":{"status":"ok","timestamp":1691008036702,"user_tz":420,"elapsed":224,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["def update(layer, learning_rate):\n","\n","  # Update the parameters using the instance variables constructed\n","  # from the Layer class for your layer instance.\n","  #layer.weights = None\n","  #layer.biases = None\n","  layer.weights = layer.weights - learning_rate * layer.dW\n","  layer.biases = layer.biases - learning_rate * layer.db\n"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mevGZw4bwHh3"},"source":["**Step 1.1l:** Layer Class Back Propagation - update method call\n","Call the update method with a learning rate of 0.001.\n","\n","<br>\n","\n","TESTING NOTE:  If you need to run this cell (e.g. to debug your code) more than one time sucessively, you will need to rerun the preceding initialization steps since the instance variables in the Layer class (i.e. the weights and biases) will assume new values and therefore will be used as the stored values for each subsequent call.  This in turn will result in your doctests failing since they are built upon the first update following the initialization steps."]},{"cell_type":"code","metadata":{"id":"Q6eJSpnlwSw0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691008041285,"user_tz":420,"elapsed":184,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"d1368d90-d392-427b-9fe2-a87b13cd8141"},"source":["# Call the update function.\n","\n","# Call the update method and pass your layer instance and learning rate.\n","#update(None, None)\n","\n","learning_rate = 0.001\n","\n","update(layer, learning_rate)\n","\n","print (\"Updated weights: \\n\", layer.weights)\n","print (\"Updated biases: \\n\", layer.biases)\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(layer.weights[0][0], 3))\n","  0.792\n","  >>> print(np.round(layer.weights[3][5], 3))\n","  0.07\n","  >>> print(np.round(layer.biases[0][0], 3))\n","  0.002\n","  >>> print(np.round(layer.biases[3][0], 3))\n","  0.001\n","\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated weights: \n"," [[ 0.79208515  0.17763236  0.43844179  1.00259796  0.83225612 -0.43905097\n","   0.42471018 -0.06683026 -0.04861594  0.18321403]\n"," [ 0.06490766  0.65070067  0.34081303  0.05462312  0.1991396   0.14953941\n","   0.66819797 -0.09149502  0.13953674 -0.38233011]\n"," [-1.14173175  0.29230712  0.38658762 -0.33190629  1.01506513 -0.6504121\n","   0.02046383 -0.08371116  0.6854797   0.65711722]\n"," [ 0.07148965  0.16880458 -0.39680625 -0.88616226 -0.15588041  0.07003232\n","   0.55026671  0.53734024 -0.17385476 -0.13579965]\n"," [-0.46984281 -0.63550952 -0.76451049  0.87189416 -0.22939554 -0.19593853\n","  -0.55959325  0.34595532 -0.71954296 -0.0953017 ]]\n","Updated biases: \n"," [[ 0.00207288]\n"," [ 0.00046457]\n"," [ 0.        ]\n"," [ 0.00110263]\n"," [-0.00133457]]\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=4)"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"sjommhFs0ikz"},"source":["### 1.2  Dense Class\n","In this section you will create a Dense class, which is a child of the Layer class.  You will test your code using the example data in the next section.  Complete the following steps and refer to the previous lecture units for guidance.\n","- Step 1.2a:  Create the Dense class as a child of the Layer class\n","- Step 1.2b:  Write the class constructor\n","- Step 1.2c:  Write the 'forward' method, passing 'self' instead of the layer object as was done in the previous section.  You also need to reference the Z and A variables as self.\n","- Step 1.2d:  Write the 'backward' method, again passing and utilizing 'self' instead of the layer object.  You do not need to implement the backpropagation dAL initialization yet (will be done in the model class).\n","- Step 1.2e:  Write the 'update' method, replacing the layer object with 'self'.\n","\n","<br>\n","\n","\n","![alt text](https://docs.google.com/uc?export=download&id=18T3oO0UHH38cMiwp5s4c8BWeLLZNw0Ph)\n"]},{"cell_type":"code","metadata":{"id":"eia6NkWiMe7T","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1691008123499,"user_tz":420,"elapsed":198,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"ee154ee1-b87b-441d-f5f8-9d5914829308"},"source":["\"\"\"class Dense(Layer):\n","\n","\n","  # Inherit from the Layer class.\n","  # Constructor parameter order: output_units, input_units, activation, initialization\n","  #   set initialization default to 'he'\n","  def __init__(self, None, None, None, None='None'):\n","    super().__init__(None, None, None, None)\n","\n","  # Forward step for a single layer.\n","  # See forward implementation from the Layer class section but\n","  #   instead of referencing the layer instance, use self\n","  # Store input as a instance variable.\n","  # Calculate Z and set as an instance variable.\n","  # Calculate sigmoid, ReLu and tanh activations.\n","\n","  def forward(self, input):\n","    self.input = None\n","    Z = None\n","    self.Z = None\n","\n","    # sigmoid activation\n","    if None == \"None\":\n","      A = None\n","\n","    # relu activation\n","    elif None == \"None\":\n","      A = None\n","\n","    # tanh activation\n","    if None == 'None':\n","      A = None\n","\n","    self.A = A\n","\n","\n","    assert (self.Z.shape == (self.weights.shape[0], self.A.shape[1]))\n","    assert (self.A.shape == (self.weights.shape[0], self.input.shape[1]))\n","\n","    return A\n","\n","\n","  # Backward step for a single layer.\n","  # See backward implementation from the Layer class section but\n","  #   instead of referencing the layer instance, use self\n","  def backward(self, dA_prev):\n","\n","    dZ = None\n","    m = None\n","    self.dW = None\n","    self.db = None\n","    dA_prev = None\n","\n","\n","    assert (dA_prev.shape == self.input.shape)\n","    assert (self.dW.shape == self.weights.shape)\n","    assert (self.db.shape == self.biases.shape)\n","\n","    return dA_prev\n","\n","\n","\n","  # Parameter update for a single layer.\n","  # See update implementation from the Layer class section but\n","  #   instead of referencing the layer instance, use self\n","\n","  def update(self, learning_rate):\n","    self.weights = self.weights - learning_rate * self.dW\n","    self.biases = self.biases - learning_rate * self.db\n","\n","  #Accessor method for weights (self.weights)\n","  def getWeights(self):\n","    return self.weights\n","\n","  #Accessor method for biases (self.biases)\n","  def getBiases(self):\n","    return self.biases\n","\"\"\""],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'class Dense(Layer):\\n\\n\\n  # Inherit from the Layer class.\\n  # Constructor parameter order: output_units, input_units, activation, initialization\\n  #   set initialization default to \\'he\\'\\n  def __init__(self, None, None, None, None=\\'None\\'):\\n    super().__init__(None, None, None, None)\\n\\n  # Forward step for a single layer.\\n  # See forward implementation from the Layer class section but\\n  #   instead of referencing the layer instance, use self\\n  # Store input as a instance variable.\\n  # Calculate Z and set as an instance variable.\\n  # Calculate sigmoid, ReLu and tanh activations.\\n\\n  def forward(self, input):\\n    self.input = None\\n    Z = None\\n    self.Z = None\\n\\n    # sigmoid activation\\n    if None == \"None\":\\n      A = None\\n\\n    # relu activation\\n    elif None == \"None\":\\n      A = None\\n\\n    # tanh activation\\n    if None == \\'None\\':\\n      A = None\\n\\n    self.A = A\\n\\n\\n    assert (self.Z.shape == (self.weights.shape[0], self.A.shape[1]))\\n    assert (self.A.shape == (self.weights.shape[0], self.input.shape[1]))\\n\\n    return A\\n\\n\\n  # Backward step for a single layer.\\n  # See backward implementation from the Layer class section but\\n  #   instead of referencing the layer instance, use self\\n  def backward(self, dA_prev):\\n\\n    dZ = None\\n    m = None\\n    self.dW = None\\n    self.db = None\\n    dA_prev = None\\n\\n\\n    assert (dA_prev.shape == self.input.shape)\\n    assert (self.dW.shape == self.weights.shape)\\n    assert (self.db.shape == self.biases.shape)\\n\\n    return dA_prev\\n\\n\\n\\n  # Parameter update for a single layer.\\n  # See update implementation from the Layer class section but\\n  #   instead of referencing the layer instance, use self\\n\\n  def update(self, learning_rate):\\n    self.weights = self.weights - learning_rate * self.dW\\n    self.biases = self.biases - learning_rate * self.db\\n\\n  #Accessor method for weights (self.weights)\\n  def getWeights(self):\\n    return self.weights\\n\\n  #Accessor method for biases (self.biases)\\n  def getBiases(self):\\n    return self.biases\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["class Dense(Layer):\n","\n","  def __init__(self, output_units, input_units, activation, initialization='he'):\n","    super().__init__(output_units, input_units, activation, initialization)\n","\n","  def forward(self, input):\n","    self.input = input\n","    Z = np.dot(self.weights, input) + self.biases\n","    self.Z = Z\n","\n","    # sigmoid activation\n","    if self.activation == \"sigmoid\":\n","      A = self.sigmoid(Z)\n","\n","    # relu activation\n","    elif self.activation == \"relu\":\n","      A = self.relu(Z)\n","\n","    # tanh activation\n","    elif self.activation == 'tanh':\n","      A = self.tanh(Z)\n","\n","    self.A = A\n","\n","    assert (self.Z.shape == (self.weights.shape[0], self.A.shape[1]))\n","    assert (self.A.shape == (self.weights.shape[0], self.input.shape[1]))\n","\n","    return A\n","\n","  def backward(self, dA_prev):\n","    dZ = self.activation_backwards(dA_prev)\n","    m = self.input.shape[1]\n","    self.dW = 1/m * np.dot(dZ, self.input.T)\n","    self.db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n","    dA_prev = np.dot(self.weights.T, dZ)\n","\n","    assert (dA_prev.shape == self.input.shape)\n","    assert (self.dW.shape == self.weights.shape)\n","    assert (self.db.shape == self.biases.shape)\n","\n","    return dA_prev\n","\n","  def update(self, learning_rate):\n","    self.weights = self.weights - learning_rate * self.dW\n","    self.biases = self.biases - learning_rate * self.db\n","\n","  def getWeights(self):\n","    return self.weights\n","\n","  def getBiases(self):\n","    return self.biases\n","\n"],"metadata":{"id":"GnKJo0vN77xu","executionInfo":{"status":"ok","timestamp":1691008147960,"user_tz":420,"elapsed":211,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yyn-TZF-xLBc"},"source":["### 1.3  Model Class\n","In this section you will create a Model class.  Test your code using the example data in the following code cell.  Complete the following steps and refer to the previous lecture units for guidance.\n","- Step 1.3a:  Create the Model class\n","- Step 1.3b:  Write the class constructor.  Set a small epsilon value to 1e-8.\n","- Step 1.3c:  Write the 'build' method that inputs a list of layer instances defining the network.\n","- Step 1.3d:  Write the 'feedforward' method which propagates the input X through all the layers by calling the 'forward' method in a loop. Note that your input to each successive layer in the network is the output from the previous layer. Set your AL variable equal to the final layer output and return AL.\n","- Step 1.3e:  Write the 'compute_cost' method.  Implement the binary cross entropy loss equation.  Call the numpy 'squeeze' function to reduce the dimensions of the resultant cost.  Return the cost.\n","- Step 1.3f:  Write the 'backpropagation' method which inputs the derivative of the final layer output.  Loop over the layers in reverse order, calling the 'backward' method for each loop iteration.\n","Hint:  https://docs.python.org/3/library/functions.html#reversed\n","- Step 1.3g:  Write the 'parameter_update' method.  Loop over the layers, calling the 'update' method for each looip iteration.\n","- Step 1.3h:  Write the 'train' method which implements a single iteration of training.  The method should input the input X, label Y, and learning rate, and return the cost for that iteration.  Remember to use 'self' when accessing the 'network' data structure, and when calling the 'feedforward', 'compute_cost', 'backpropagation' and 'parameter_update' methods.\n","- Step 1.3i:  Write the 'fit' method which runs the training over a number of iterations or 'epochs'.  Input the X, Y training data and learning rate.  Set the 'verbose' parameter to true if you want to see the costs printed.  Do not worry about the 'callback' parameter at this time (we'll use this in a later unit). Initialize an empty list which stores the cost for each epoch run, then loop over the number of iterations (epochs) and call the 'train' method, appending the returned cost to your list.\n","- Step 1.3j:  Write the 'predict' method which returns the predicted labels for our data.  Call the 'feedforward' method which returns AL, the activation for the final layer.  Convert the labels to binary values using the numpy 'where' method.\n","Hint:  https://numpy.org/doc/stable/reference/generated/numpy.where.html\n","- Step 1.3k:  Write the 'evaluate' method which inputs the predictions and the training labels (i.e. the ground truth values).  Return an accuracy score.\n","- Step 1.3l:  Write the 'plot' method to plot the costs (y-axis) versus the epoch number (x-axis).\n","\n","<br>\n","\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1SNWhSJ7TKQgEud7x2PnG4GHaR77RLhsR)\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=10uol8KD6opqBjo-RMiKznJi3pFGCwb9L)\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1B0Nl2alAUOn4aJhzIdyLQ15MnDyvoCgB)\n"]},{"cell_type":"code","metadata":{"id":"nu7JS8zENMUu","executionInfo":{"status":"ok","timestamp":1691008389318,"user_tz":420,"elapsed":226,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"eb37c30f-06b8-46ca-c298-7bfc1c6f3b3d"},"source":["\"\"\"class Model:\n","\n","  #Constructor just sets epsilon\n","  def __init__(self):\n","    self.epsilon = 1e-8\n","\n","  # Build the network.\n","  # Allow the network to be passed in as a single variable (a list of layers).\n","  # Set an instance variable to the network.\n","  def build(self, network):\n","    self.network = None\n","\n","\n","  # Forward propagation\n","  def feedforward(self, X):\n","\n","    # Set the input to X.\n","    input = None\n","\n","    # Loop over the layers in the network and call the forward method for each layer's input.\n","    # Note the network has been passed into the build method as a list of layers.\n","    for None in None\n","      #Call the forward method passing in the input\n","      input = None\n","\n","    # Set AL to the final value of input (i.e. the output of the final layer).\n","    # Note, it will be generated from the last iteration of the previous loop\n","    AL = None\n","\n","\n","    assert (AL.shape == (1, X.shape[1]))\n","    return AL\n","\n","\n","  # Compute the cost\n","  #   m = number of examples in Y\n","  #   AL = activations of the the last (output) layer for the number of examples m\n","  #   Y = labels for the number of examples m\n","  #   Use binary cross entropy loss equation\n","  #\n","  # Remember to call np.squeeze().\n","  def compute_cost(self, m, AL, Y):\n","\n","    #Compute the cost using binary cross entropy loss equation\n","    cost = None\n","    #Squeeze the result to remove the 1 dimensional entry\n","    cost = None\n","\n","    return cost\n","\n","\n","  # Backpropagate through layers.\n","  #   dAL = derivative of the output layer activations\n","  def backpropagation(self, dAL):\n","\n","    # Set dA_prev to dAL.\n","    dA_prev = None\n","\n","    # Loop over the layers in network in reverse order and\n","    #   call the backward method on each dA_prev.\n","    for None in None:\n","      dA_prev = None\n","\n","\n","  # Update the parameters\n","  #   learning_rate = the parameter update equations learning rate\n","  def parameter_update(self, learning_rate):\n","\n","    # Loop over layers in network and call the update method from each.\n","    for None in None:\n","      None\n","\n","\n","  # Single iteration of descent.\n","  #   X = training data\n","  #   Y = training labels\n","  #   learning_rate = the parameter update equations learning rate\n","  def train(self, X, Y, learning_rate):\n","\n","    # Extract number of examples from the shape of Y.\n","    m = None\n","\n","    # Compute AL by running forward propagation.\n","    #   call feedforward and pass in the training data\n","    AL = None\n","\n","    # Compute the cost\n","    #   call compute_cost and pass in the number of examples, output layer activations, and Y labels\n","    cost = None\n","\n","    # Reshape output Y to the output layer activations\n","    Y = Y.reshape(AL.shape)\n","\n","    # Initialize backpropagation by calculating dAL.\n","    #   calculate the derivative of the binary cross entropy loss\n","    dAL = None\n","\n","    # Backpropagate through layers.\n","    #   call backpropagation and pass in dAL\n","    None\n","\n","    # Update the parameters.\n","    #   call parameter_update and pass in the learning rate\n","    None\n","\n","    return cost\n","\n","\n","  # Function for training loop.\n","  #   X_train, Y_train = training dataset and labels\n","  #   epochs = number of training iterations\n","  #   learning_rate = the parameter update equations learning rate\n","  #   verbose = enable flag for print of the costs during training\n","  #   callback = set default to None (will be used in a later unit)\n","  def fit(self, X_train, Y_train, epochs, learning_rate, verbose=False, callback=None):\n","\n","    # Initialize list of costs (to an empty list).\n","    costs = None\n","\n","    # Loop over the number of training iterations.\n","    for None in None:\n","\n","      # Call the train method and append the cost to the list.\n","      cost = None\n","      None\n","\n","      #Print and store costs if verbose flag is set\n","      if (i % 50 == 0):\n","        if verbose==True:\n","          print (\"Iteration: {}, cost: {}\". format(i, cost))\n","\n","        if (callback is not None):\n","          callback(i, X_train, Y_train)\n","\n","    return costs\n","\n","\n","  # Predict labels.\n","  #   X = training data\n","  def predict(self, X):\n","\n","    # Get the predictions (output of the final layer after training).\n","    #   call feedforward method and pass in the training data.\n","    predictions = None\n","\n","    # Set predictions to either 0 or 1.\n","    #   use numpy where function to set output to 1 if > 0.5\n","    predictions = None\n","\n","    return predictions\n","\n","\n","  # Evaluate the prediction accuracy.\n","  #   predictions = predicted output labels (y_hat)\n","  #   Y = training labels\n","  def evaluate(self, predictions, Y):\n","\n","    # calculate accuracy.\n","    #   100 - (mean of the difference of the predictions and training labels)*100\n","    accuracy = None\n","\n","    return accuracy\n","    \"\"\""],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'class Model:\\n\\n  #Constructor just sets epsilon\\n  def __init__(self):\\n    self.epsilon = 1e-8\\n\\n  # Build the network.\\n  # Allow the network to be passed in as a single variable (a list of layers).\\n  # Set an instance variable to the network.\\n  def build(self, network):\\n    self.network = None\\n\\n\\n  # Forward propagation\\n  def feedforward(self, X):\\n\\n    # Set the input to X.\\n    input = None\\n\\n    # Loop over the layers in the network and call the forward method for each layer\\'s input.\\n    # Note the network has been passed into the build method as a list of layers.\\n    for None in None\\n      #Call the forward method passing in the input\\n      input = None\\n\\n    # Set AL to the final value of input (i.e. the output of the final layer).\\n    # Note, it will be generated from the last iteration of the previous loop\\n    AL = None\\n\\n\\n    assert (AL.shape == (1, X.shape[1]))\\n    return AL\\n\\n\\n  # Compute the cost\\n  #   m = number of examples in Y\\n  #   AL = activations of the the last (output) layer for the number of examples m\\n  #   Y = labels for the number of examples m\\n  #   Use binary cross entropy loss equation\\n  #\\n  # Remember to call np.squeeze().\\n  def compute_cost(self, m, AL, Y):\\n\\n    #Compute the cost using binary cross entropy loss equation\\n    cost = None\\n    #Squeeze the result to remove the 1 dimensional entry\\n    cost = None\\n\\n    return cost\\n\\n\\n  # Backpropagate through layers.\\n  #   dAL = derivative of the output layer activations\\n  def backpropagation(self, dAL):\\n\\n    # Set dA_prev to dAL.\\n    dA_prev = None\\n\\n    # Loop over the layers in network in reverse order and\\n    #   call the backward method on each dA_prev.\\n    for None in None:\\n      dA_prev = None\\n\\n\\n  # Update the parameters\\n  #   learning_rate = the parameter update equations learning rate\\n  def parameter_update(self, learning_rate):\\n\\n    # Loop over layers in network and call the update method from each.\\n    for None in None:\\n      None\\n\\n\\n  # Single iteration of descent.\\n  #   X = training data\\n  #   Y = training labels\\n  #   learning_rate = the parameter update equations learning rate\\n  def train(self, X, Y, learning_rate):\\n\\n    # Extract number of examples from the shape of Y.\\n    m = None\\n\\n    # Compute AL by running forward propagation.\\n    #   call feedforward and pass in the training data\\n    AL = None\\n\\n    # Compute the cost\\n    #   call compute_cost and pass in the number of examples, output layer activations, and Y labels\\n    cost = None\\n\\n    # Reshape output Y to the output layer activations\\n    Y = Y.reshape(AL.shape)\\n\\n    # Initialize backpropagation by calculating dAL.\\n    #   calculate the derivative of the binary cross entropy loss\\n    dAL = None\\n\\n    # Backpropagate through layers.\\n    #   call backpropagation and pass in dAL\\n    None\\n\\n    # Update the parameters.\\n    #   call parameter_update and pass in the learning rate\\n    None\\n\\n    return cost\\n\\n\\n  # Function for training loop.\\n  #   X_train, Y_train = training dataset and labels\\n  #   epochs = number of training iterations\\n  #   learning_rate = the parameter update equations learning rate\\n  #   verbose = enable flag for print of the costs during training\\n  #   callback = set default to None (will be used in a later unit)\\n  def fit(self, X_train, Y_train, epochs, learning_rate, verbose=False, callback=None):\\n\\n    # Initialize list of costs (to an empty list).\\n    costs = None\\n\\n    # Loop over the number of training iterations.\\n    for None in None:\\n\\n      # Call the train method and append the cost to the list.\\n      cost = None\\n      None\\n\\n      #Print and store costs if verbose flag is set\\n      if (i % 50 == 0):\\n        if verbose==True:\\n          print (\"Iteration: {}, cost: {}\". format(i, cost))\\n\\n        if (callback is not None):\\n          callback(i, X_train, Y_train)\\n\\n    return costs\\n\\n\\n  # Predict labels.\\n  #   X = training data\\n  def predict(self, X):\\n\\n    # Get the predictions (output of the final layer after training).\\n    #   call feedforward method and pass in the training data.\\n    predictions = None\\n\\n    # Set predictions to either 0 or 1.\\n    #   use numpy where function to set output to 1 if > 0.5\\n    predictions = None\\n\\n    return predictions\\n\\n\\n  # Evaluate the prediction accuracy.\\n  #   predictions = predicted output labels (y_hat)\\n  #   Y = training labels\\n  def evaluate(self, predictions, Y):\\n\\n    # calculate accuracy.\\n    #   100 - (mean of the difference of the predictions and training labels)*100\\n    accuracy = None\\n\\n    return accuracy\\n    '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["class Model:\n","\n","  def __init__(self):\n","    self.epsilon = 1e-8\n","\n","  def build(self, network):\n","    self.network = network\n","\n","  def feedforward(self, X):\n","    input = X\n","    for layer in self.network:\n","      input = layer.forward(input)\n","    AL = input\n","    assert (AL.shape == (1, X.shape[1]))\n","    return AL\n","\n","  def compute_cost(self, m, AL, Y):\n","    cost = -(1/m) * np.sum(Y * np.log(AL + self.epsilon) + (1 - Y) * np.log(1 - AL + self.epsilon))\n","    cost = np.squeeze(cost)\n","    return cost\n","\n","  def backpropagation(self, dAL):\n","    dA_prev = dAL\n","    for layer in reversed(self.network):\n","      dA_prev = layer.backward(dA_prev)\n","\n","  def parameter_update(self, learning_rate):\n","    for layer in self.network:\n","      layer.update(learning_rate)\n","\n","  def train(self, X, Y, learning_rate):\n","    m = Y.shape[1]\n","    AL = self.feedforward(X)\n","    cost = self.compute_cost(m, AL, Y)\n","    Y = Y.reshape(AL.shape)\n","    dAL = - (np.divide(Y, AL + self.epsilon) - np.divide(1 - Y, 1 - AL + self.epsilon))\n","    self.backpropagation(dAL)\n","    self.parameter_update(learning_rate)\n","    return cost\n","\n","  def fit(self, X_train, Y_train, epochs, learning_rate, verbose=False, callback=None):\n","    costs = []\n","    for i in range(epochs):\n","      cost = self.train(X_train, Y_train, learning_rate)\n","      costs.append(cost)\n","      if (i % 50 == 0) and verbose:\n","        print(\"Iteration: {}, cost: {}\".format(i, cost))\n","      if callback is not None and (i % 50 == 0):\n","        callback(i, X_train, Y_train)\n","    return costs\n","\n","  def predict(self, X):\n","    predictions = self.feedforward(X)\n","    predictions = np.where(predictions > 0.5, 1, 0)\n","    return predictions\n","\n","  def evaluate(self, predictions, Y):\n","    accuracy = 100 - np.mean(np.abs(predictions - Y)) * 100\n","    return accuracy\n"],"metadata":{"id":"2mHc1aIF869M","executionInfo":{"status":"ok","timestamp":1691008398362,"user_tz":420,"elapsed":3285,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"mV-Di66v7jhr","executionInfo":{"status":"ok","timestamp":1691008404266,"user_tz":420,"elapsed":238,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Use matplotlib to plot the costs.\n","\n","def plot(costs):\n","\n","  plt.plot(costs)\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Cost')\n","\n","  return plt"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_CRsielelzCy"},"source":["## Part 2:  Test your framework\n","Finally, you will test the framework you created with a simple perceptron example.\n","- Step a:  Use the synthetic dataset created in the code cell below.\n","- Step b:  Build your model as a simple perceptron (i.e. 1 output) with 2 independent variables (use shape[0]) and 4 examples.  Use the sigmoid activation function.\n","- Step c:  Call the 'fit' method on your model passing as inputs the X and Y training data.  Run for 2501 epochs and a learning rate of 0.0075.\n","- Step d:  Plot your costs.\n","\n","WHEN YOU COMPLETE THIS SECTION - STOP, AND UPLOAD YOUR COLAB SCRIPT FOR THIS ASSIGNMENT.  THE OTHER PARTS IN THE COLAB SCRIPT WILL BE FOR SUBSEQUENT PROJECT/ASSIGNMENTS\n"]},{"cell_type":"code","metadata":{"id":"opwxjtTOOPG4","executionInfo":{"status":"ok","timestamp":1691008604875,"user_tz":420,"elapsed":981,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/","height":536},"outputId":"841cde0e-8def-40ba-c980-6017a85e1d25"},"source":["# Test your model with a simple perceptron and synthetic dataset\n","\n","# Example data.\n","np.random.seed(3) #do not change - for grading purposes\n","x_example_2 = np.random.randn(2, 4)\n","y_example_2 = np.array([0, 0, 0, 1])\n","y_example_2 = y_example_2.reshape(1, 4)\n","\n","print (\"X is of shape: \", x_example_2.shape)\n","print (\"Y is of shape: \", y_example_2.shape)\n","print(x_example_2)\n","\n","\n","\n","#Build the model as a simple perceptron (i.e. 1 output) with 2 independent variables (use shape[0]) and 4 examples\n","#  (use the example_2 data above)\n","np.random.seed(0) #do not change - for grading purposes\n","\n","# Instantiate a simple perceptron.\n","#perceptron = None\n","perceptron = Model()\n","\n","perceptron.build([\n","                  #Dense(None, None, 'None')\n","                  Dense(1, x_example_2.shape[0], 'sigmoid')\n","])\n","\n","# Fit the model.  Use 2501 epochs and a learning rate of 0.0075\n","#costs = None\n","costs=perceptron.fit(x_example_2, y_example_2, epochs=2501, learning_rate=0.0075)\n","\n","# Plot the costs\n","plot(costs)\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(costs[0], 3))\n","  2.119\n","  >>> print(np.round(costs[75], 3))\n","  1.585\n","  >>> print(np.round(costs[1025], 3))\n","  0.184\n","  >>> print(np.round(costs[1557], 3))\n","  0.127\n","  >>> print(np.round(costs[2500], 3))\n","  0.083\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["X is of shape:  (2, 4)\n","Y is of shape:  (1, 4)\n","[[ 1.78862847  0.43650985  0.09649747 -1.8634927 ]\n"," [-0.2773882  -0.35475898 -0.08274148 -0.62700068]]\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=5)"]},"metadata":{},"execution_count":21},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"sddqJVK71OR1"},"source":["# Project 2.1a:  Logic Function Prediction\n","\n","Note: You should restart the runtime and rerun the initialization steps (drive mount, imports, and function cells) the properly set up the environment before running these steps."]},{"cell_type":"markdown","metadata":{"id":"PWjLU0S61Rw5"},"source":["### Part 1: Logical AND\n","In this section you will test your framework using the AND logic function. Test your code using the example data in the following code cells. Complete the following steps and refer to the previous lecture units for guidance.\n"]},{"cell_type":"markdown","metadata":{"id":"Az7zSop8D0zh"},"source":["**Step a:**  Setup the datasets\n","- Run the following code cell to setup your training and test datasets.\n","- Print out the shapes and output data.  You should see the following outputi for the shapes: (2, 4), (1, 4), (2, 4), (1, 4), and the following for the training:   X_train:  [[0 0 1 1] [0 1 0 1]], Y_train:  [[0 0 0 1]]\n"]},{"cell_type":"code","metadata":{"id":"XMFR5XwdffFd","executionInfo":{"status":"ok","timestamp":1691008623262,"user_tz":420,"elapsed":221,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"87b4e029-2d90-4b1e-b305-fa5451eae7bb"},"source":["# Set the input data\n","X_train = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n","\n","# Set the labels, the correct results for the AND operation\n","Y_train = np.array([0, 0, 0, 1])\n","\n","X_test = np.array([[1, 1, 0, 0], [0, 1, 0, 1]])\n","Y_test = np.array([0, 1, 0, 0])\n","\n","Y_train = Y_train.reshape(1, 4)\n","Y_test = Y_test.reshape(1, 4)\n","\n","print(X_train.shape)\n","print(Y_train.shape)\n","print(X_test.shape)\n","print(Y_test.shape)\n","print(X_train)\n","print(Y_train)"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["(2, 4)\n","(1, 4)\n","(2, 4)\n","(1, 4)\n","[[0 0 1 1]\n"," [0 1 0 1]]\n","[[0 0 0 1]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"8QOk_UfZEia6"},"source":["**Step b:** Create the model.\n","Setup a perceptron as an instance of the Model class (single node).\n","Recall our perceptron is simply an instance of the Dense class with 1 output unit, and a number of input units equal to the number of features in X (use the shape function on X_train as the input parameter).  Set the activation to be sigmoid."]},{"cell_type":"code","metadata":{"id":"F_DMO2vphF-b","executionInfo":{"status":"error","timestamp":1691008623875,"user_tz":420,"elapsed":420,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/","height":236},"outputId":"fe4a7ae7-da83-4438-a147-678aaf2d3383"},"source":["# Instantiate a simple perceptron.\n","#np.random.seed(0) #do not change - for grading purposes\n","\n","#perceptron = None\n","\n","# Build your model.\n","#perceptron.build([\n","                  #None\n","#])"],"execution_count":23,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-3f7c4c517e48>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Build your model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m perceptron.build([\n\u001b[0m\u001b[1;32m      8\u001b[0m                   \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m ])\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'build'"]}]},{"cell_type":"code","source":["# Instantiate a simple perceptron.\n","np.random.seed(0) # do not change - for grading purposes\n","\n","perceptron = Model()\n","\n","# Build your model.\n","perceptron.build([\n","    Dense(1, X_train.shape[0], 'sigmoid')\n","])\n","\n"],"metadata":{"id":"f5ogZwt2-Oo4","executionInfo":{"status":"ok","timestamp":1691010138314,"user_tz":420,"elapsed":217,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KWJtzpp4F5Kf"},"source":["**Step c:** Train the model\n","Fit the perceptron model to the data by calling the âfitâ method of the perceptron object.  Remember that âfitâ returns a list of costs.\n","Use 2501 epochs and a learning_rate of 0.0075.  Don't set anything for the verbose or callback parameters at this point.\n"]},{"cell_type":"code","metadata":{"id":"Ty7CxLnthHKI","executionInfo":{"status":"ok","timestamp":1691010143215,"user_tz":420,"elapsed":830,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Fit your model to the training data (remember the fit() method returns a list of costs).\n","# Use 2501 epochs and a learning_rate of 0.0075. Don't set anything for the verbose or callback parameters\n","#costs = None\n","costs = perceptron.fit(X_train, Y_train, epochs=2501, learning_rate=0.0075)\n","\n"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2XeE1NrHGjcI"},"source":["**Step d:** Plot and verify the costs.\n","\n","Your costs plot should look like this:\n","\n","<br>\n","\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1hp8RjzBRMWIUgqx4H-PgWtsPZX8_caay)\n","\n","\n","Make sure your doctests run without error\n"]},{"cell_type":"code","metadata":{"id":"ONpJEkrahNkF","executionInfo":{"status":"ok","timestamp":1691010146759,"user_tz":420,"elapsed":25,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/","height":504},"outputId":"34c5b5e2-4e8a-4882-965e-30fd0eae8e35"},"source":["# Call the plotting function.\n","#None\n","\n","plot(costs)\n","\n","print(costs)\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(costs[0], 3))\n","  0.909\n","  >>> print(np.round(costs[75], 3))\n","  0.779\n","  >>> print(np.round(costs[1025], 3))\n","  0.408\n","  >>> print(np.round(costs[1557], 3))\n","  0.37\n","  >>> print(np.round(costs[2500], 3))\n","  0.32\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.9092941074737066, 0.9073131596673802, 0.9053390766956841, 0.9033718478832352, 0.901411462469177, 0.899457909607788, 0.8975111783690916, 0.895571257739465, 0.8936381366222568, 0.8917118038384027, 0.8897922481270432, 0.8878794581461475, 0.8859734224731374, 0.884074129605511, 0.8821815679614746, 0.8802957258805711, 0.878416591624314, 0.8765441533768206, 0.8746783992454503, 0.8728193172614439, 0.8709668953805612, 0.8691211214837273, 0.8672819833776741, 0.8654494687955877, 0.8636235653977546, 0.8618042607722127, 0.8599915424354009, 0.858185397832812, 0.8563858143396457, 0.8545927792614646, 0.8528062798348506, 0.8510263032280622, 0.8492528365416938, 0.8474858668093357, 0.8457253809982359, 0.843971366009963, 0.8422238086810675, 0.8404826957837488, 0.838748014026519, 0.8370197500548696, 0.8352978904519388, 0.8335824217391792, 0.8318733303770252, 0.8301706027655639, 0.828474225245204, 0.8267841840973464, 0.8251004655450551, 0.8234230557537279, 0.8217519408317691, 0.8200871068312608, 0.8184285397486365, 0.816776225525352, 0.8151301500485594, 0.8134902991517805, 0.8118566586155792, 0.8102292141682346, 0.8086079514864146, 0.8069928561958495, 0.8053839138720039, 0.8037811100407513, 0.8021844301790455, 0.800593859715593, 0.7990093840315273, 0.7974309884610786, 0.7958586582922469, 0.7942923787674713, 0.7927321350843035, 0.791177912396075, 0.7896296958125697, 0.7880874704006904, 0.7865512211851282, 0.7850209331490308, 0.7834965912346684, 0.7819781803441003, 0.7804656853398406, 0.7789590910455222, 0.7774583822465604, 0.7759635436908152, 0.7744745600892543, 0.7729914161166114, 0.7715140964120478, 0.7700425855798093, 0.7685768681898836, 0.7671169287786569, 0.7656627518495676, 0.7642143218737605, 0.762771623290738, 0.7613346405090116, 0.7599033579067506, 0.7584777598324297, 0.7570578306054754, 0.7556435545169113, 0.7542349158299998, 0.7528318987808849, 0.7514344875792316, 0.7500426664088637, 0.7486564194284008, 0.7472757307718919, 0.7459005845494495, 0.7445309648478801, 0.7431668557313117, 0.7418082412418234, 0.7404551054000694, 0.7391074322059017, 0.7377652056389914, 0.7364284096594492, 0.7350970282084404, 0.7337710452088004, 0.7324504445656488, 0.7311352101669968, 0.7298253258843583, 0.7285207755733549, 0.7272215430743187, 0.7259276122128949, 0.7246389668006398, 0.7233555906356178, 0.7220774675029954, 0.7208045811756326, 0.7195369154146729, 0.7182744539701288, 0.7170171805814667, 0.7157650789781881, 0.7145181328804084, 0.7132763259994339, 0.7120396420383341, 0.7108080646925138, 0.7095815776502801, 0.7083601645934096, 0.7071438091977096, 0.7059324951335784, 0.7047262060665619, 0.7035249256579091, 0.7023286375651221, 0.7011373254425045, 0.6999509729417085, 0.6987695637122756, 0.697593081402177, 0.6964215096583505, 0.6952548321272326, 0.6940930324552922, 0.6929360942895539, 0.6917840012781259, 0.6906367370707192, 0.689494285319167, 0.6883566296779393, 0.6872237538046541, 0.6860956413605871, 0.6849722760111775, 0.6838536414265288, 0.6827397212819093, 0.6816304992582466, 0.6805259590426221, 0.679426084328758, 0.6783308588175045, 0.677240266217323, 0.6761542902447639, 0.6750729146249435, 0.6739961230920167, 0.672923899389646, 0.6718562272714672, 0.6707930905015522, 0.6697344728548678, 0.6686803581177316, 0.6676307300882636, 0.6665855725768356, 0.6655448694065166, 0.664508604413513, 0.6634767614476095, 0.6624493243726021, 0.6614262770667295, 0.6604076034231018, 0.6593932873501244, 0.6583833127719186, 0.6573776636287392, 0.6563763238773881, 0.6553792774916244, 0.6543865084625711, 0.653398000799118, 0.652413738528322, 0.6514337056958016, 0.6504578863661303, 0.6494862646232261, 0.6485188245707344, 0.6475555503324122, 0.646596426052504, 0.6456414358961179, 0.6446905640495958, 0.6437437947208796, 0.6428011121398773, 0.6418625005588203, 0.6409279442526226, 0.6399974275192308, 0.6390709346799758, 0.6381484500799168, 0.6372299580881847, 0.6363154430983194, 0.6354048895286052, 0.634498281822402, 0.6335956044484733, 0.6326968419013094, 0.6318019787014489, 0.6309109993957949, 0.6300238885579291, 0.62914063078842, 0.6282612107151311, 0.6273856129935222, 0.6265138223069483, 0.6256458233669563, 0.6247816009135752, 0.6239211397156059, 0.6230644245709058, 0.6222114403066692, 0.6213621717797064, 0.6205166038767161, 0.6196747215145586, 0.6188365096405207, 0.6180019532325804, 0.6171710372996678, 0.6163437468819202, 0.615520067050937, 0.6146999829100285, 0.6138834795944621, 0.6130705422717067, 0.61226115614167, 0.6114553064369361, 0.6106529784229979, 0.6098541573984853, 0.6090588286953923, 0.6082669776792988, 0.6074785897495896, 0.6066936503396704, 0.6059121449171798, 0.6051340589841987, 0.6043593780774557, 0.6035880877685296, 0.6028201736640484, 0.6020556214058851, 0.6012944166713508, 0.6005365451733826, 0.5997819926607315, 0.599030744918143, 0.5982827877665391, 0.597538107063192, 0.5967966887019004, 0.5960585186131563, 0.5953235827643141, 0.5945918671597538, 0.5938633578410408, 0.5931380408870845, 0.592415902414292, 0.5916969285767195, 0.5909811055662203, 0.590268419612591, 0.5895588569837124, 0.5888524039856889, 0.588149046962986, 0.5874487722985612, 0.5867515664139965, 0.5860574157696233, 0.5853663068646494, 0.5846782262372775, 0.5839931604648265, 0.5833110961638449, 0.5826320199902253, 0.5819559186393124, 0.5812827788460124, 0.5806125873848957, 0.5799453310702989, 0.5792809967564228, 0.5786195713374305, 0.5779610417475383, 0.577305394961107, 0.5766526179927307, 0.5760026978973205, 0.5753556217701884, 0.5747113767471259, 0.5740699500044821, 0.5734313287592384, 0.5727955002690797, 0.5721624518324647, 0.5715321707886927, 0.5709046445179682, 0.570279860441462, 0.5696578060213716, 0.5690384687609782, 0.5684218362046999, 0.5678078959381454, 0.5671966355881632, 0.5665880428228882, 0.5659821053517874, 0.5653788109257024, 0.5647781473368894, 0.564180102419057, 0.5635846640474028, 0.5629918201386455, 0.5624015586510566, 0.5618138675844898, 0.561228734980407, 0.5606461489219028, 0.5600660975337274, 0.5594885689823064, 0.5589135514757585, 0.5583410332639122, 0.5577710026383192, 0.557203447932266, 0.5566383575207838, 0.5560757198206566, 0.5555155232904261, 0.5549577564303956, 0.5544024077826322, 0.5538494659309661, 0.5532989195009881, 0.5527507571600463, 0.5522049676172394, 0.5516615396234091, 0.5511204619711305, 0.5505817234947001, 0.5500453130701229, 0.5495112196150969, 0.5489794320889957, 0.5484499394928511, 0.5479227308693311, 0.5473977953027188, 0.5468751219188877, 0.5463546998852769, 0.5458365184108631, 0.5453205667461322, 0.5448068341830488, 0.5442953100550232, 0.5437859837368796, 0.5432788446448182, 0.542773882236381, 0.5422710860104114, 0.5417704455070155, 0.5412719503075197, 0.5407755900344292, 0.5402813543513825, 0.539789232963106, 0.5392992156153675, 0.538811292094926, 0.5383254522294836, 0.5378416858876326, 0.5373599829788039, 0.5368803334532123, 0.5364027273018015, 0.5359271545561872, 0.5354536052885994, 0.5349820696118232, 0.5345125376791379, 0.5340449996842562, 0.5335794458612605, 0.5331158664845393, 0.5326542518687218, 0.5321945923686114, 0.5317368783791183, 0.5312811003351908, 0.5308272487117456, 0.5303753140235962, 0.5299252868253822, 0.5294771577114952, 0.5290309173160053, 0.5285865563125856, 0.5281440654144364, 0.527703435374208, 0.5272646569839226, 0.5268277210748952, 0.5263926185176534, 0.5259593402218571, 0.5255278771362155, 0.525098220248406, 0.5246703605849885, 0.5242442892113228, 0.5238199972314819, 0.5233974757881661, 0.5229767160626163, 0.5225577092745257, 0.5221404466819514, 0.5217249195812246, 0.5213111193068606, 0.5208990372314679, 0.5204886647656561, 0.5200799933579439, 0.5196730144946654, 0.5192677196998766, 0.5188641005352608, 0.5184621486000329, 0.5180618555308442, 0.5176632130016854, 0.5172662127237893, 0.516870846445533, 0.5164771059523405, 0.5160849830665818, 0.5156944696474749, 0.515305557590985, 0.5149182388297235, 0.5145325053328477, 0.5141483491059576, 0.5137657621909946, 0.5133847366661388, 0.5130052646457047, 0.5126273382800384, 0.5122509497554122, 0.5118760912939211, 0.5115027551533772, 0.5111309336272029, 0.5107606190443269, 0.510391803769076, 0.5100244802010693, 0.5096586407751098, 0.5092942779610785, 0.5089313842638241, 0.5085699522230571, 0.5082099744132388, 0.5078514434434737, 0.5074943519573994, 0.5071386926330774, 0.5067844581828828, 0.5064316413533934, 0.5060802349252803, 0.5057302317131962, 0.505381624565664, 0.5050344063649664, 0.5046885700270332, 0.5043441085013297, 0.5040010147707452, 0.5036592818514796, 0.5033189027929315, 0.5029798706775854, 0.5026421786208983, 0.5023058197711875, 0.5019707873095158, 0.5016370744495795, 0.5013046744375936, 0.5009735805521788, 0.5006437861042472, 0.5003152844368883, 0.49998806892525455, 0.49966213297644724, 0.4993374700294025, 0.4990140735547757, 0.4986919370548283, 0.49837105406331117, 0.49805141814535203, 0.497733022897339, 0.497415861946806, 0.49709992895231836, 0.4967852176033573, 0.4964717216202046, 0.4961594347538282, 0.4958483507857664, 0.49553846352801345, 0.49522976682290365, 0.49492225454299654, 0.4946159205909616, 0.4943107588994634, 0.4940067634310458, 0.4937039281780176, 0.493402247162337, 0.4931017144354961, 0.49280232407840724, 0.49250407020128606, 0.4922069469435382, 0.4919109484736439, 0.49161606898904264, 0.4913223027160192, 0.49102964390958787, 0.4907380868533796, 0.4904476258595255, 0.49015825526854373, 0.4898699694492247, 0.48958276279851676, 0.48929662974141214, 0.48901156473083307, 0.4887275622475175, 0.4884446167999056, 0.48816272292402624, 0.4878818751833827, 0.48760206816884033, 0.4873232964985124, 0.48704555481764744, 0.48676883779851565, 0.48649314014029726, 0.48621845656896856, 0.48594478183719003, 0.48567211072419425, 0.48540043803567295, 0.4851297586036657, 0.4848600672864477, 0.48459135896841826, 0.48432362855998945, 0.4840568709974743, 0.48379108124297676, 0.48352625428427953, 0.48326238513473435, 0.4829994688331511, 0.48273750044368785, 0.4824764750557401, 0.4822163877838317, 0.48195723376750466, 0.4816990081712101, 0.481441706184199, 0.481185323020413, 0.48092985391837595, 0.4806752941410851, 0.4804216389759032, 0.48016888373445, 0.47991702375249506, 0.4796660543898499, 0.47941597103026007, 0.4791667690812995, 0.47891844397426253, 0.47867099116405776, 0.47842440612910164, 0.478178684371213, 0.47793382141550633, 0.47768981281028716, 0.4774466541269462, 0.4772043409598548, 0.47696286892626033, 0.4767222336661808, 0.4764824308423022, 0.4762434561398735, 0.47600530526660345, 0.47576797395255754, 0.47553145795005425, 0.4752957530335632, 0.47506085499960177, 0.4748267596666339, 0.47459346287496756, 0.4743609604866532, 0.474129248385383, 0.473898322476389, 0.47366817868634337, 0.4734388129632573, 0.4732102212763813, 0.4729823996161051, 0.4727553439938586, 0.472529050442012, 0.47230351501377776, 0.4720787337831115, 0.4718547028446136, 0.4716314183134318, 0.47140887632516315, 0.4711870730357566, 0.47096600462141613, 0.47074566727850475, 0.47052605722344654, 0.47030717069263206, 0.47008900394232156, 0.46987155324855034, 0.4696548149070332, 0.4694387852330693, 0.46922346056144887, 0.46900883724635756, 0.46879491166128373, 0.4685816801989243, 0.46836913927109186, 0.4681572853086218, 0.46794611476127934, 0.46773562409766845, 0.46752580980513814, 0.46731666838969266, 0.46710819637589923, 0.46690039030679753, 0.4666932467438092, 0.46648676226664687, 0.4662809334732252, 0.46607575697957093, 0.46587122941973325, 0.46566734744569516, 0.4654641077272852, 0.46526150695208834, 0.46505954182535875, 0.46485820906993225, 0.4646575054261384, 0.4644574276517137, 0.4642579725217155, 0.46405913682843547, 0.4638609173813131, 0.4636633110068509, 0.46346631454852844, 0.4632699248667177, 0.46307413883859877, 0.4628789533580746, 0.46268436533568796, 0.46249037169853763, 0.4622969693901945, 0.4621041553706192, 0.46191192661607955, 0.4617202801190676, 0.46152921288821824, 0.46133872194822756, 0.46114880433977046, 0.4609594571194218, 0.46077067735957355, 0.46058246214835563, 0.4603948085895557, 0.4602077138025398, 0.4600211749221721, 0.4598351890987368, 0.4596497534978591, 0.45946486530042696, 0.4592805217025129, 0.4590967199152963, 0.4589134571649864, 0.45873073069274484, 0.45854853775460913, 0.45836687562141626, 0.4581857415787267, 0.4580051329267483, 0.4578250469802616, 0.4576454810685441, 0.45746643253529534, 0.4572878987385634, 0.4571098770506701, 0.4569323648581368, 0.4567553595616123, 0.45657885857579794, 0.4564028593293763, 0.4562273592649375, 0.45605235583890824, 0.4558778465214785, 0.45570382879653126, 0.4555303001615703, 0.45535725812765016, 0.45518470021930446, 0.45501262397447695, 0.45484102694445017, 0.4546699066937767, 0.4544992608002094, 0.45432908685463286, 0.45415938246099397, 0.45399014523623377, 0.45382137281021995, 0.45365306282567786, 0.45348521293812427, 0.45331782081579963, 0.4531508841396007, 0.45298440060301504, 0.4528183679120539, 0.4526527837851868, 0.45248764595327573, 0.45232295215950935, 0.45215870015933873, 0.45199488772041224, 0.4518315126225111, 0.45166857265748506, 0.4515060656291889, 0.4513439893534189, 0.45118234165784943, 0.45102112038196984, 0.4508603233770223, 0.4506999485059391, 0.45053999364328046, 0.45038045667517346, 0.4502213354992498, 0.45006262802458474, 0.44990433217163683, 0.44974644587218654, 0.44958896706927665, 0.4494318937171514, 0.449275223781198, 0.4491189552378854, 0.4489630860747073, 0.44880761429012117, 0.4486525378934916, 0.44849785490503, 0.44834356335573833, 0.44818966128735055, 0.4480361467522751, 0.4478830178135378, 0.44773027254472514, 0.44757790902992767, 0.44742592536368325, 0.44727431965092135, 0.4471230900069071, 0.4469722345571858, 0.4468217514375277, 0.4466716387938729, 0.4465218947822768, 0.44637251756885515, 0.4462235053297313, 0.4460748562509801, 0.44592656852857626, 0.4457786403683399, 0.44563106998588353, 0.4454838556065597, 0.44533699546540784, 0.44519048780710246, 0.4450443308859005, 0.4448985229655902, 0.4447530623194389, 0.4446079472301423, 0.44446317598977314, 0.4443187468997307, 0.4441746582706897, 0.44403090842255055, 0.44388749568438945, 0.4437444183944082, 0.4436016748998851, 0.4434592635571253, 0.4433171827314122, 0.44317543079695804, 0.4430340061368565, 0.4428929071430332, 0.4427521322161989, 0.4426116797658004, 0.44247154820997425, 0.44233173597549863, 0.4421922414977465, 0.4420530632206393, 0.4419141995965998, 0.4417756490865056, 0.44163741015964375, 0.44149948129366473, 0.441361860974536, 0.44122454769649777, 0.44108753996201727, 0.4409508362817437, 0.4408144351744639, 0.4406783351670576, 0.44054253479445354, 0.4404070325995851, 0.44027182713334656, 0.4401369169545502, 0.4400023006298821, 0.439867976733859, 0.4397339438487869, 0.43960020056471655, 0.43946674547940207, 0.4393335771982586, 0.43920069433432013, 0.4390680955081979, 0.43893577934803896, 0.43880374448948445, 0.43867198957562864, 0.4385405132569786, 0.43840931419141244, 0.43827839104413974, 0.438147742487661, 0.43801736720172757, 0.43788726387330157, 0.4377574311965171, 0.4376278678726394, 0.4374985726100271, 0.4373695441240926, 0.43724078113726284, 0.4371122823789414, 0.43698404658547, 0.43685607250008984, 0.43672835887290395, 0.43660090446083977, 0.4364737080276106, 0.43634676834367936, 0.4362200841862204, 0.43609365433908326, 0.43596747759275545, 0.4358415527443259, 0.4357158785974487, 0.435590453962307, 0.4354652776555764, 0.43534034850039016, 0.4352156653263022, 0.43509122696925295, 0.4349670322715335, 0.4348430800817505, 0.43471936925479127, 0.4345958986517893, 0.4344726671400897, 0.43434967359321497, 0.4342269168908303, 0.43410439591871053, 0.43398210956870553, 0.4338600567387073, 0.433738236332616, 0.4336166472603069, 0.43349528843759744, 0.43337415878621466, 0.4332532572337616, 0.4331325827136856, 0.43301213416524564, 0.4328919105334803, 0.4327719107691753, 0.43265213382883266, 0.4325325786746377, 0.4324132442744288, 0.4322941296016655, 0.43217523363539734, 0.4320565553602327, 0.4319380937663088, 0.4318198478492604, 0.43170181661018897, 0.4315839990556335, 0.4314663941975394, 0.4313490010532288, 0.43123181864537097, 0.43111484600195227, 0.43099808215624713, 0.4308815261467879, 0.43076517701733685, 0.43064903381685615, 0.4305330955994794, 0.43041736142448284, 0.4303018303562573, 0.43018650146427884, 0.4300713738230812, 0.4299564465122279, 0.4298417186162836, 0.4297271892247868, 0.42961285743222233, 0.42949872233799347, 0.4293847830463948, 0.42927103866658556, 0.4291574883125611, 0.4290441311031281, 0.42893096616187626, 0.4288179926171525, 0.4287052096020337, 0.42859261625430145, 0.42848021171641537, 0.428367995135487, 0.4282559656632543, 0.4281441224560554, 0.4280324646748036, 0.42792099148496127, 0.4278097020565158, 0.4276985955639527, 0.42758767118623187, 0.42747692810676224, 0.42736636551337726, 0.4272559825983097, 0.42714577855816827, 0.4270357525939119, 0.42692590391082663, 0.42681623171850136, 0.4267067352308031, 0.42659741366585435, 0.4264882662460084, 0.4263792921978265, 0.42627049075205414, 0.4261618611435976, 0.4260534026115011, 0.4259451143989237, 0.4258369957531163, 0.42572904592539895, 0.4256212641711378, 0.4255136497497235, 0.425406201924548, 0.4252989199629822, 0.4251918031363544, 0.425084850719928, 0.4249780619928796, 0.4248714362382767, 0.4247649727430572, 0.4246586707980064, 0.42455252969773694, 0.42444654874066645, 0.4243407272289973, 0.4242350644686944, 0.4241295597694653, 0.4240242124447385, 0.42391902181164365, 0.42381398719098984, 0.42370910790724625, 0.4236043832885207, 0.42349981266654024, 0.4233953953766304, 0.42329113075769564, 0.42318701815219883, 0.42308305690614206, 0.4229792463690464, 0.4228755858939326, 0.4227720748373017, 0.4226687125591152, 0.42256549842277624, 0.42246243179511, 0.42235951204634536, 0.422256738550095, 0.42215411068333714, 0.4220516278263967, 0.42194928936292675, 0.4218470946798898, 0.4217450431675389, 0.421643134219401, 0.4215413672322562, 0.4214397416061219, 0.42133825674423353, 0.42123691205302694, 0.42113570694212044, 0.42103464082429753, 0.4209337131154887, 0.4208329232347542, 0.4207322706042667, 0.42063175464929353, 0.42053137479817987, 0.4204311304823318, 0.42033102113619825, 0.42023104619725526, 0.4201312051059884, 0.42003149730587624, 0.4199319222433735, 0.4198324793678948, 0.41973316813179806, 0.4196339879903676, 0.4195349384017989, 0.4194360188271813, 0.4193372287304826, 0.4192385675785329, 0.41914003484100815, 0.41904162999041517, 0.41894335250207515, 0.41884520185410845, 0.41874717752741886, 0.41864927900567767, 0.4185515057753094, 0.4184538573254749, 0.41835633314805776, 0.4182589327376479, 0.4181616555915272, 0.418064501209654, 0.4179674690946489, 0.41787055875177925, 0.41777376968894453, 0.4176771014166625, 0.4175805534480533, 0.4174841252988263, 0.41738781648726453, 0.41729162653421137, 0.41719555496305555, 0.4170996012997173, 0.4170037650726345, 0.416908045812748, 0.41681244305348836, 0.41671695633076183, 0.4166215851829364, 0.4165263291508281, 0.4164311877776878, 0.416336160609187, 0.41624124719340505, 0.4161464470808154, 0.4160517598242721, 0.4159571849789969, 0.4158627221025661, 0.41576837075489737, 0.41567413049823654, 0.415580000897145, 0.41548598151848637, 0.41539207193141425, 0.4152982717073591, 0.41520458042001596, 0.4151109976453311, 0.4150175229614907, 0.4149241559489071, 0.4148308961902074, 0.4147377432702207, 0.4146446967759664, 0.4145517562966407, 0.4144589214236063, 0.4143661917503791, 0.41427356687261674, 0.41418104638810643, 0.4140886298967534, 0.41399631700056916, 0.41390410730365934, 0.4138120004122128, 0.4137199959344891, 0.4136280934808082, 0.41353629266353803, 0.41344459309708326, 0.41335299439787476, 0.41326149618435726, 0.41317009807697913, 0.4130787996981806, 0.41298760067238294, 0.4128965006259775, 0.4128054991873149, 0.412714595986694, 0.412623790656351, 0.41253308283044865, 0.41244247214506624, 0.4123519582381878, 0.4122615407496927, 0.41217121932134443, 0.41208099359678024, 0.411990863221501, 0.41190082784286075, 0.41181088711005637, 0.4117210406741173, 0.4116312881878954, 0.4115416293060554, 0.41145206368506393, 0.4113625909831802, 0.41127321086044566, 0.4111839229786746, 0.4110947270014438, 0.41100562259408313, 0.41091660942366587, 0.41082768715899853, 0.41073885547061195, 0.41065011403075147, 0.4105614625133671, 0.41047290059410474, 0.41038442795029606, 0.41029604426094984, 0.4102077492067422, 0.4101195424700079, 0.4100314237347302, 0.409943392686533, 0.4098554490126709, 0.4097675924020199, 0.40967982254506974, 0.4095921391339137, 0.40950454186224033, 0.4094170304253244, 0.4093296045200182, 0.40924226384474294, 0.4091550080994799, 0.4090678369857618, 0.4089807502066646, 0.4088937474667983, 0.408806828472299, 0.40871999293082023, 0.4086332405515247, 0.4085465710450761, 0.40845998412363005, 0.4083734795008268, 0.4082870568917826, 0.4082007160130813, 0.408114456582767, 0.4080282783203353, 0.40794218094672524, 0.4078561641843119, 0.4077702277568982, 0.40768437138970653, 0.407598594809372, 0.4075128977439336, 0.4074272799228268, 0.4073417410768762, 0.40725628093828736, 0.40717089924063954, 0.40708559571887776, 0.4070003701093057, 0.40691522214957787, 0.40683015157869223, 0.406745158136983, 0.406660241566113, 0.4065754016090665, 0.4064906380101419, 0.4064059505149444, 0.4063213388703789, 0.4062368028246429, 0.4061523421272193, 0.40606795652886907, 0.40598364578162466, 0.40589940963878285, 0.4058152478548974, 0.4057311601857728, 0.405647146388457, 0.4055632062212344, 0.4054793394436192, 0.405395545816349, 0.4053118251013774, 0.4052281770618681, 0.40514460146218717, 0.40506109806789753, 0.4049776666457517, 0.40489430696368545, 0.40481101879081144, 0.40472780189741236, 0.4046446560549351, 0.4045615810359835, 0.4044785766143128, 0.40439564256482297, 0.4043127786635523, 0.40422998468767146, 0.4041472604154769, 0.4040646056263851, 0.4039820201009259, 0.4038995036207367, 0.40381705596855666, 0.40373467692822, 0.40365236628465045, 0.40357012382385526, 0.40348794933291865, 0.403405842599997, 0.40332380341431207, 0.40324183156614524, 0.4031599268468322, 0.40307808904875664, 0.40299631796534485, 0.4029146133910598, 0.4028329751213954, 0.402751402952871, 0.40266989668302605, 0.4025884561104139, 0.4025070810345964, 0.4024257712561389, 0.402344526576604, 0.4022633467985466, 0.4021822317255084, 0.40210118116201204, 0.40202019491355656, 0.40193927278661123, 0.40185841458861077, 0.40177762012794976, 0.4016968892139778, 0.40161622165699334, 0.4015356172682397, 0.4014550758598995, 0.4013745972450887, 0.40129418123785243, 0.4012138276531599, 0.40113353630689874, 0.4010533070158705, 0.40097313959778536, 0.4008930338712572, 0.4008129896557989, 0.40073300677181733, 0.4006530850406078, 0.40057322428435044, 0.4004934243261047, 0.40041368498980395, 0.4003340061002519, 0.40025438748311726, 0.4001748289649286, 0.4000953303730703, 0.4000158915357779, 0.39993651228213256, 0.39985719244205814, 0.3997779318463144, 0.3996987303264943, 0.3996195877150186, 0.39954050384513146, 0.39946147855089587, 0.3993825116671896, 0.3993036030297, 0.3992247524749206, 0.3991459598401458, 0.39906722496346675, 0.39898854768376746, 0.3989099278407199, 0.3988313652747799, 0.3987528598271832, 0.39867441133994086, 0.39859601965583485, 0.3985176846184143, 0.398439406071991, 0.3983611838616359, 0.3982830178331735, 0.39820490783317974, 0.39812685370897605, 0.39804885530862666, 0.39797091248093386, 0.397893025075434, 0.3978151929423939, 0.39773741593280654, 0.39765969389838707, 0.39758202669156895, 0.39750441416550053, 0.39742685617404044, 0.3973493525717541, 0.3972719032139098, 0.3971945079564748, 0.39711716665611185, 0.397039879170175, 0.39696264535670617, 0.39688546507443123, 0.3968083381827565, 0.39673126454176466, 0.39665424401221155, 0.3965772764555221, 0.396500361733787, 0.39642349970975904, 0.39634669024684926, 0.3962699332091239, 0.3961932284613003, 0.39611657586874366, 0.39603997529746343, 0.39596342661411, 0.39588692968597106, 0.3958104843809681, 0.3957340905676531, 0.39565774811520504, 0.3955814568934264, 0.3955052167727403, 0.3954290276241867, 0.3953528893194188, 0.39527680173070023, 0.39520076473090154, 0.3951247781934971, 0.3950488419925615, 0.3949729560027666, 0.39489712009937794, 0.39482133415825227, 0.3947455980558333, 0.39466991166914955, 0.39459427487581034, 0.39451868755400343, 0.394443149582491, 0.39436766084060765, 0.39429222120825635, 0.39421683056590573, 0.3941414887945872, 0.39406619577589164, 0.3939909513919666, 0.39391575552551306, 0.3938406080597824, 0.3937655088785741, 0.3936904578662319, 0.39361545490764127, 0.3935404998882265, 0.3934655926939481, 0.393390733211299, 0.39331592132730286, 0.39324115692950995, 0.39316643990599565, 0.39309177014535646, 0.3930171475367078, 0.39294257196968135, 0.39286804333442177, 0.39279356152158423, 0.3927191264223314, 0.39264473792833154, 0.3925703959317546, 0.3924961003252705, 0.39242185100204563, 0.39234764785574117, 0.3922734907805091, 0.39219937967099106, 0.39212531442231463, 0.3920512949300912, 0.391977321090413, 0.3919033927998511, 0.39182950995545196, 0.39175567245473625, 0.3916818801956948, 0.3916081330767868, 0.39153443099693763, 0.39146077385553557, 0.3913871615524299, 0.3913135939879284, 0.39124007106279424, 0.3911665926782445, 0.3910931587359471, 0.3910197691380185, 0.3909464237870216, 0.3908731225859627, 0.3907998654382898, 0.39072665224788994, 0.3906534829190869, 0.39058035735663854, 0.3905072754657352, 0.39043423715199665, 0.3903612423214703, 0.39028829088062844, 0.3902153827363666, 0.39014251779600057, 0.3900696959672648, 0.3899969171583097, 0.3899241812776996, 0.3898514882344106, 0.3897788379378283, 0.3897062302977457, 0.3896336652243606, 0.389561142628274, 0.38948866242048785, 0.38941622451240265, 0.3893438288158154, 0.3892714752429176, 0.38919916370629326, 0.3891268941189162, 0.3890546663941489, 0.3889824804457397, 0.3889103361878209, 0.3888382335349071, 0.3887661724018924, 0.3886941527040493, 0.38862217435702595, 0.3885502372768446, 0.3884783413798992, 0.3884064865829539, 0.38833467280314055, 0.3882628999579574, 0.3881911679652665, 0.38811947674329217, 0.38804782621061895, 0.3879762162861894, 0.38790464688930304, 0.3878331179396135, 0.38776162935712705, 0.3876901810622008, 0.38761877297554087, 0.3875474050182002, 0.387476077111577, 0.3874047891774129, 0.38733354113779106, 0.38726233291513434, 0.3871911644322036, 0.3871200356120956, 0.38704894637824194, 0.3869778966544063, 0.3869068863646834, 0.38683591543349705, 0.3867649837855984, 0.38669409134606425, 0.3866232380402951, 0.38655242379401356, 0.38648164853326306, 0.3864109121844054, 0.3863402146741198, 0.38626955592940027, 0.3861989358775554, 0.3861283544462053, 0.38605781156328056, 0.3859873071570205, 0.385916841155972, 0.385846413488987, 0.3857760240852215, 0.38570567287413376, 0.385635359785483, 0.38556508474932727, 0.3854948476960225, 0.38542464855622005, 0.3853544872608664, 0.38528436374120034, 0.3852142779287523, 0.38514422975534246, 0.38507421915307904, 0.38500424605435724, 0.38493431039185744, 0.38486441209854383, 0.38479455110766264, 0.38472472735274094, 0.38465494076758533, 0.38458519128627977, 0.38451547884318493, 0.3844458033729362, 0.3843761648104427, 0.38430656309088496, 0.38423699814971474, 0.3841674699226526, 0.38409797834568715, 0.3840285233550729, 0.38395910488732976, 0.38388972287924106, 0.3838203772678521, 0.38375106799046915, 0.38368179498465804, 0.3836125581882426, 0.383543357539303, 0.38347419297617547, 0.3834050644374498, 0.38333597186196866, 0.38326691518882605, 0.383197894357366, 0.3831289093071815, 0.3830599599781128, 0.3829910463102464, 0.3829221682439137, 0.38285332571968966, 0.3827845186783916, 0.3827157470610778, 0.3826470108090466, 0.38257830986383456, 0.3825096441672159, 0.38244101366120054, 0.3823724182880335, 0.3823038579901933, 0.38223533271039095, 0.3821668423915685, 0.3820983869768981, 0.38202996640978054, 0.38196158063384444, 0.3818932295929446, 0.38182491323116097, 0.3817566314927979, 0.3816883843223823, 0.381620171664663, 0.3815519934646092, 0.3814838496674098, 0.3814157402184717, 0.38134766506341916, 0.38127962414809236, 0.38121161741854637, 0.3811436448210501, 0.3810757063020852, 0.3810078018083446, 0.3809399312867321, 0.38087209468436073, 0.3808042919485516, 0.38073652302683325, 0.3806687878669403, 0.3806010864168126, 0.3805334186245937, 0.38046578443863044, 0.3803981838074714, 0.3803306166798659, 0.3802630830047633, 0.38019558273131165, 0.3801281158088568, 0.3800606821869411, 0.3799932818153031, 0.3799259146438758, 0.37985858062278566, 0.3797912797023522, 0.37972401183308646, 0.37965677696569033, 0.3795895750510554, 0.379522406040262, 0.37945526988457806, 0.3793881665354587, 0.37932109594454455, 0.3792540580636615, 0.379187052844819, 0.37912008024020966, 0.37905314020220826, 0.3789862326833705, 0.37891935763643236, 0.37885251501430917, 0.37878570477009443, 0.3787189268570593, 0.37865218122865113, 0.3785854678384933, 0.3785187866403834, 0.3784521375882932, 0.3783855206363672, 0.37831893573892206, 0.37825238285044566, 0.3781858619255958, 0.37811937291919995, 0.3780529157862541, 0.377986490481922, 0.37792009696153384, 0.3778537351805861, 0.3777874050947404, 0.3777211066598223, 0.37765483983182113, 0.3775886045668887, 0.37752240082133837, 0.37745622855164473, 0.3773900877144425, 0.3773239782665252, 0.37725790016484545, 0.3771918533665132, 0.37712583782879533, 0.3770598535091148, 0.37699390036505, 0.37692797835433334, 0.3768620874348515, 0.3767962275646437, 0.3767303987019016, 0.3766646008049677, 0.3765988338323357, 0.37653309774264887, 0.3764673924946993, 0.37640171804742784, 0.37633607435992256, 0.3762704613914185, 0.37620487910129674, 0.3761393274490836, 0.3760738063944501, 0.37600831589721095, 0.37594285591732435, 0.3758774264148904, 0.3758120273501513, 0.3757466586834899, 0.37568132037542934, 0.3756160123866326, 0.37555073467790123, 0.37548548721017494, 0.3754202699445308, 0.3753550828421828, 0.37528992586448084, 0.3752247989729103, 0.3751597021290912, 0.3750946352947776, 0.37502959843185657, 0.3749645915023482, 0.3748996144684045, 0.37483466729230885, 0.37476974993647494, 0.3747048623634469, 0.3746400045358981, 0.3745751764166302, 0.37451037796857356, 0.3744456091547854, 0.3743808699384501, 0.37431616028287806, 0.374251480151505, 0.3741868295078917, 0.37412220831572296, 0.37405761653880765, 0.3739930541410773, 0.3739285210865857, 0.3738640173395088, 0.3737995428641435, 0.37373509762490703, 0.3736706815863372, 0.3736062947130905, 0.37354193696994253, 0.3734776083217871, 0.3734133087336353, 0.37334903817061577, 0.3732847965979729, 0.3732205839810672, 0.37315640028537467, 0.3730922454764856, 0.3730281195201045, 0.3729640223820495, 0.3728999540282516, 0.3728359144247544, 0.37277190353771295, 0.37270792133339425, 0.3726439677781753, 0.37258004283854385, 0.3725161464810971, 0.3724522786725412, 0.3723884393796911, 0.3723246285694697, 0.3722608462089073, 0.3721970922651413, 0.3721333667054155, 0.3720696694970794, 0.37200600060758826, 0.3719423600045017, 0.37187874765548423, 0.37181516352830357, 0.3717516075908315, 0.3716880798110417, 0.37162458015701105, 0.3715611085969177, 0.37149766509904103, 0.3714342496317614, 0.3713708621635596, 0.3713075026630159, 0.3712441710988099, 0.37118086743972045, 0.3711175916546243, 0.37105434371249624, 0.37099112358240827, 0.37092793123352946, 0.37086476663512524, 0.3708016297565569, 0.3707385205672814, 0.37067543903685063, 0.37061238513491074, 0.37054935883120255, 0.3704863600955598, 0.37042338889791004, 0.37036044520827327, 0.3702975289967615, 0.37023464023357894, 0.370171778889021, 0.37010894493347374, 0.3700461383374143, 0.36998335907140917, 0.36992060710611496, 0.36985788241227713, 0.36979518496072994, 0.36973251472239604, 0.3696698716682858, 0.36960725576949716, 0.3695446669972148, 0.3694821053227104, 0.36941957071734144, 0.36935706315255135, 0.3692945825998687, 0.3692321290309073, 0.36916970241736535, 0.36910730273102493, 0.36904492994375193, 0.3689825840274959, 0.36892026495428865, 0.3688579726962449, 0.3687957072255613, 0.36873346851451627, 0.3686712565354696, 0.36860907126086184, 0.3685469126632142, 0.3684847807151278, 0.36842267538928375, 0.36836059665844234, 0.36829854449544297, 0.36823651887320374, 0.3681745197647206, 0.3681125471430679, 0.36805060098139675, 0.36798868125293627, 0.36792678793099154, 0.3678649209889444, 0.36780308040025256, 0.36774126613844943, 0.3676794781771437, 0.36761771649001895, 0.3675559810508334, 0.36749427183341954, 0.36743258881168334, 0.3673709319596047, 0.36730930125123656, 0.36724769666070467, 0.3671861181622069, 0.36712456573001384, 0.36706303933846757, 0.3670015389619813, 0.3669400645750398, 0.3668786161521986, 0.3668171936680831, 0.36675579709738937, 0.36669442641488303, 0.3666330815953991, 0.3665717626138416, 0.3665104694451835, 0.3664492020644666, 0.36638796044679967, 0.36632674456736036, 0.36626555440139374, 0.3662043899242112, 0.3661432511111919, 0.3660821379377811, 0.3660210503794904, 0.36595998841189725, 0.3658989520106447, 0.3658379411514412, 0.3657769558100601, 0.36571599596233934, 0.36565506158418154, 0.36559415265155293, 0.36553326914048423, 0.36547241102706907, 0.3654115782874641, 0.3653507708978896, 0.36528998883462777, 0.3652292320740236, 0.36516850059248385, 0.3651077943664768, 0.3650471133725326, 0.36498645758724235, 0.36492582698725795, 0.36486522154929224, 0.3648046412501178, 0.3647440860665677, 0.3646835559755345, 0.36462305095397046, 0.3645625709788868, 0.36450211602735383, 0.3644416860765005, 0.36438128110351403, 0.36432090108563986, 0.36426054600018126, 0.364200215824499, 0.36413991053601136, 0.3640796301121934, 0.36401937453057714, 0.36395914376875105, 0.36389893780435995, 0.36383875661510456, 0.3637786001787416, 0.36371846847308276, 0.3636583614759955, 0.36359827916540205, 0.3635382215192794, 0.36347818851565916, 0.3634181801326267, 0.36335819634832217, 0.3632982371409386, 0.36323830248872324, 0.3631783923699763, 0.3631185067630508, 0.3630586456463528, 0.362998808998341, 0.362938996797526, 0.36287920902247084, 0.36281944565179025, 0.36275970666415036, 0.3626999920382689, 0.36264030175291473, 0.36258063578690747, 0.36252099411911726, 0.362461376728465, 0.36240178359392167, 0.3623422146945081, 0.3622826700092951, 0.36222314951740286, 0.3621636531980009, 0.36210418103030795, 0.3620447329935912, 0.3619853090671671, 0.36192590923039986, 0.3618665334627026, 0.3618071817435359, 0.36174785405240834, 0.36168855036887604, 0.36162927067254236, 0.3615700149430582, 0.36151078316012075, 0.3614515753034745, 0.3613923913529103, 0.36133323128826506, 0.3612740950894221, 0.36121498273631075, 0.36115589420890537, 0.3610968294872266, 0.36103778855134006, 0.36097877138135614, 0.36091977795743074, 0.36086080825976397, 0.36080186226860067, 0.3607429399642301, 0.36068404132698506, 0.36062516633724295, 0.3605663149754245, 0.36050748722199427, 0.3604486830574597, 0.3603899024623717, 0.36033114541732414, 0.36027241190295345, 0.36021370189993884, 0.36015501538900213, 0.3600963523509066, 0.3600377127664585, 0.3599790966165053, 0.35992050388193614, 0.3598619345436819, 0.3598033885827149, 0.35974486598004796, 0.3596863667167355, 0.3596278907738724, 0.359569438132594, 0.3595110087740764, 0.3594526026795356, 0.35939421983022796, 0.35933586020744956, 0.35927752379253614, 0.3592192105668632, 0.35916092051184545, 0.3591026536089368, 0.3590444098396304, 0.358986189185458, 0.3589279916279902, 0.35886981714883615, 0.3588116657296435, 0.3587535373520977, 0.35869543199792275, 0.35863734964888017, 0.35857929028676944, 0.3585212538934275, 0.3584632404507285, 0.3584052499405842, 0.3583472823449434, 0.3582893376457914, 0.3582314158251508, 0.3581735168650805, 0.3581156407476761, 0.3580577874550692, 0.35799995696942766, 0.35794214927295576, 0.35788436434789284, 0.35782660217651463, 0.3577688627411322, 0.3577111460240917, 0.3576534520077752, 0.3575957806745992, 0.35753813200701545, 0.3574805059875107, 0.35742290259860576, 0.35736532182285663, 0.35730776364285355, 0.35725022804122064, 0.35719271500061645, 0.35713522450373325, 0.35707775653329715, 0.3570203110720683, 0.3569628881028398, 0.35690548760843854, 0.3568481095717245, 0.35679075397559096, 0.3567334208029638, 0.356676110036802, 0.35661882166009734, 0.3565615556558739, 0.3565043120071884, 0.3564470906971299, 0.3563898917088193, 0.35633271502541, 0.35627556063008686, 0.35621842850606694, 0.35616131863659856, 0.3561042310049618, 0.35604716559446814, 0.3559901223884602, 0.35593310137031153, 0.3558761025234274, 0.3558191258312432, 0.3557621712772254, 0.35570523884487104, 0.3556483285177079, 0.35559144027929374, 0.35553457411321693, 0.3554777300030958, 0.3554209079325785, 0.3553641078853439, 0.3553073298450995, 0.35525057379558334, 0.35519383972056257, 0.35513712760383376, 0.355080437429223, 0.35502376918058565, 0.3549671228418057, 0.35491049839679634, 0.3548538958294998, 0.3547973151238867, 0.35474075626395635, 0.3546842192337367, 0.35462770401728416, 0.354571210598683, 0.35451473896204594, 0.3544582890915138, 0.35440186097125514, 0.35434545458546646, 0.35428906991837206, 0.3542327069542236, 0.3541763656773005, 0.3541200460719096, 0.3540637481223846, 0.3540074718130869, 0.35395121712840455, 0.353894984052753, 0.35383877257057406, 0.35378258266633666, 0.35372641432453633, 0.3536702675296951, 0.35361414226636134, 0.35355803851911005, 0.3535019562725421, 0.3534458955112848, 0.3533898562199913, 0.3533338383833408, 0.3532778419860384, 0.35322186701281477, 0.35316591344842635, 0.35310998127765514, 0.35305407048530846, 0.35299818105621883, 0.35294231297524453, 0.35288646622726866, 0.35283064079719934, 0.35277483666996967, 0.3527190538305378, 0.3526632922638865, 0.35260755195502325, 0.35255183288898007, 0.35249613505081356, 0.35244045842560456, 0.35238480299845854, 0.3523291687545049, 0.3522735556788971, 0.352217963756813, 0.3521623929734541, 0.35210684331404585, 0.35205131476383744, 0.3519958073081017, 0.35194032093213534, 0.35188485562125804, 0.35182941136081347, 0.3517739881361681, 0.3517185859327121, 0.3516632047358584, 0.3516078445310432, 0.35155250530372584, 0.35149718703938826, 0.3514418897235352, 0.3513866133416944, 0.35133135787941616, 0.3512761233222732, 0.35122090965586084, 0.3511657168657967, 0.35111054493772087, 0.35105539385729556, 0.351000263610205, 0.350945154182156, 0.3508900655588767, 0.3508349977261176, 0.35077995066965084, 0.35072492437527036, 0.3506699188287919, 0.35061493401605254, 0.3505599699229109, 0.3505050265352476, 0.3504501038389638, 0.3503952018199823, 0.35034032046424746, 0.3502854597577242, 0.3502306196863988, 0.3501758002362785, 0.35012100139339136, 0.35006622314378655, 0.35001146547353357, 0.34995672836872305, 0.34990201181546565, 0.34984731579989325, 0.3497926403081578, 0.3497379853264317, 0.3496833508409077, 0.3496287368377988, 0.34957414330333825, 0.34951957022377933, 0.3494650175853953, 0.3494104853744795, 0.3493559735773451, 0.3493014821803252, 0.34924701116977264, 0.3491925605320597, 0.34913813025357876, 0.3490837203207414, 0.34902933071997866, 0.34897496143774126, 0.348920612460499, 0.34886628377474127, 0.3488119753669764, 0.348757687223732, 0.3487034193315548, 0.3486491716770105, 0.3485949442466837, 0.3485407370271781, 0.3484865500051161, 0.3484323831671385, 0.34837823649990557, 0.3483241099900957, 0.34827000362440597, 0.348215917389552, 0.34816185127226773, 0.34810780525930574, 0.34805377933743664, 0.34799977349344935, 0.34794578771415136, 0.3478918219863679, 0.3478378762969424, 0.34778395063273637, 0.3477300449806292, 0.3476761593275183, 0.3476222936603187, 0.3475684479659633, 0.34751462223140306, 0.347460816443606, 0.3474070305895584, 0.3473532646562635, 0.3472995186307424, 0.3472457925000333, 0.34719208625119224, 0.34713839987129225, 0.3470847333474237, 0.34703108666669413, 0.3469774598162284, 0.3469238527831682, 0.3468702655546724, 0.3468166981179168, 0.34676315046009426, 0.3467096225684144, 0.3466561144301039, 0.3466026260324055, 0.3465491573625796, 0.34649570840790245, 0.3464422791556675, 0.3463888695931844, 0.3463354797077794, 0.3462821094867951, 0.3462287589175908, 0.34617542798754164, 0.3461221166840396, 0.3460688249944925, 0.34601555290632446, 0.34596230040697584, 0.3459090674839029, 0.34585585412457787, 0.3458026603164895, 0.34574948604714184, 0.3456963313040551, 0.34564319607476524, 0.34559008034682415, 0.3455369841077993, 0.345483907345274, 0.345430850046847, 0.34537781220013275, 0.3453247937927613, 0.345271794812378, 0.34521881524664394, 0.3451658550832352, 0.3451129143098436, 0.34505999291417605, 0.34500709088395476, 0.34495420820691725, 0.34490134487081603, 0.3448485008634187, 0.34479567617250817, 0.34474287078588217, 0.34469008469135365, 0.34463731787675006, 0.3445845703299141, 0.3445318420387032, 0.34447913299098953, 0.3444264431746603, 0.3443737725776169, 0.3443211211877759, 0.3442684889930684, 0.34421587598143966, 0.34416328214085007, 0.34411070745927397, 0.3440581519247007, 0.3440056155251333, 0.34395309824859, 0.3439006000831027, 0.34384812101671786, 0.34379566103749615, 0.34374322013351244, 0.34369079829285554, 0.34363839550362885, 0.3435860117539493, 0.3435336470319482, 0.34348130132577087, 0.34342897462357636, 0.34337666691353774, 0.34332437818384187, 0.3432721084226895, 0.3432198576182953, 0.34316762575888743, 0.3431154128327081, 0.34306321882801266, 0.3430110437330706, 0.3429588875361649, 0.3429067502255918, 0.34285463178966136, 0.3428025322166971, 0.34275045149503575, 0.34269838961302757, 0.3426463465590365, 0.34259432232143916, 0.3425423168886258, 0.3424903302490002, 0.3424383623909788, 0.3423864133029917, 0.3423344829734818, 0.3422825713909051, 0.3422306785437312, 0.3421788044204421, 0.3421269490095328, 0.34207511229951176, 0.3420232942788999, 0.34197149493623136, 0.34191971426005296, 0.3418679522389243, 0.3418162088614178, 0.3417644841161187, 0.34171277799162475, 0.3416610904765467, 0.3416094215595076, 0.34155777122914344, 0.3415061394741024, 0.3414545262830456, 0.3414029316446465, 0.34135135554759066, 0.34129979798057686, 0.3412482589323156, 0.3411967383915301, 0.3411452363469557, 0.3410937527873404, 0.341042287701444, 0.34099084107803884, 0.34093941290590957, 0.3408880031738526, 0.34083661187067693, 0.3407852389852035, 0.3407338845062652, 0.34068254842270707, 0.3406312307233864, 0.3405799313971719, 0.3405286504329449, 0.34047738781959824, 0.3404261435460366, 0.3403749176011768, 0.34032370997394734, 0.34027252065328845, 0.3402213496281524, 0.3401701968875029, 0.3401190624203155, 0.3400679462155774, 0.3400168482622874, 0.33996576854945615, 0.3399147070661055, 0.3398636638012694, 0.3398126387439927, 0.3397616318833323, 0.33971064320835614, 0.3396596727081441, 0.3396087203717869, 0.33955778618838695, 0.3395068701470581, 0.3394559722369255, 0.33940509244712536, 0.3393542307668053, 0.3393033871851244, 0.3392525616912526, 0.3392017542743714, 0.33915096492367325, 0.33910019362836147, 0.33904944037765106, 0.3389987051607676, 0.33894798796694803, 0.3388972887854403, 0.3388466076055031, 0.3387959444164064, 0.3387452992074309, 0.33869467196786834, 0.3386440626870213, 0.33859347135420326, 0.3385428979587384, 0.338492342489962, 0.33844180493722, 0.3383912852898687, 0.33834078353727587, 0.3382902996688194, 0.3382398336738882, 0.33818938554188177, 0.33813895526220994, 0.3380885428242936, 0.3380381482175641, 0.3379877714314631, 0.33793741245544306, 0.3378870712789668, 0.33783674789150775, 0.3377864422825496, 0.33773615444158683, 0.33768588435812386, 0.337635632021676, 0.3375853974217685, 0.3375351805479372, 0.33748498138972816, 0.33743479993669756, 0.3373846361784124, 0.33733449010444927, 0.33728436170439535, 0.33723425096784804, 0.3371841578844146, 0.33713408244371285, 0.33708402463537035, 0.3370339844490252, 0.3369839618743252, 0.33693395690092826, 0.33688396951850247, 0.33683399971672606, 0.3367840474852867, 0.33673411281388277, 0.33668419569222197, 0.3366342961100223, 0.33658441405701156, 0.33653454952292733, 0.3364847024975171, 0.33643487297053837, 0.33638506093175813, 0.33633526637095335, 0.3362854892779111, 0.33623572964242765, 0.33618598745430905, 0.3361362627033713, 0.33608655537944027, 0.336036865472351, 0.33598719297194846, 0.3359375378680872, 0.3358879001506313, 0.3358382798094547, 0.33578867683444047, 0.3357390912154814, 0.3356895229424803, 0.33563997200534845, 0.3355904383940076, 0.33554092209838826, 0.33549142310843083, 0.33544194141408507, 0.33539247700530966, 0.33534302987207343, 0.33529360000435393, 0.3352441873921384, 0.33519479202542335, 0.3351454138942143, 0.33509605298852657, 0.3350467092983843, 0.3349973828138212, 0.3349480735248799, 0.33489878142161233, 0.33484950649407974, 0.33480024873235237, 0.33475100812650993, 0.3347017846666407, 0.3346525783428426, 0.3346033891452225, 0.334554217063896, 0.33450506208898834, 0.3344559242106333, 0.3344068034189739, 0.3343576997041622, 0.33430861305635906, 0.33425954346573455, 0.3342104909224674, 0.3341614554167457, 0.3341124369387658, 0.3340634354787335, 0.33401445102686317, 0.3339654835733784, 0.3339165331085111, 0.3338675996225025, 0.3338186831056022, 0.3337697835480688, 0.3337209009401698, 0.3336720352721811, 0.33362318653438783, 0.3335743547170833, 0.3335255398105698, 0.3334767418051585, 0.33342796069116876, 0.3333791964589289, 0.33333044909877585, 0.33328171860105515, 0.3332330049561209, 0.33318430815433564, 0.3331356281860708, 0.3330869650417062, 0.3330383187116301, 0.3329896891862393, 0.3329410764559393, 0.33289248051114384, 0.33284390134227526, 0.33279533893976443, 0.33274679329405044, 0.3326982643955808, 0.33264975223481197, 0.33260125680220787, 0.3325527780882415, 0.33250431608339415, 0.332455870778155, 0.3324074421630222, 0.33235903022850166, 0.33231063496510793, 0.3322622563633636, 0.3322138944137997, 0.3321655491069554, 0.33211722043337827, 0.3320689083836239, 0.3320206129482562, 0.3319723341178472, 0.3319240718829773, 0.33187582623423467, 0.331827597162216, 0.331779384657526, 0.33173118871077717, 0.33168300931259087, 0.33163484645359587, 0.33158670012442903, 0.33153857031573575, 0.3314904570181689, 0.33144236022238993, 0.331394279919068, 0.33134621609888015, 0.3312981687525116, 0.33125013787065555, 0.3312021234440131, 0.33115412546329337, 0.3311061439192132, 0.33105817880249755, 0.3310102301038792, 0.3309622978140989, 0.33091438192390515, 0.33086648242405436, 0.3308185993053109, 0.3307707325584466, 0.3307228821742416, 0.3306750481434837, 0.33062723045696807, 0.33057942910549837, 0.33053164407988544, 0.33048387537094825, 0.3304361229695132, 0.3303883868664146, 0.33034066705249443, 0.3302929635186024, 0.33024527625559585, 0.3301976052543398, 0.33014995050570706, 0.33010231200057794, 0.33005468972984037, 0.3300070836843901, 0.32995949385513, 0.3299119202329712, 0.3298643628088321, 0.32981682157363856, 0.329769296518324, 0.3297217876338297, 0.329674294911104, 0.32962681834110336, 0.32957935791479126, 0.3295319136231386, 0.3294844854571243, 0.32943707340773437, 0.32938967746596215, 0.32934229762280887, 0.3292949338692829, 0.3292475861963999, 0.32920025459518326, 0.32915293905666376, 0.32910563957187916, 0.32905835613187506, 0.3290110887277041, 0.3289638373504266, 0.3289166019911099, 0.32886938264082866, 0.32882217929066526, 0.32877499193170884, 0.32872782055505634, 0.32868066515181155, 0.3286335257130858, 0.32858640222999747, 0.32853929469367266, 0.32849220309524396, 0.3284451274258518, 0.3283980676766436, 0.3283510238387739, 0.32830399590340453, 0.32825698386170454, 0.32820998770485, 0.32816300742402427, 0.32811604301041775, 0.32806909445522825, 0.32802216174966015, 0.3279752448849254, 0.327928343852243, 0.327881458642839, 0.32783458924794634, 0.32778773565880537, 0.32774089786666294, 0.32769407586277377, 0.32764726963839885, 0.3276004791848067, 0.3275537044932726, 0.32750694555507864, 0.32746020236151463, 0.3274134749038764, 0.3273667631734676, 0.3273200671615982, 0.3272733868595856, 0.32722672225875393, 0.32718007335043425, 0.32713344012596446, 0.3270868225766896, 0.32704022069396127, 0.3269936344691383, 0.3269470638935864, 0.3269005089586777, 0.3268539696557916, 0.3268074459763144, 0.32676093791163896, 0.32671444545316497, 0.3266679685922993, 0.32662150732045525, 0.32657506162905303, 0.3265286315095198, 0.3264822169532893, 0.326435817951802, 0.32638943449650537, 0.32634306657885337, 0.3262967141903069, 0.32625037732233364, 0.32620405596640767, 0.32615775011401005, 0.32611145975662825, 0.326065184885757, 0.32601892549289724, 0.3259726815695565, 0.32592645310724944, 0.32588024009749683, 0.32583404253182674, 0.3257878604017732, 0.3257416936988773, 0.32569554241468646, 0.3256494065407549, 0.3256032860686435, 0.3255571809899196, 0.32551109129615696, 0.3254650169789364, 0.3254189580298446, 0.32537291444047545, 0.3253268862024291, 0.32528087330731215, 0.32523487574673793, 0.32518889351232605, 0.32514292659570293, 0.325096974988501, 0.32505103868235974, 0.325005117668925, 0.3249592119398486, 0.32491332148678936, 0.3248674463014125, 0.32482158637538927, 0.3247757417003979, 0.3247299122681226, 0.3246840980702544, 0.32463829909849035, 0.32459251534453415, 0.3245467468000959, 0.32450099345689176, 0.3244552553066449, 0.32440953234108416, 0.3243638245519451, 0.3243181319309697, 0.32427245446990605, 0.32422679216050876, 0.32418114499453854, 0.3241355129637627, 0.32408989605995475, 0.32404429427489445, 0.32399870760036775, 0.323953136028167, 0.323907579550091, 0.32386203815794445, 0.32381651184353877, 0.3237710005986911, 0.3237255044152253, 0.32368002328497103, 0.3236345571997645, 0.32358910615144804, 0.32354367013187013, 0.3234982491328856, 0.32345284314635536, 0.3234074521641463, 0.32336207617813206, 0.32331671518019184, 0.3232713691622114, 0.3232260381160824, 0.32318072203370285, 0.3231354209069767, 0.3230901347278143, 0.32304486348813183, 0.32299960717985177, 0.3229543657949026, 0.32290913932521903, 0.32286392776274175, 0.3228187310994175, 0.32277354932719915, 0.32272838243804586, 0.3226832304239224, 0.3226380932768, 0.3225929709886556, 0.32254786355147247, 0.32250277095723967, 0.3224576931979527, 0.3224126302656124, 0.3223675821522262, 0.3223225488498073, 0.3222775303503749, 0.32223252664595414, 0.32218753772857633, 0.3221425635902786, 0.32209760422310396, 0.3220526596191017, 0.3220077297703265, 0.3219628146688397, 0.3219179143067079, 0.3218730286760041, 0.3218281577688069, 0.32178330157720114, 0.32173846009327733, 0.3216936333091318, 0.32164882121686705, 0.3216040238085913, 0.3215592410764186, 0.321514473012469, 0.3214697196088683, 0.3214249808577482, 0.32138025675124626, 0.32133554728150593, 0.3212908524406765, 0.3212461722209129, 0.321201506614376, 0.32115685561323254, 0.32111221920965516, 0.3210675973958218, 0.3210229901639169, 0.32097839750613033, 0.3209338194146575, 0.32088925588170003, 0.32084470689946504, 0.32080017246016557, 0.32075565255602034, 0.32071114717925364, 0.3206666563220959, 0.3206221799767829, 0.32057771813555636, 0.32053327079066357, 0.3204888379343577, 0.3204444195588976, 0.3204000156565475, 0.32035562621957786, 0.3203112512402645, 0.3202668907108889]\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=5)"]},"metadata":{},"execution_count":40},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"mx-TX8EjSI0h"},"source":["**Step e:** Make predictions on train and test sets and evaluate performance.\n","- Training set prediction\n","- Test set prediction\n","- Performance Evaluation for Training set\n","- Performance Evaluation for Test set\n","\n","Make sure your doctests run without error\n"]},{"cell_type":"code","metadata":{"id":"bZ3bkA9-hmmG","executionInfo":{"status":"ok","timestamp":1691010155628,"user_tz":420,"elapsed":1046,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"81386cd7-7c35-4547-aeaf-5ea7e4253dd4"},"source":["# Make predictions on the train set.\n","#predictions_train = None\n","\n","predictions_train = perceptron.predict(X_train)\n","\n","# Make predictions on the test set.\n","#predictions_test = None\n","\n","predictions_test = perceptron.predict(X_test)\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(predictions_train)\n","  [[0 0 0 1]]\n","  >>> print(predictions_test)\n","  [[0 1 0 0]]\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=2)"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"Sa5qtGtMh7Yd","executionInfo":{"status":"ok","timestamp":1691010367870,"user_tz":420,"elapsed":197,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"76a11287-413c-45e3-bdc7-ff6b25813e2b"},"source":["# Evaluate performance on the train set.\n","#None\n","train_accuracy = perceptron.evaluate(predictions_train, Y_train)\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(perceptron.evaluate(predictions_train, Y_train))\n","  100.0\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=1)"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"W89-zhAih_rp","executionInfo":{"status":"ok","timestamp":1691010443524,"user_tz":420,"elapsed":457,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"27404827-8157-4523-dffd-1e2c6c37fe12"},"source":["# Evaluate performance on the test set.\n","#None\n","train_accuracy = perceptron.evaluate(predictions_train, Y_test)\n","\n","import doctest\n","\"\"\"\n","  >>> print(perceptron.evaluate(predictions_test, Y_test))\n","  100.0\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=1)"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"vr2dkN7e1UVM"},"source":["### Part 2: Logical OR\n","In this section you will test your framework using the OR logic function. Test your code using the example data in the following code cells. Complete the following steps and refer to the previous lecture units for guidance.\n","- Step a:  Recreate your datasets for the OR function\n","- Step b:  Create the model\n","- Step c:  Train the model\n","- Step d:  Plot and verify the costs\n","- Step e:  Make predictions on train and test sets and evaluate performance\n","\n","Your costs plot should look like this:\n","\n","<br>\n","\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1EvBKPP7GP32MrTXs_YksKRha8uv2AOHO)\n","\n","\n","Make sure your doctests run without error\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"qAOGDxUfgCy3","executionInfo":{"status":"aborted","timestamp":1691008623884,"user_tz":420,"elapsed":33,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Set the input data\n","X_train = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n","\n","# Set the labels, the correct results for the AND operation\n","Y_train = np.array([0, 1, 1, 1])\n","\n","X_test = np.array([[1, 1, 0, 0], [0, 1, 0, 1]])\n","Y_test = np.array([1, 1, 0, 1])\n","\n","Y_train = Y_train.reshape(1, 4)\n","Y_test = Y_test.reshape(1, 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w5Hn50vRhZht","executionInfo":{"status":"aborted","timestamp":1691008623886,"user_tz":420,"elapsed":34,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["np.random.seed(0) #do not change - for grading purposes\n","\n","# Instantiate a simple perceptron.\n","perceptron = None\n","\n","# Build your model.\n","perceptron.build([\n","                  None\n","])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dsC96uYvhbtd","executionInfo":{"status":"aborted","timestamp":1691008623887,"user_tz":420,"elapsed":34,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Fit your model to the training data (remember the fit() method returns a list of costs).\n","# Use 2501 epochs and a learning_rate of 0.01. Don't set anything for the verbose or callback parameters\n","costs = None\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dW9RpllGhfA5","executionInfo":{"status":"aborted","timestamp":1691008623888,"user_tz":420,"elapsed":34,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Call the plotting function.\n","None\n","\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(costs[0], 3))\n","  0.368\n","  >>> print(np.round(costs[75], 3))\n","  0.354\n","  >>> print(np.round(costs[1025], 3))\n","  0.265\n","  >>> print(np.round(costs[1557], 3))\n","  0.235\n","  >>> print(np.round(costs[2500], 3))\n","  0.196\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XD34uYh1iJkr","executionInfo":{"status":"aborted","timestamp":1691008623891,"user_tz":420,"elapsed":37,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Make predictions on the train set.\n","predictions_train = None\n","\n","# Make predictions on the test set.\n","predictions_test = None\n","\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(predictions_train)\n","  [[0 1 1 1]]\n","  >>> print(predictions_test)\n","  [[1 1 0 1]]\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0XtTvktniJoZ","executionInfo":{"status":"aborted","timestamp":1691008623894,"user_tz":420,"elapsed":39,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Evaluate performance on the train set.\n","None\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(perceptron.evaluate(predictions_train, Y_train))\n","  100.0\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VONGYkriiKEu","executionInfo":{"status":"aborted","timestamp":1691008623896,"user_tz":420,"elapsed":40,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Evaluate performance on the test set.\n","None\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(perceptron.evaluate(predictions_test, Y_test))\n","  100.0\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QFZLOXDC1V-Q"},"source":["### Part 3: Logical XOR\n","In this section you will test your framework using the exclusive-or (XOR) logic function. Test your code using the example data in the following code cells.  You will first recreate the steps in the previous subsections using a single node perceptron model, then will repeat the steps using a multi-layer perceptron model.   Complete the following steps and refer to the previous lecture units for guidance.\n","- Step a:  Recreate your datasets for the XOR function\n","- Step b:  Create the model\n","- Step c:  Train the model\n","- Step d:  Plot and verify the costs\n","- Step e:  Make predictions on train and test sets and evaluate performance\n","\n","\n","Your costs plot for the simple Perceptron model should look like this:\n","\n","<br>\n","\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1ksrc4rG-JrhIEN_B3eCxByfuUlGbEVUU)\n","\n","\n","<br>\n","\n","Your costs for the MultiLayer Perceptron model should look like this:\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1KOftXWKIty0BrkSWnBg6Jl8QOkbxeqHO)\n","\n","\n","Make sure your doctests run without errors.\n","\n","<br>\n","\n","WHEN YOU COMPLETE THIS SECTION - STOP, AND UPLOAD YOUR COLAB SCRIPT FOR THIS ASSIGNMENT. THE OTHER PARTS IN THE COLAB SCRIPT WILL BE FOR SUBSEQUENT PROJECT/ASSIGNMENTS"]},{"cell_type":"markdown","source":["#### 3a: Perceptron Model\n"],"metadata":{"id":"a_nVikCTvzPF"}},{"cell_type":"code","metadata":{"id":"XnYRkPvQixlR","executionInfo":{"status":"aborted","timestamp":1691008623898,"user_tz":420,"elapsed":41,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Set the input data\n","X_train = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n","\n","# Set the labels, the correct results for the xor operation\n","Y_train = np.array([0, 1, 1, 0])\n","\n","X_test = np.array([[1, 1, 0, 0], [0, 1, 0, 1]])\n","Y_test = np.array([1, 0, 0, 1])\n","\n","Y_train = Y_train.reshape(1, 4)\n","Y_test = Y_test.reshape(1, 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BMO7hcPzjXZg","executionInfo":{"status":"aborted","timestamp":1691008623901,"user_tz":420,"elapsed":1127,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Instantiate a simple perceptron.\n","np.random.seed(0) #do not change - for grading purposes\n","\n","perceptron = None\n","\n","# Build your model.\n","perceptron.build([\n","                  None\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KL0Y8t4CjguA","executionInfo":{"status":"aborted","timestamp":1691008623902,"user_tz":420,"elapsed":1121,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Fit your model to the training data (remember the fit() method returns a list of costs).\n","# Use 2501 epochs and a learning_rate of 0.0075. Don't set anything for the verbose or callback parameters\n","\n","costs = None\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8vOU36GW0q2_","executionInfo":{"status":"aborted","timestamp":1691008623903,"user_tz":420,"elapsed":1116,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Call the plotting function.\n","None\n","\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(costs[0], 3))\n","  0.909\n","  >>> print(np.round(costs[75], 3))\n","  0.862\n","  >>> print(np.round(costs[1025], 3))\n","  0.721\n","  >>> print(np.round(costs[1557], 3))\n","  0.712\n","  >>> print(np.round(costs[2500], 3))\n","  0.703\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cJrMbiIL0kAR","executionInfo":{"status":"aborted","timestamp":1691008623906,"user_tz":420,"elapsed":1112,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Make predictions on the train set.\n","predictions_train = None\n","\n","# Make predictions on the test set.\n","predictions_test = None\n","\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(predictions_train)\n","  [[0 0 1 1]]\n","  >>> print(predictions_test)\n","  [[1 1 0 0]]\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WnvAcNkzih2B","executionInfo":{"status":"aborted","timestamp":1691008623909,"user_tz":420,"elapsed":1109,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Evaluate performance on the train set.\n","None\n","\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(perceptron.evaluate(predictions_train, Y_train))\n","  50.0\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xa6dunhgiiXT","executionInfo":{"status":"aborted","timestamp":1691008623910,"user_tz":420,"elapsed":1104,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Evaluate performance on the test set.\n","None\n","\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(perceptron.evaluate(predictions_test, Y_test))\n","  50.0\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SHGXL1jiioBx"},"source":["#### 3b: Multi-Layer Model\n"]},{"cell_type":"code","metadata":{"id":"szGUcZZ34pkb","executionInfo":{"status":"aborted","timestamp":1691008623912,"user_tz":420,"elapsed":1100,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Instantiate a MultiLayer perceptron.\n","\n","l_layer = Model()\n","\n","# Build your model.\n","# Set up 3 Dense objects with input layer set to 2 features and 4 examples\n","# corresponding to the function truth table, 1 hidden layer with 2 nodes,\n","# and output layer with 1 node. Set the output layer activation to\n","# sigmoid and the other layers to relu.\n","np.random.seed(0) #do not change - for grading purposes\n","L1 = None\n","\n","np.random.seed(0) #do not change - for grading purposes\n","L2 = None\n","\n","np.random.seed(0) #do not change - for grading purposes\n","Lout = None\n","\n","#Build your model with L1, L2, and Lout\n","l_layer.build([\n","               None,\n","               None,\n","               None\n","])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vSI9TqWm4-Rv","executionInfo":{"status":"aborted","timestamp":1691008623913,"user_tz":420,"elapsed":1095,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Fit your model to the training data (remember the fit() method returns a list of costs).\n","# Use 2501 epochs and a learning_rate of 0.0075. Don't set anything for the verbose or callback parameters\n","costs = l_layer.fit(X_train, Y_train, 2501, 0.0075)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2T5Vg_a_DEYC","executionInfo":{"status":"aborted","timestamp":1691008623914,"user_tz":420,"elapsed":1089,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["#Confirm the weights and biases after training\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(L1.getWeights()[0][0], 3))\n","  1.157\n","  >>> print(np.round(L1.getWeights()[1][1], 3))\n","  1.568\n","  >>> print(np.round(L1.getBiases()[1][0], 3))\n","  -1.545\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P_0ead6j5Dqj","executionInfo":{"status":"aborted","timestamp":1691008623915,"user_tz":420,"elapsed":1082,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Call the plotting function.\n","None\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(costs[0], 3))\n","  3.361\n","  >>> print(np.round(costs[75], 3))\n","  0.869\n","  >>> print(np.round(costs[1025], 3))\n","  0.637\n","  >>> print(np.round(costs[1557], 3))\n","  0.461\n","  >>> print(np.round(costs[2500], 3))\n","  0.187\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GZGyPl9o5F0e","executionInfo":{"status":"aborted","timestamp":1691008623919,"user_tz":420,"elapsed":1076,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Make predictions on the train set.\n","predictions_train = None\n","\n","# Make predictions on the test set.\n","predictions_test = None\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(predictions_train)\n","  [[0 1 1 0]]\n","  >>> print(predictions_test)\n","  [[1 0 0 1]]\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wAkybxdmjhZJ","executionInfo":{"status":"aborted","timestamp":1691008623920,"user_tz":420,"elapsed":1071,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Evaluate performance on the train set.\n","None\n","\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(l_layer.evaluate(predictions_train, Y_train))\n","  100.0\n","\"\"\"\n","\n","doctest.testmod()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xIHAA3h25J7P","executionInfo":{"status":"aborted","timestamp":1691008623921,"user_tz":420,"elapsed":1066,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"source":["# Evaluate performance on the test set.\n","None\n","\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(l_layer.evaluate(predictions_test, Y_test))\n","  100.0\n","\"\"\"\n","\n","doctest.testmod()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y5rqbKwYb-3D"},"source":["#Project 2.2b:  Binary Image Classification\n","\n","Note: You should restart the runtime and rerun the initialization steps (drive mount, imports, and function cells) the properly set up the environment before running these steps.\n","\n","In this project you will use your framework to classify image data as either a cat or non-cat image.\n","You will build, train, and evaluate the performance of a simple Perceptron, then extend it to a MultiLayer Perceptron.\n","Record and report the following performance parameters.\n","- Speed of model convergence, i.e. the number of epochs required to achieve an acceptable loss (costs). Use < 0.2 as an acceptable loss metric.\n","- Train and test performance.  Reflect on this result.  What do the performance metrics imply for your model?"]},{"cell_type":"markdown","metadata":{"id":"jpSe3AKFk5kc"},"source":["### Part 1: Simple Perceptron Model\n","In this section you will test your framework to classify real image data using a simple perceptron model. Test your code using the example data in the following code cells. Complete the following steps and refer to the previous lecture units for guidance.\n","\n","<br>\n","\n","Your costs plot should like this this:\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1cikFPozNOf6gQOcmeuB_cA0rb8IfUCCV)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FiAOjNWCllU4"},"source":["**Step a:**  Setup the datasets\n","- Run the following code cell to setup your training and test datasets.\n","- Use the load_h5py_data method to load the complete dataset.  The function also allows you to index one of the images and display the output on your console.\n","- Print out the shapes using the see_shapes() method.  You should see the following output for the shapes:\n","  - Number of training examples: m_train =  209\n","  - Number of testing examples: m_test =  50\n","  - X_train shape:  (209, 64, 64, 3)\n","  - Y_train shape:  (1, 209)\n","  - X_test shape:  (50, 64, 64, 3)\n","  - Y_test shape:  (1, 50)\n","- Flatten your images using the flatten() method, and print out the shapes of the flattened images.  You should see the following output:\n","  - Flattened X_train shape: (12288, 209)\n","  - Y_train shape: (1, 209)\n","  - Flattened X_test shape: (12288, 50)\n","  - Y_test shape: (1, 50)\n","- Normalize the image data using the normalize() method"]},{"cell_type":"code","metadata":{"id":"aTJHRGurOLY9"},"source":["# Load image data.\n","X_train, Y_train, X_test, Y_test = load_h5py_data(5, show_example=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GKkpeqA2S85w"},"source":["# Call the see_shapes() function.\n","None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8oVhBsm-S88c"},"source":["# Call the flatten() function.\n","X_train, X_test = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"01VT0YBAS8_i"},"source":["# Call the normalize() function.\n","X_train, X_test = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k4ql9N8tjBqj"},"source":["**Step b:**  Create a simple perceptron model\n","Setup 1 Dense layer consisting of a single instance of the Dense class with 1 output unit, and a number of input units equal to the number of features in X.\n","Set the activation function to sigmoid."]},{"cell_type":"code","metadata":{"id":"XE06sOu-hKAf"},"source":["### BEGIN CODE HERE\n","\n","# Instantiate a simple perceptron.\n","perceptron = None\n","\n","np.random.seed(3) #do not change for grading purposes\n","\n","# Build your model.\n","perceptron.build([\n","                  None\n","])\n","\n","### END CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5bjAke9Jnnt6"},"source":["**Step c:**  Train the model using the fit() method\n","Use 2501 epochs and a learning_rate of 0.0075.\n","Donât worry about passing anything for verbose or callback.\n"]},{"cell_type":"code","metadata":{"id":"s3DOxzFDhKDv"},"source":["### BEGIN CODE HERE\n","\n","# Fit your model to the training data (remember the fit() method returns a list of costs).\n","# Use 2501 epochs and a learning_rate of 0.0075. Donât pass anything for verbose or callback.\n","costs = None\n","\n","### END CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bdaQGhb2n0u_"},"source":["**Step d:**  Plot the costs\n","Make sure your doctests pass without errors."]},{"cell_type":"code","metadata":{"id":"1XbP8crR8SPr"},"source":["### BEGIN CODE HERE\n","\n","# Call the plotting function.\n","None\n","\n","### END CODE HERE\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(costs[0], 3))\n","  0.74\n","  >>> print(np.round(costs[75], 3))\n","  0.543\n","  >>> print(np.round(costs[1025], 3))\n","  0.153\n","  >>> print(np.round(costs[1557], 3))\n","  0.114\n","  >>> print(np.round(costs[2500], 3))\n","  0.078\n","\"\"\"\n","\n","doctest.testmod()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j2ZCO9yvyk5y"},"source":["**Step e:**  Make predictions on training and test datasets, using the perceptronâs âpredictâ method.\n","Make sure your doctests pass without errors.\n"]},{"cell_type":"code","metadata":{"id":"EkPlQdGAhKM6"},"source":["### BEGIN CODE HERE\n","\n","# Make predictions on the train set.\n","predictions_train = None\n","\n","# Make predictions on the test set.\n","predictions_test = None\n","\n","### END CODE HERE\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(predictions_train[0][17])\n","  0\n","  >>> print(predictions_train[0][50])\n","  1\n","  >>> print(predictions_test[0][15])\n","  1\n","  >>> print(predictions_test[0][6])\n","  0\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z6iHAaoLzwZt"},"source":["**Step f:** Evaluate performance of the training and test datasets\n","Make sure your doctests pass without errors.\n"]},{"cell_type":"code","metadata":{"id":"RSWryWctiT6t"},"source":["### BEGIN CODE HERE\n","\n","# Evaluate performance on the train set.\n","None\n","\n","### END CODE HERE\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(perceptron.evaluate(predictions_train, Y_train), 3))\n","  99.522\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6kBcarqijCa"},"source":["### BEGIN CODE HERE\n","\n","# Evaluate performance on the test set.\n","None\n","\n","### END CODE HERE\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(perceptron.evaluate(predictions_test, Y_test), 3))\n","  66.0\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UIvgC9D8jEgk"},"source":["### Part 2: Two-layer Model\n","In this section you will test your framework to classify real image data using a multi-layer model, we'll start with 2 layers. Test your code using the example data in the following code cells. Complete the following steps and refer to the previous lecture units for guidance.\n","\n","\n","**Note**, you will need to reload the images and rerun the preprocessing steps when switching the model.\n","\n","\n","<br>\n","\n","Your costs plot should like this this:\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1LvxtdMkJnpEadJw1I2i8axxUuikXlxj4)\n"]},{"cell_type":"markdown","metadata":{"id":"VaaQ-HPW2YtR"},"source":["**Step a:**  Create a two-layer model\n","Setup 2 Dense layers.  When building the network, use a hidden layer (ie the first layer) with 7 output units, a number of input units equal to the number of features in X, and a relu activation function.\n","Use a simple perceptron as your output layer.  Remember to set the input units for the output layer equal to the output units of the first layer, and use a sigmoid activation function in the output layer.\n"]},{"cell_type":"code","metadata":{"id":"JGi6qjsAjH8C"},"source":["### BEGIN CODE HERE\n","\n","# Instantiate your model.\n","two_layer = None\n","\n","np.random.seed(5)  #do not change for grading purposes\n","\n","# Build a model with one hidden layer.\n","two_layer.build([\n","               None,\n","               None\n","])\n","\n","### END CODE HERE\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4wRP0naI5TfX"},"source":["**Step b:**  Train the model\n","Call the âfitâ method, and use the same number of epochs and learning rate  as with the simple perceptron model to get a list of costs, then plot them to visualize training.\n","Make sure your doctests pass without errors."]},{"cell_type":"code","metadata":{"id":"wf628Ra8jH-u"},"source":["### BEGIN CODE HERE\n","\n","# Fit your model to the training data.\n","# Use 2501 epochs and a learning_rate of 0.0075. Donât pass anything for verbose or callback.\n","costs = None\n","\n","### END CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DEUrBlRA8dLN"},"source":["### BEGIN CODE HERE\n","\n","# Plot the costs.\n","None\n","\n","### END CODE HERE\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(costs[0], 3))\n","  0.958\n","  >>> print(np.round(costs[75], 3))\n","  0.543\n","  >>> print(np.round(costs[1025], 3))\n","  0.137\n","  >>> print(np.round(costs[1557], 3))\n","  0.048\n","  >>> print(np.round(costs[2500], 3))\n","  0.021\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z0iObIF06xSn"},"source":["**Step c:** Evaluate performance of the training and test datasets\n","Make sure your doctests pass without errors.\n"]},{"cell_type":"code","metadata":{"id":"G9XPURNRjIBT"},"source":["### BEGIN CODE HERE\n","\n","# Make predictions on the train set.\n","predictions_train = None\n","\n","# Make predictions on the test set.\n","predictions_test = None\n","\n","### END CODE HERE\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(predictions_train[0][17])\n","  0\n","  >>> print(predictions_train[0][50])\n","  1\n","  >>> print(predictions_test[0][15])\n","  1\n","  >>> print(predictions_test[0][6])\n","  0\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pfw_XkGOjIEQ"},"source":["### BEGIN CODE HERE\n","\n","# Evaluate performance on the train set.\n","None\n","\n","### END CODE HERE\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(two_layer.evaluate(predictions_train, Y_train), 3))\n","  99.522\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9DIC4M_RjIIM"},"source":["### BEGIN CODE HERE\n","\n","# Evaluate performance on the test set.\n","None\n","\n","### END CODE HERE\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(two_layer.evaluate(predictions_test, Y_test), 3))\n","  76.0\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ReaD5II5kO99"},"source":["### Part 3: L-layer (multi-layer) Model\n","In this section you will experiment and test your framework to classify real image data using a multi-layer model.  Test your code using the example data in the following code cells. Complete the following steps and refer to the previous lecture units for guidance.\n","Try different 'hyperparameter' configurations, i.e. a different number of layers and nodes per layer. Your final configuration show optimize the train and test performance without having too large of a network (try to avoid having too many nodes or too many layers as this could result in overfitting).\n","Your final network configuration should converge to below 0.1 in around 1000 epochs, and have a training score above 95% and a test score of at least 80%.\n","Hints:\n","- The performance requirement is attainable with a 4-layer network, i.e. 3 hidden layers and 1 output layer (the input nodes do not count as a layer).\n","- Use Sigmoid activation for the output layer, and ReLu for all other layers.\n","\n","**Note**, you will need to reload the images and rerun the preprocessing steps when switching the model.\n","\n","Reflect on your model and its performance.  What were your hyperparameters set to, i.e. the number of layers, and number of nodes per layer?  What was the model train and test performance?"]},{"cell_type":"markdown","metadata":{"id":"ihl5EKfa8Woy"},"source":["**Step a:**  Create your MultiLayer Perceptron model\n"]},{"cell_type":"code","metadata":{"id":"eZiSSO38kUKC"},"source":["# Instantiate your model.\n","l_layer = None\n","\n","np.random.seed(3)  #do not change for grading purposes\n","\n","\n","l_layer.build([ None\n","])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XXP2R5yF84sk"},"source":["**Step b:**  Train the model\n","Call the âfitâ method, and use the same number of epochs and learning rate  as with the simple perceptron model to get a list of costs, then plot them to visualize training.\n","\n","WHEN YOU COMPLETE THIS SECTION - STOP, AND UPLOAD YOUR COLAB SCRIPT FOR THIS ASSIGNMENT. THE OTHER PARTS IN THE COLAB SCRIPT WILL BE FOR SUBSEQUENT PROJECT/ASSIGNMENTS"]},{"cell_type":"code","metadata":{"id":"HGpGVAIQkUvN"},"source":["# Fit your model to the training data.\n","# Use 2501 epochs and a learning_rate of 0.0075. Donât pass anything for verbose or callback.\n","\n","costs = None\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yL4Ekk6R8j1e"},"source":["# Plot the costs.\n","\n","None\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jeE2z7DTkUyi"},"source":["# Make predictions on the train set.\n","predictions_train = None\n","\n","# Make predictions on the test set.\n","predictions_test = None\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cD7DFBdDkU2c"},"source":["# Evaluate performance on the train set.\n","None\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NbOZ4ciUk5VU"},"source":["# Evaluate performance on the test set.\n","None\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TukR7jsU52iQ"},"source":["# Project 2.2a:  Binary Classification on Planar Data\n","\n","In this project, weâll continue the use of the multi-layer perceptron framework you created to perform a more complex classification task.\n","\n","In this section, we'll show how you can generalize the framework and apply it to binary classification on planar data.  This task will demonstrate the capabilities of your framework to classify data with non-linear decision boundaries similar to what was seen with the XOR gate.\n","\n","<br>\n","\n","**Note:** You should restart the runtime and rerun the initialization steps (drive mount, imports, and function cells) to properly set up the environment before running these steps.\n"]},{"cell_type":"markdown","metadata":{"id":"gvPd8uYiYO0-"},"source":["**Part A:**  Data set up.\n","\n","Run the following code cells to create the dataset and make_plot function, then call make_plot() with a suitable title to plot it.  The dataset is generated using the sklearn make_moons function.\n","\n","You should see a distribution of points in a plane.\n","The points are either red or blue, and you will be building a model to define a decision boundary which separates the points based on color.\n","Run the \"Reshaping\" cell then call see_shapes() to see more information about your data.  You should see the following shapes:\n","- Number of training examples: m_train =  2\n","- Number of testing examples: m_test =  2\n","- X_train shape:  (2, 900)\n","- Y_train shape:  (1, 900)\n","- X_test shape:  (2, 100)\n","- Y_test shape:  (1, 100)\n","\n","Each example has two features (that is, the x and y coordinates in the plane) and one label (that is,  the color of the point).\n","\n","<br>\n","\n","[Credit for visualizations](https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795)\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"uzFOanXE3jxw"},"source":["sns.set_style(\"whitegrid\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yEVVGTE03S48"},"source":["# number of samples in the data set\n","N_SAMPLES = 1000\n","\n","X, Y = make_moons(n_samples = N_SAMPLES, noise=0.2, random_state=100)\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPtjmeVt3S8L"},"source":["# The plot function making up the graph of a dataset\n","def make_plot(X, y, plot_name, file_name=None, XX=None, YY=None, preds=None, dark=False):\n","    if (dark):\n","        plt.style.use('dark_background')\n","    else:\n","        sns.set_style(\"whitegrid\")\n","    plt.figure(figsize=(16,12))\n","    axes = plt.gca()\n","    axes.set(xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n","    plt.title(plot_name, fontsize=30)\n","    plt.subplots_adjust(left=0.20)\n","    plt.subplots_adjust(right=0.80)\n","    if(XX is not None and YY is not None and preds is not None):\n","        plt.contourf(XX, YY, preds.reshape(XX.shape), 25, alpha = 1, cmap=cm.Spectral)\n","        plt.contour(XX, YY, preds.reshape(XX.shape), levels=[.5], cmap=\"Greys\", vmin=0, vmax=.6)\n","    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=40, cmap=plt.cm.Spectral, edgecolors='black')\n","    if(file_name):\n","        plt.savefig(file_name)\n","        plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0DTp9bTe-RKN"},"source":["# boundary of the graph\n","GRID_X_START = -1.5\n","GRID_X_END = 2.5\n","GRID_Y_START = -1.0\n","GRID_Y_END = 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N0s1VbKm-RNQ"},"source":["# Create grid.\n","grid = np.mgrid[GRID_X_START:GRID_X_END:100j,GRID_X_START:GRID_Y_END:100j]\n","grid_2d = grid.reshape(2, -1).T\n","XX, YY = grid"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot your dataset.\n","Your plot should look like this:\n","\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1ZKt-ju3qJBFV-nd9FJwJ7Rom4GW7Qd0U)\n"],"metadata":{"id":"VkXnrs_ctdvS"}},{"cell_type":"code","metadata":{"id":"-xauNrK83S-C"},"source":["# Call make_plot (pass X, Y, and the name of the plot only).\n","make_plot(X, Y, \"Dataset\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9OiKHVZA8VRb"},"source":["# Reshaping.\n","X_train = X_train.T\n","X_test = X_test.T\n","Y_train = Y_train.reshape(1, Y_train.shape[0])\n","Y_test = Y_test.reshape(1, Y_test.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4TkqdROi8q8n"},"source":["# Call see_shapes().\n","see_shapes(X_train, X_test, Y_train, Y_test)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cPj53CJggsS2"},"source":["### Part 1:  Simple Perceptron Model\n","\n","First we'll see how well a simple perceptron can do.\n","Instantiate your âplanar perceptronâ model as a simple perceptron.\n","This should be the same as the perceptron you made previously, but it's important that we create a new object as we need to rebuild the model to have the right input shape and reinitialized parameters.\n","Follow the same procedure as you did in the previous project to build your model and fit it to the data.  Use 10,000 epochs and a learning rate of 0.01.\n","Plot your costs and make predictions on the train and test sets, then evaluate performance.\n","You should see that a simple perceptron classifies the data with around 87% accuracy at test time.\n","The final cell in the simple perceptron section plots the decision boundary as decided by the model post-training, and saves it as a png file.  Note that you can simply plot the output to the console by omitting the âfile nameâ parameter.\n","If youâve defined a file name, run the cell then find the file in the file system of your colab notebook, and double click it to view in a scratch cell.\n","As we discussed in a previous unit in this course and previous project in this script, you should see that the perceptron model gives a linear decision boundary (that is a straight line boundary).  However, note that since our dataset is not linearly separable, this isnât good enough."]},{"cell_type":"markdown","metadata":{"id":"rDUuRiJpa2Ta"},"source":["**Step a:** Create a simple perceptron model\n","Setup 1 Dense layer consisting of a single instance of the Dense class with 1 output unit, and a number of input units equal to the number of features in X.\n","Set the activation function to sigmoid."]},{"cell_type":"code","metadata":{"id":"0pQHq_-1fpn-"},"source":["# Instantiate your model.\n","np.random.seed(0) #do not change - for grading purposes\n","\n","planar_perceptron = None\n","\n","# Build a simple perceptron model.\n","# Setup 1 Dense layer consisting of a single instance of the Dense class\n","# with 1 output unit, and a number of input units equal to the\n","# number of features in X. Set the activation function to sigmoid.\n","planar_perceptron.build([\n","                         None\n","])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ABzCtS8AbLxy"},"source":["**Step b:** Train the model using the fit() method.\n","Use 10000 epochs and a learning_rate of 0.01.\n","Donât worry about passing anything for verbose or callback."]},{"cell_type":"code","metadata":{"id":"-OfSSE0-f2VC"},"source":["# Fit your model to the training data.\n","# Use 10000 epochs and a learning_rate of 0.01. Donât pass anything for verbose or callback.\n","costs = None\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eEoqPyU3bfni"},"source":["**Step c:** Plot the costs.\n","Make sure your doctests pass without errors.\n","Note, the plot() method was written in the Model Class section.\n","\n","Your costs plot should look like this:\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1ORSa9MM8xIVQl68Zl-oLwnUDbfE1e3qE)\n","\n","\n","Make sure your doctests run without error\n"]},{"cell_type":"code","metadata":{"id":"2ECYXrnEgJMg"},"source":["# Plot the costs.\n","None\n","\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(costs[0], 3))\n","  0.636\n","  >>> print(np.round(costs[75], 3))\n","  0.594\n","  >>> print(np.round(costs[1025], 3))\n","  0.415\n","  >>> print(np.round(costs[1557], 3))\n","  0.383\n","  >>> print(np.round(costs[2500], 3))\n","  0.349\n","  >>> print(np.round(costs[5000], 3))\n","  0.308\n","  >>> print(np.round(costs[7500], 3))\n","  0.292\n","  >>> print(np.round(costs[9999], 3))\n","  0.285\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fksZiCpSsV1L"},"source":["**Step d:**  Make predictions on train and test sets and evaluate performance.\n","Make sure your doctests pass without errors."]},{"cell_type":"code","metadata":{"id":"X9Wru97_ga2U"},"source":["# Make predictions on the train set.\n","predictions_train = None\n","\n","# Make predictions on the test set.\n","predictions_test = None\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(predictions_train[0][17])\n","  0\n","  >>> print(predictions_train[0][50])\n","  1\n","  >>> print(predictions_test[0][15])\n","  1\n","  >>> print(predictions_test[0][6])\n","  0\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghwM3XkZggXt"},"source":["# Evaluate performance on the train set.\n","None\n","\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(planar_perceptron.evaluate(predictions_train, Y_train), 3))\n","  87.444\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nwFUDYi4gloW"},"source":["# Evaluate performance on the test set.\n","None\n","\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(planar_perceptron.evaluate(predictions_test, Y_test), 3))\n","  83.0\n","\"\"\"\n","\n","doctest.testmod()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kwshs3cZxN-X"},"source":["**Step e:** Plot the dataset along with the predicted outputs and decision boundary.\n","\n","Your classification and decision boundary plot should look like this:\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1Tz6QCMqrwoIz6kjkcbK-9E3NHHbAz2vQ)"]},{"cell_type":"code","metadata":{"id":"135tDeofgKe9"},"source":["preds = planar_perceptron.predict(np.transpose(grid_2d))\n","preds = preds.reshape(preds.shape[1], 1)\n","X_plt = X_test.T\n","Y_plt = Y_test.T\n","\n","# Plot decision boundary.\n","# If a filename is specified, a png file of the plot will be written to your current working directory\n","#   otherwise the plot will be displayed in the console.\n","#make_plot(X_plt, Y_plt, 'Simple Perceptron Model', file_name='Simple Perceptron.png', XX=XX, YY=YY, preds=preds)\n","make_plot(X_plt, Y_plt, 'Simple Perceptron Model', XX=XX, YY=YY, preds=preds)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DxYcWGG6g0Ng"},"source":["### Part 2: L-layer (multi-layer) Model\n","In this section you will experiment and test your framework to classify planar data using a multi-layer model. Test your code using the example data in the following code cells. Complete the following steps and refer to the previous lecture units for guidance. Try different 'hyperparameter' configurations, i.e. a different number of layers and nodes per layer. Your final configuration show optimize the train and test performance without having too large of a network (try to avoid having too many nodes or too many layers as this could result in overfitting).\n","\n","Your final network configuration should converge to below 0.1 in around 5000 epochs, and have both training and test scores above 95%.\n","Hints:\n","\n","The performance requirement is attainable with a 5-layer network, i.e. 4 hidden layers and 1 output layer (the input nodes do not count as a layer).\n","Use Sigmoid activation for the output layer, and ReLu for all other layers.\n","\n","Reflect on your model and its performance. What were your hyperparameters set to, i.e. the number of layers, and number of nodes per layer? What was the model train and test performance?\n","\n","\n","**Note**, you will need to reload the dataset and rerun the preprocessing steps when switching the model.\n","\n","\n","\n"]},{"cell_type":"markdown","source":["**Step a:** Create a MultiLayer perceptron model\n","Setup an L Dense layer with the number of input units equal to the number of features in X.\n","Set the output activation function to sigmoid, and the other layers to ReLu."],"metadata":{"id":"lhsCHDxQh3z5"}},{"cell_type":"code","metadata":{"id":"XoWYDfXJ3TBQ"},"source":["np.random.seed(0) #do not change - for grading purposes\n","\n","# Instantiate your model.\n","planar_net = None\n","\n","# Build a deep neural network.\n","planar_net.build([ None\n","])\n","\n","### END CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JW1zZ3j50zH0"},"source":["**Step b:** Train the model using the fit() method.\n","Use 10000 epochs and a learning_rate of 0.01.\n","Donât worry about passing anything for verbose or callback."]},{"cell_type":"code","metadata":{"id":"vFYabgtx3TDp"},"source":["# Fit your model to the training data.\n","# Use 10000 epochs and a learning_rate of 0.01. Donât pass anything for verbose or callback.\n","costs = None\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0eFjHvjH1VMh"},"source":["**Step c:** Plot the costs.<br>\n","\n","Your costs should look something like this:\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1EKRRYp2_MVgILrQMaI1q7SKlRbiA6gOz)\n","\n"]},{"cell_type":"code","metadata":{"id":"LncSC89w-DN_"},"source":["# Plot the costs.\n","None\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I7EiVmke_aqz"},"source":["**Step d:**  Make predictions on train and test sets and evaluate performance."]},{"cell_type":"code","metadata":{"id":"IoYyWAww3TGw"},"source":["# Make predictions on the train set.\n","predictions_train = None\n","\n","# Make predictions on the test set.\n","predictions_test = None\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNX8GTJN-CaP"},"source":["# Evaluate performance on the train set.\n","None\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3bfHYy-D-L3i"},"source":["# Evaluate performance on the test set.\n","None\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step e:** Plot the dataset along with the predicted outputs and decision boundary.\n","\n","<br>\n","\n","Your classification and decision boundary plot should look like this:\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1g7p7NO8J3ICJNQ9tIK0Nj-lb-rh86SWe)\n"],"metadata":{"id":"kTn74izwcjPi"}},{"cell_type":"code","metadata":{"id":"zV49kzXdD6i6"},"source":["preds = planar_net.predict(np.transpose(grid_2d))\n","preds = preds.reshape(preds.shape[1], 1)\n","X_test = X_test.T\n","Y_test = Y_test.T\n","\n","# Plot decision boundary.\n","make_plot(X_test, Y_test, \"Deep Model\", file_name='Final Decision Boundary for Deep Model.png', XX=XX, YY=YY, preds=preds)\n","#make_plot(X_test, Y_test, \"NumPy Model\", XX=XX, YY=YY, preds=preds)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yNTh7Gb20xkb"},"source":["### Part 3:  Visualize Learning\n","\n","Now visualize the entire training process so you can see the training in action. To do this, weâll be using a callback function.  Reference the lecture unit for more details on the visualization process and callback functions.\n","Letâs take a look at the callback_plotâ function.\n","Most of the code here should be familiar, because this function takes in an index (that is, the epoch number), âX trainâ and âY trainâ, then plots the decision boundary for the model at that epoch.\n","It then saves the .png to the file system in colab.\n","Now return to your colab implementation of the âfitâ method.  Notice that we can pass a callback as an argument to fit().\n","If a callback function has been passed (that is, the  callback parameter is not âNoneâ), then we execute the function on every 50th epoch. This means we will have many .png files saved to our system (for instance, for 2500 epochs, this will be 2500/50 = 50 images.\n","<br>\n","Now, reinitialize your model (to reset the parameters) with the same architecture as before.\n","**Note**, you will need to reload the dataset and rerun the preprocessing steps when switching the model.\n","<br>\n","Fit it to the data and remember to pass callback_plotâ as the callback function, but keep everything else the same. Training should take a little longer (around 5 minutes) since we are visualizing.\n","Once you are done training, take a look at the next cell in your reference Colab script. It's not important to understand the specifics of whatâs happening here, but this cell ties all the .png files together into a single gif.\n","Give your gif variable, \"anim_fileâ, a descriptive name, such as âtraining.gifâ.\n","Then run the cell.\n"]},{"cell_type":"code","metadata":{"id":"iJK_GnDFE4Jf"},"source":["# Callback function to plot decision boundaries during training.\n","def callback_plot(index, X_train, Y_train):\n","    plot_title = \"L Layer Model - It: {:05}\".format(index)\n","    file_name = \"numpy_model_{:05}.png\".format(index//50)\n","    preds = planar_net.predict(np.transpose(grid_2d))\n","    preds = preds.reshape(preds.shape[1], 1)\n","    X_plt = X_train.T\n","    Y_plt = Y_train.T\n","    make_plot(X_plt, Y_plt, plot_title, file_name=file_name, XX=XX, YY=YY, preds=preds, dark=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uwwa6lvUIs7L"},"source":["np.random.seed(0) #do not change - for grading purposes\n","\n","# Instantiate your model.\n","planar_net = None\n","\n","# Build a deep neural network.\n","planar_net.build([ None\n","])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step a:** Create a folder to store the graphics files and change directory to it."],"metadata":{"id":"qloVNiBhjsqK"}},{"cell_type":"code","metadata":{"id":"4a6eBThb-9a0"},"source":["#Change directory to the folder you want to store your files to\n","# e.g.:  %cd drive/MyDrive/Colab\\ Notebooks/MLF-500/ML500_ProjVisualizationFiles/\n","# Use '%cd drive/'your path here...'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oKpjaGjQEFl4"},"source":["# Fit your model to the training data - REMEMBER to pass your callback function to fit().\n","# Use 10000 epochs and learning rate 0.01\n","costs = None\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step b:** Run the following \"stitcher\" code to create an animation comprised of the generated graphics files"],"metadata":{"id":"YjZ9yG2oj2PS"}},{"cell_type":"code","metadata":{"id":"shIdNDOHTnwT"},"source":["# Create a gif.\n","# Credit to Tensorflow tutorials: https://www.tensorflow.org/tutorials/generative/dcgan#create_a_gif\n","import imageio\n","import glob\n","\n","anim_file = 'training.gif'\n","\n","with imageio.get_writer(anim_file, mode='I') as writer:\n","  filenames = glob.glob('numpy_model_*.png')\n","  filenames = sorted(filenames)\n","  last = -1\n","  for i,filename in enumerate(filenames):\n","    frame = 2*(i)\n","    if round(frame) > round(last):\n","      last = frame\n","    else:\n","      continue\n","    image = imageio.imread(filename)\n","    writer.append_data(image)\n","  image = imageio.imread(filename)\n","  writer.append_data(image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BCC4s6mQ3_0U"},"source":["At this point you should have a training.gif file stored in the directory (folder) you specified in the previous cell.  Check out the visualization to see how your network converged to the decision boundary!\n","\n","<br>\n","\n","WHEN YOU COMPLETE THIS SECTION - STOP, AND UPLOAD YOUR COLAB SCRIPT FOR THIS ASSIGNMENT. THE OTHER PARTS IN THE COLAB SCRIPT WILL BE FOR SUBSEQUENT PROJECT/ASSIGNMENTS"]},{"cell_type":"markdown","metadata":{"id":"x0nc0nVGo4Tf"},"source":["# Project 2.2b:  Binary Classification on Complex Images\n","In this project, weâre going to return to classifying an image dataset.\n","This time, instead of using the âh5pyâ, which we called the  âcats,  no-catsâ dataset, weâll use one that contains both cats and dogs, which we call âcats versus dogsâ.\n","<br>\n","**Note:** You should restart the runtime and rerun the initialization steps (drive mount, imports, and function cells) to properly set up the environment before running these steps.\n","\n","**Step a:** Load the data.  Please refer to the lecture video for more details on the procedure.\n","**Step b:** Preprocess the data,  then convert to a numpy array for both the train and test inputs and output.\n","**Step c:**  Call the see_shapes function.   You should see the following output:\n","- Number of training examples: m_train =  2326\n","- Number of testing examples: m_test =  465\n","- X_train shape:  (2326, 64, 64, 3)\n","- Y_train shape:  (2326, 1)\n","- X_test shape:  (465, 64, 64, 3)\n","- Y_test shape:  (465, 1)\n","\n","**Step d:** Flatten the images and then run the final reshaping for clarity.\n","**Step e:** Instantiate your model.  Build it with the same architecture as you used previously for the cat vs non-cat dataset.\n","**Step f:** Fit it to the data.  Set verbose to be True so you can see the training in progress and donât worry about setting the callback this time.\n","**Step g:** Plot the costs, and evaluate performance.  Your costs plot should look something like this:\n","\n","\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=10OcW-ix6boH7tJUp1SqQT__O8Lv8sa9O)\n","\n","\n","<br>\n","\n","Your costs should converge below 0.1 within 1500 epochs, and your train score should be above 95%, but you should still see a poor test score (around 50%).\n"]},{"cell_type":"code","metadata":{"id":"7CCoPGeAi7la"},"source":["# Load the data.\n","(ds_train, ds_test), ds_info = load_data('cats_vs_dogs')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgHM1Mny3D6r"},"source":["tfds.visualization.show_examples(ds_train, ds_info)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3O-9GG7Hi7oB"},"source":["# Preprocess the train set.\n","ds_train = preprocess(ds_train, ds_info, 64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V7KszuHxi7rL"},"source":["# Preprocess the test set.\n","ds_test = preprocess(ds_test, ds_info, 64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Csik4sJHj3zz"},"source":["# Cast to numpy.\n","X_train, Y_train = as_numpy(ds_train)\n","X_test, Y_test = as_numpy(ds_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lGms6TqCi7uZ"},"source":["# Call the see_shapes() function.\n","None\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Urc5KwEKjYNY"},"source":["# Flatten the images.\n","X_train, X_test = None\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7D7ohKz-jrPh"},"source":["# Reshape for clarity.\n","Y_train = Y_train.reshape(1, X_train.shape[1])\n","Y_test = Y_test.reshape(1, X_test.shape[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kYPZLphqVu8T"},"source":["np.random.seed(0) #do not change - for grading purposes\n","\n","# Instantiate your model.\n","model = None\n","\n","# Build a deep neural network.\n","# Build it with the same architecture as you used previously for the cat vs non-cat dataset.\n","model.build([None\n","])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w9MzvLbIcIfl"},"source":["# Fit your model to the training data and print the costs.\n","# Use 2001 epochs and a learning rate of 0.0075.\n","# Set the verbose parameter to True so that you can see the training details.\n","costs = None\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oASbuziHyhQ6"},"source":["# Plot the costs.\n","None\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PuwyqFE2ZDpj"},"source":["# Make predictions on the train set.\n","predictions_train = None\n","\n","# Make predictions on the test set.\n","predictions_test = none\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IcNGdZ6tz0cE"},"source":["# Evaluate performance on the train set.\n","None\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bhFxdPmK4L4S"},"source":["# Evaluate performance on the test set.\n","None\n"],"execution_count":null,"outputs":[]}]}