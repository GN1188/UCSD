{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MavRPT-dfP1"
      },
      "source": [
        "**Copyright: Â© NexStream Technical Education, LLC**.  \n",
        "All rights reserved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poa0_c7TVSi9"
      },
      "source": [
        "#Gradient Descent with Simple Linear Regression\n",
        "In this project, you will generate a Simple Linear Regression model using gradient descent to minimize the cost function of the sum of squared errors.  You will compare the coefficients computed directly for a provided model, then you will recalculate the coefficients using the gradient descent cost minimization technique.\n",
        "\n",
        "Please reference the video lectures on Gradient Descent for a description of this example and the main functions you will need in the implementation.\n",
        "Please complete the following steps in your Colab Script.  The reference script below provides template code and hints to help with each step.  You will be turning in code and screenshots of your console output in an accompanying assignment.\n",
        "\n",
        "Note that in Step 1, you will directly compute the b0 and b1 coefficients of a Simple Linear Regression model as was done in a previous project in this course (EDA assignment).  \n",
        "In Step 2, you will find the optimum b0 and b1 coefficients using gradient descent. Specifically, you will initialize b0 and b1 with random values, then iterate over several steps (fit your model) using a provided gradient (partial derivative) equation of the cost function to update b0 and b1.  You will save the values of b0 and b1 over the training iterations so that you can visualize their convergence to optimum values. \n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "-  **Step 1**:  \n",
        "Calculate the coefficients for a simple linear regression model using the following equations:  (NOTE, you may NOT use any machine learning library models for this step - you must calculate the parameters use the equations shown).  \n",
        "Normalize the dataset using the min and max of each of the variables (min/max of the X's and min/max of the y's), then use the equation shown to normalize each of the values in the columns:\n",
        "          X_normalized = (X - Xmin)/(Xmax - Xmin)\n",
        "          y_normalized = (y - ymin)/(ymax - ymin)\n",
        " The equations used to calculate the gradient are the partial derivatives of the cost function with respect to b0 and b1.  \n",
        "\n",
        "$$\\hat y = b_0+b_1x_1$$  \n",
        "$$b_1=\\frac {\\sum\n",
        "(x_n-\\bar x)(y_n-\\bar y)} {\\sum\n",
        "(x_n-\\bar x)^2} $$  \n",
        "$$b_0=\\bar y-b_1\\bar x$$  \n",
        "\n",
        "-  **Step 2**:  \n",
        "Write a function that implements gradient descent from scratch (i.e. you may not call any library functions) to generate the linear regression equation coefficients by minimizing the cost function.\n",
        "Here we use the Sum of Squared Errors (SSE) cost function where n is the number of samples in the dataset, y is the dependent variable in the dataset, $b_0$ is the y-intercept for the linear equation, $b_1$ is the slope of the linear equation, and $x_1$ is the independent variable. \n",
        "\n",
        "$$SSE = \\frac{1}{n}\\sum(y - \\hat y)^2 = \\frac{1}{n}\\sum(y - (b_0+b_1x_1))^2$$  \n",
        "\n",
        "           API:  def gradientDescent(coeffs, X, y, lr, iterations):\n",
        "           Input: coefficients: array of coefficients (i.e. b0 and b1) initialized to random values\n",
        "                  X: normalized training dataset independent vars (see hint below)\n",
        "                  y: normalized training dataset dependent vars (see hint below)\n",
        "                  lr:  learning rate  \n",
        "                  iterations: number of iterations to run\n",
        "           Output: Returns b0_arr, b1_arr, SSE_arr (in this order), where b0_arr, b1_arr, SSE_arr are numpy arrays \n",
        "                   of the b0, b1 coefficients and sum-of-squared error respectively for each of the iterations performed in your gradient descent loop.\n",
        "\n",
        "$$\\frac{\\partial(SSE)}{\\partial(b0)} = \\frac{1}{n}\\cdot 2(y-(b_0+b_1x_1))\\cdot -1 = -\\frac{2}{n}(y-\\hat y) $$\n",
        "\n",
        " $$\\frac{\\partial(SSE)}{\\partial(b1)} = \\frac{1}{n}\\cdot 2(y-(b_0+b_1x_1))\\cdot -x_1 = -\\frac{2}{n}(y-\\hat y)x_1 $$\n",
        "> Hint:  \n",
        " - The following calculations are done over each sample in the training set (i.e. use a for loop).\n",
        " - Calc the model equation:  yhat = b0 + b1x \n",
        " - Calc the gradient of b0:  sum(-(y-yhat))   \n",
        " - Calc the gradient of b1:  sum(-(y-yhat)*X)\n",
        " - Update the coefficients:  new coef = old coef + (learning rate)(error)(input).  \n",
        "    - Update b0 = b0 - (learning rate) * (gradient of b0)  \n",
        "    - Append calculated b0 to b0 array\n",
        "    - Update b1 = b1 - (learning rate) * (gradient of b1)\n",
        "    - Append calculated b1 to b1 array\n",
        "\n",
        "\n",
        "- **Step 3**:  \n",
        "Plot the equation of your model from the manual (Step 1) model and gradient descent (Step 2) model on the same graph along with your dataset points.  \n",
        "Your plot should look something like this for random initial values of b0 and b1 and 50 iterations (note, it may not look exactly as the plot shown as it will be dependent on the initial random values of the coefficients).  The blue line in the graph is the equation with directly computed coefficients (Step 1), while the red line represents the coefficients calculated by minimizine the cost function with Gradient Descent (Step 2).  Also note that the plotted dataset has been normalized.\n",
        "\n",
        "![alt text](https://docs.google.com/uc?export=download&id=1F5mxjjzqnuNC9mZNsJwXcHVDcN6TXKgw)\n",
        "\n",
        "\n",
        "- **Step 4**:  \n",
        "Experiment with different numbers of iterations to see how your gradient descent model changes compared to that of manual model.   You should observe an improvement with increased number of iterations such the two techniques converge with higher numbers of iterations. (i.e. the lines should coincide). Print out the the minimum number of iterations, values of your coefficients, and SSE for the line that visually is closest to the direct coefficients method (you should be able to find a number of iterations that basically overlays the two prediction lines). Note that we want to find a minimum number of iterations that also provides a minimized cost. \n",
        "\n",
        "\n",
        "- **Step 5**:  \n",
        "Plot on a 3D graph your coefficients (b0 and b1), on x and y axes respectively, and SSE (Sum of Square Errors) on the z-axis.  Your plot should should the SSE converging to a minimum as shown in the figure below.  Note your graph will not look exactly as the plot shown as it will be depending on the initial random values of the coefficients and the number of iterations.  \n",
        "\n",
        "![alt text](https://docs.google.com/uc?export=download&id=1buL6OC40DMZJduCQIh66PDBOxYbRieN3)\n",
        "\n",
        "\n",
        "- **Step 6**:  \n",
        "Calculate the performance for both techniques (Step 2, Step 3) using the r-squared score metric.  \n",
        "Hint: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html  \n",
        "Why does the R-square score for the direct calculated coefficients remain constant, and why is it relatively low?  \n",
        "Why does the R-square score change from run to run for the gradient descent calculated coefficients?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "punMFTXRZJBi"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import random\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#Input Parameters\n",
        "Age = np.array([43,21,25,42,57,59,35,15,55,50,65,10,45,35])\n",
        "Glucose = np.array([99,65,79,75,87,81,80,80,90,70,95,67,90,82])                   \n",
        "\n",
        "\n",
        "#Step 1:  Create a function which MANALLY (using the equations) calculates the \n",
        "#         coefficients for a simple linear regression model, then plot the calculated \n",
        "#         regression line on top of the dataset.\n",
        "#         Your function must input numpy arrays for the x and y variables and return b0 and b1.\n",
        "#         Your function MUST use equations shown in the text cell above. \n",
        "#         Note - you may not use a machine learning library model for this step.\n",
        "#         Note - you may (should) use the function you created in the Linear Prediction Project.\n",
        "#         Note - normalize your data using the min/max method before calculating the coefficients.\n",
        "\n",
        "#Normalize the data using the following min/max method:\n",
        "#Normalized Sample = (Sample - Sample.min)/(Sample.max - Sample.min)\n",
        "# YOUR CODE HERE...\n",
        "\n",
        "\n",
        "#Write a function to calculate the Linear Regression coefficients manually.\n",
        "#Use the function in your previous project on simple linear regression.\n",
        "def simpleLRcoeffsManual(X, Y):\n",
        "# YOUR CODE HERE...\n",
        "\n",
        "\n",
        "#Print out your coefficients, prediction (yhat or ypred), and the SSE\n",
        "# YOUR CODE HERE...\n",
        "\n",
        "\n",
        "#Plot the data and prediction line for the manual calculation\n",
        "# YOUR CODE HERE...\n",
        "\n",
        "\n",
        "#Step 2:  Write a function that implements gradient descent to minimize the cost function.\n",
        "#         where the cost function is sum-of-square error (SSE) = 1/n*sum(y - yhat)^2 = 1/n*sum(y-(b0+b1*X))^2\n",
        "#         Inputs:  coeffs:  array of coefficients b0 and b1 initialized to random values \n",
        "#                  X:  dataset normalized independent variable values\n",
        "#                  y:  dataset normalized dependent variable values\n",
        "#                  lr: learning rate\n",
        "#                  iterations:  number of iterations\n",
        "#         Outputs: Returns b0_arr, b1_arr, SSE_arr (in this order)\n",
        "#         def gradientDescent(coeffs, X, y, lr, iterations):\n",
        "#\n",
        "def gradientDescent(coeffs, X, y, lr, iterations):\n",
        "# YOUR CODE HERE...\n",
        "\n",
        "\n",
        "# Step 3: Plot the equation of your model from the manual (Step 1) model and gradient descent (Step 2) model \n",
        "#         on the same graph along with your dataset points. \n",
        "#         Note, Use the normalized dataset. \n",
        "# YOUR CODE HERE...\n",
        "\n",
        "\n",
        "# Step 4:  Experiment with different numbers of iterations to see how your gradient descent \n",
        "#          model changes compared to that of manual model.   You should observe an improvement \n",
        "#          with increased number of iterations such the two techniques converge with higher numbers \n",
        "#          of iterations. (i.e. the lines should coincide). Print out the the minimum number of \n",
        "#          iterations, values of your coefficients, and SSE for the line that visually is closest to the \n",
        "#          direct coefficients method.\n",
        "# YOUR CODE HERE...\n",
        "\n",
        "\n",
        "# Step 5: Plot on a 3D graph your coefficients (b0 and b1), on x and y axes respectively, \n",
        "#         and SSE (Sum of Square Errors) on the z-axis.  Your plot should should the \n",
        "#         SSE converging to a minimum as shown in the figure below.  Note your graph will \n",
        "#         not look exactly as the plot shown as it will be depending on the initial \n",
        "#         random values of the coefficients and the number of iterations. \n",
        "#         Hint: https://matplotlib.org/2.0.2/mpl_toolkits/mplot3d/tutorial.html \n",
        "# YOUR CODE HERE...\n",
        "\n",
        "#Highlight the last coefficents (should be the ones with the minimized cost)\n",
        "\n",
        "\n",
        "#Step 6:  Calculate the performance for both techniques (Step 2, Step 3) using the r-squared score metric.  \n",
        "# YOUR CODE HERE...\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}