{"cells":[{"cell_type":"markdown","metadata":{"id":"LPC88Ps3BgJ1"},"source":["**Copyright: © NexStream Technical Education, LLC**.\n","All rights reserved"]},{"cell_type":"markdown","metadata":{"id":"pmAb6VjuyCVX"},"source":["**Gradient Descent and the Perceptron**\n","\n","In this project, you will integrate gradient descent to a perceptron model with logistic regression (i.e. using the sigmoid activation and binary cross entropy cost function) on a synthetic dataset. Then in a subsequent assignment you will extend what you develop here and apply it to an image dataset.  The code you will develop implements an unvectorized version (Part-1), then a vectorized version (Part-2), and finally you will compare the efficiency between the two versions (Part-3).\n","The code developed in this unit will form the base for the backpropagation step which will be integrated into your DNN framework.\n","\n","Please complete the following steps in the Colab Script.  The reference script below provides template code and hints to help with each step.\n","\n","The following instructions are identified as Steps in the text cells preceding their corresponding code cell. Read through the instructions and write/fill-in the appropriate code in the cells.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LCc0FU8q6ssl"},"source":["##Part A:  Dataset Creation and Preprocessing\n","\n","In this section, you will set up your drive, create a synthetic dataset, and preprocess the data in preparation for the regression and descent algorithms.  Please follow the steps outlined in the following cells and fill in your code where prompted."]},{"cell_type":"markdown","metadata":{"id":"saxoakxEfuWZ"},"source":["**Step A-1:**\n","- Mount your Google drive.\n","- Upload the file tf_image_utils.py from the materials folder provided with this course and copy it to your project directory.\n","- Import the numpy module as np and the random module.\n","- Initialize a random seed.  This will be used for checking your outputs."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MzEhqIvp1U2Q","outputId":"7e7da5c3-96a4-4895-b067-922395920648","executionInfo":{"status":"ok","timestamp":1691304509889,"user_tz":420,"elapsed":17462,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd /content/drive/MyDrive/Colab Notebooks/\n","#See the reference cp command below.  Update this to your own drive path.\n","#!cp drive/MyDrive/MachineLearning/DNN/tf_image_utils.py .\n","\n","#!cp '/content/drive/MyDrive/ComputerScience/MachineLearning/Neural Networks/DNN/tf_image_utils.py' ."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"sG2eQhBf4eZH","executionInfo":{"status":"ok","timestamp":1691304517070,"user_tz":420,"elapsed":4057,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["# Imports\n","\n","import numpy as np\n","import tensorflow as tf\n","import tf_image_utils\n","from tf_image_utils import load_data, preprocess, as_numpy\n","import tensorflow_datasets as tfds\n"]},{"cell_type":"markdown","metadata":{"id":"iH2bspvo_hEe"},"source":["**Step A-2:** Create a Synthetic Dataset\n","\n","- Use the following code cells to generate synthetic data and create the train and test datasets.\n","- Write the function \"see_shapes\" to print the shapes of the datasets, then call the function to display the shapes.\n","Your shapes output should be the following:\n","  - Number of training examples: m_train =  80\n","  - Number of testing examples: m_test =  20\n","  - Height/Width of each image: num_px =  24\n","  - Each image is of size:  24 x 24\n","  - X_train shape:  (80, 24, 24)\n","  - Y_train shape:  (1, 80)\n","  - X_test shape:  (20, 24, 24)\n","  - Y_test shape:  (1, 20)\n","- Verify the doctest modules included in the cells run without any errors"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"VolKv5VGeOgY","executionInfo":{"status":"ok","timestamp":1691304521574,"user_tz":420,"elapsed":150,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["# Creates synthetic data.\n","\n","def create_X(m):\n","  np.random.seed(3) # do not change - for grading purposes\n","  data = np.zeros((m, 24, 24))\n","  for i in range(m):\n","    for j in range(24):\n","      data[i][j] = np.random.randint(0, 255, 24)\n","  return np.array(data)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"IbSLkIP9a2mC","executionInfo":{"status":"ok","timestamp":1691304523586,"user_tz":420,"elapsed":163,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["# Creates synthetic labels.\n","\n","def create_Y(m, X):\n","  labels = np.zeros((1, m))\n","  for i in range(m):\n","    if (X[i][0][0] % 2 == 0):\n","      labels[:,i] = 1\n","    else:\n","      labels[:,i] = 0\n","  return labels"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"eJ6gUaZgcT-C","executionInfo":{"status":"ok","timestamp":1691304526133,"user_tz":420,"elapsed":6,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["def create_dataset(m):\n","  X = create_X(m)\n","  Y = create_Y (m, X)\n","  return X, Y"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"jNu9Z4Ynch1n","executionInfo":{"status":"ok","timestamp":1691304528015,"user_tz":420,"elapsed":7,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["X_train, Y_train = create_dataset(80)\n","X_test, Y_test = create_dataset(20)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"2VjYeIqAsoIx","executionInfo":{"status":"ok","timestamp":1691304529199,"user_tz":420,"elapsed":153,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["# function to see the shapes of the data.\n","def see_shapes(X_train, X_test, Y_train, Y_test):\n","\n","### BEGIN CODE HERE\n","\n","  #m_train = None\n","  #m_test =  None\n","  #image_size = None\n","  m_train = X_train.shape[0]\n","  m_test =  X_test.shape[0]\n","  image_size = X_train.shape[1]\n","  return m_train, m_test, image_size\n","\n"," ### END CODE HERE\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E6EEwiA3BNBD","outputId":"50de6908-1990-4c7c-b4dd-07cc27072b1c","executionInfo":{"status":"ok","timestamp":1691304532141,"user_tz":420,"elapsed":171,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["\n","PYDEV DEBUGGER WARNING:\n","sys.settrace() should not be used when the debugger is being used.\n","This may cause the debugger to stop working correctly.\n","If this is needed, please check: \n","http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n","to see how to restore the debug tracing back correctly.\n","Call Location:\n","  File \"/usr/lib/python3.10/doctest.py\", line 1501, in run\n","    sys.settrace(save_trace)\n","\n"]},{"output_type":"stream","name":"stdout","text":["Number of training examples: m_train =  80\n","Number of testing examples: m_test =  20\n","Height/Width of each image: num_px =  24\n","Each image is of size:  24 x 24\n","X_train shape:  (80, 24, 24)\n","Y_train shape:  (1, 80)\n","X_test shape:  (20, 24, 24)\n","Y_test shape:  (1, 20)\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=7)"]},"metadata":{},"execution_count":8}],"source":["# Call the function to see the shapes of your data.\n","\n","### BEGIN CODE HERE\n","\n","#m_train, m_test, image_size = None\n","\n","m_train, m_test, image_size = see_shapes(X_train, X_test, Y_train, Y_test)\n","\n","### END CODE HERE\n","\n","print (\"Number of training examples: m_train = \", m_train)\n","print (\"Number of testing examples: m_test = \", m_test)\n","print (\"Height/Width of each image: num_px = \", image_size)\n","print (\"Each image is of size: \", image_size, \"x\", image_size)\n","print (\"X_train shape: \", X_train.shape)\n","print (\"Y_train shape: \", Y_train.shape)\n","print (\"X_test shape: \", X_test.shape)\n","print (\"Y_test shape: \", Y_test.shape)\n","\n","import doctest\n","\"\"\"\n","  >>> print(m_train)\n","  80\n","  >>> print(m_test)\n","  20\n","  >>> print(image_size)\n","  24\n","  >>> print (X_train.shape)\n","  (80, 24, 24)\n","  >>> print (Y_train.shape)\n","  (1, 80)\n","  >>> print (X_test.shape)\n","  (20, 24, 24)\n","  >>> print (Y_test.shape)\n","  (1, 20)\n","\"\"\"\n","\n","doctest.testmod()"]},{"cell_type":"markdown","metadata":{"id":"KsN3jZAHjdzh"},"source":["**Step A-3 :** Preprocess the datsets - flatten, normalize\n","- Write the function \"flatten\" to flatten the dataset then call the function to see the shapes of the flattened data.   A trick to accomplish this in a single line is to set X_train = X_train.reshape(X_train.shape[0], -1).T. What is going on in this line? First, we call the .reshape method on X_train to begin the process of reshaping it. Then, we say that we want the first dimension of our new X_train to be the same as the first dimension of the old X_train. That is, we say that we want the first dimension of X_train to remain “m”, or the number of examples in the set. Then, the “-1” returns the product of all other dimensions. This gives us a matrix of 80 rows by 576 columns. The final step is to transpose the matrix by using the .T method, which switches the number of rows and columns. Do the same for X_test and then run the cell to call the flatten() function.\n","\n","- You should see the following output:\n","  - Flattened X_train shape: (576, 80)\n","  - Y_train shape: (1, 80)\n","  - Flattened X_test shape: (576, 20)\n","  - Y_test shape: (1, 20)\n","\n","- Write the function \"normalize\" to normalize the dataset then call the function to see the shapes of the flattened data. That is, divide each of the samples by the maximum value corresponding to the size of the pixels (in this case, it'll be 2^8 - 1 or 255).\n","- Verify the doctest modules run for both the flatten and normalize calls without any errors"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"0X5w5pYlwiBb","executionInfo":{"status":"ok","timestamp":1691304538219,"user_tz":420,"elapsed":136,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["# Flatten the dataset.\n","def flatten(X_train, X_test):\n","\n","  ### BEGIN CODE HERE ###\n","\n","  #X_train = None\n","  #X_test =  None\n","\n","  X_train = X_train.reshape(X_train.shape[0], -1).T\n","  X_test = X_test.reshape(X_test.shape[0], -1).T\n","\n","  return X_train, X_test, X_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n","\n","  ### END CODE HERE ###\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"PM1UTU8SBUfB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691304539967,"user_tz":420,"elapsed":174,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"93bbcacf-a0e6-4823-efa9-2e089a6b664b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Flattened X_train shape: (576, 80)\n","Y_train shape: (1, 80)\n","Flattened X_test shape: (576, 20)\n","Y_test shape: (1, 20)\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=4)"]},"metadata":{},"execution_count":10}],"source":["# Call the function to flatten the datasets.\n","\n","### BEGIN CODE HERE\n","\n","#X_train, X_test, X_train.shape, Y_train_shape, X_test.shape, Y_test.shape = None\n","\n","X_train, X_test, X_train_shape, Y_train_shape, X_test_shape, Y_test_shape = flatten(X_train, X_test)\n","\n","### END CODE HERE\n","\n","print (\"Flattened X_train shape: \" + str(X_train.shape))\n","print (\"Y_train shape: \" + str(Y_train.shape))\n","print (\"Flattened X_test shape: \" + str(X_test.shape))\n","print (\"Y_test shape: \" + str(Y_test.shape))\n","\n","import doctest\n","\"\"\"\n","  >>> print(X_train.shape)\n","  (576, 80)\n","  >>> print(Y_train.shape)\n","  (1, 80)\n","  >>> print(X_test.shape)\n","  (576, 20)\n","  >>> print (Y_test.shape)\n","  (1, 20)\n","\"\"\"\n","\n","doctest.testmod()\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"BdKS_QBfWMLw","executionInfo":{"status":"ok","timestamp":1691304543875,"user_tz":420,"elapsed":132,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["# Basic normalization.\n","\n","def normalize(X_train, X_test):\n","\n","  ### BEGIN CODE HERE\n","\n","  #X_train = None\n","  #X_test = None\n","\n","  X_train = X_train / 255\n","  X_test = X_test / 255\n","\n","  ### END CODE HERE\n","\n","  return X_train, X_test"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"nO-SoveMNh1K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691304545622,"user_tz":420,"elapsed":174,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"f83a3296-f7d1-41a7-e58f-6431840dab4c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=4)"]},"metadata":{},"execution_count":12}],"source":["# Call your normalization function.\n","\n","### BEGIN CODE HERE\n","\n","#X_train, X_test = None\n","\n","X_train, X_test = normalize(X_train, X_test)\n","\n","### END CODE HERE\n","\n","import doctest\n","\"\"\"\n","  >>> print(round(X_train[0][3], 2))\n","  0.12\n","  >>> print(round(Y_train[0][4], 2))\n","  1.0\n","  >>> print(round(X_test[0][0], 2))\n","  0.42\n","  >>> print (round(Y_test[0][1], 2))\n","  0.0\n","\"\"\"\n","\n","doctest.testmod()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9Y3v8XCUyJZT"},"source":["## Part 1:  Unvectorized Implementation\n","\n","In this section, you will implement unvectorized gradient descent.  Please follow the steps outlined in the following cells and fill in your code where prompted.\n","\n","First, take a look at the pseudocode overview. This is the skeleton around which you should build your code. Make sure to refer back to this cell throughout if you get stuck, and for further clarification, refer back to the lecture video on Gradient Descent\n","\n","An important note is that this is not a completely unvectorized implementation. You will be using vectors for the weights and gradients, but in conjunction with for loops. However, as you will see, this method is much less efficient than the fully vectorized approach you will implement later.\n"]},{"cell_type":"markdown","metadata":{"id":"wubAaPQEOlr_"},"source":["**Pseudocode Overview**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hq_rXP8YOwqu"},"outputs":[],"source":["### PSEUDOCODE for unvectorized gradient descent\n","\n","# Extract number of examples and number of features from X_train\n","# Initialize parameters\n","# Initialize list of costs\n","\n","# Loop over iterations\n","# Initialize gradients vector\n","# Initialize cost accumulator\n","  # Loop over examples\n","    # Index into dataset to extract a single example's feature vector and label\n","    # Compute forward propagation steps to find the activation for this example\n","    # Compute the cost for this example and add it to the cumulative total\n","    # Calculate the derivative dz for this example\n","\n","      # Loop over features\n","        # Calculate the derivative of each weight using its associated feature vector\n","\n","    # Add the derivative with respect to the bias for that example to a cumulative total\n","\n","  # Divide the cost and the gradients by the number of examples m to find the average cost and average update amount per weight.\n","  # Loop over features.\n","    # Update the weights.\n","  # Update the bias.\n","\n","  # Append the cost for the iteration to the list.\n","\n","# Return the learned parameters and the list of costs."]},{"cell_type":"markdown","metadata":{"id":"BDXu3N6zOQKM"},"source":["**Step 1-1** : Initialization and Example Extraction\n","Let’s begin by defining functions which initialize our parameters and extract a single example from the train set. Each example in our training data has 576 features, which means there will be one weight associated with each. While there are many ways of initializing weights, we are going to keep things simple by simply creating an array of 576 random numbers (multiplied by 0.01 to keep them small) as our weights vector. We can use the built-in numpy function np.random.randn to do this. You will need to pass the number of values (i.e. the number of weights) that you want in your array to randn(). When you can, try to avoid hard coding. For example, instead of hard coding the shape of the weights vector w as (576, 1), use a variable instead for the number of features in X_train. You can, however, just go ahead and set b=0, since this is a simple perceptron model (and so there will only be one bias since there is one node). Return the initialized w and b from this function.\n","\n","Then call the initialize_parameters function and generate 5 weights.\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"XQWCiZhy1m4b","executionInfo":{"status":"ok","timestamp":1691304551522,"user_tz":420,"elapsed":5,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["# Initialize weights and biases.\n","def initialize_parameters(dim):\n","  np.random.seed(3) #do not change - for grading purposes\n","\n","  #w = None\n","  #b = None\n","  w = np.random.randn(dim, 1) * 0.01\n","  b = 0\n","\n","\n","  return w, b"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"oHchBniLOqNw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691304556522,"user_tz":420,"elapsed":134,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"907b1050-f27d-4f91-d7b7-0e9b2d914001"},"outputs":[{"output_type":"stream","name":"stdout","text":["w:  [[ 0.01788628]\n"," [ 0.0043651 ]\n"," [ 0.00096497]\n"," [-0.01863493]\n"," [-0.00277388]]\n","b:  0\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=3)"]},"metadata":{},"execution_count":14}],"source":["# Test for correct output.\n","\n","w, b = initialize_parameters(5)\n","print (\"w: \", w)\n","print (\"b: \", b)\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(w[0], 3))\n","  [0.018]\n","  >>> print(np.round(w[4], 3))\n","  [-0.003]\n","  >>> print(np.round(b, 1))\n","  0\n","\"\"\"\n","\n","doctest.testmod()\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QDNtjLfa8An4"},"source":["**Step 1-2 :**  Extract an example (i.e. an input sample)\n","Next, let’s build a function to extract an example from the train set. Refer back to the pseudocode guide – you should see that this function will be called immediately after entering the for loop over the examples. Therefore, in a loop going from i=0 to i=m, we need a way to extract the ith example from the train set. To understand exactly what we are doing, let’s look back at the shapes of the data. After flattening, X_train is a matrix with 576 rows and 80 columns; that is, each column represents an example. So to extract a single example, we need to index the matrix by column. With numpy, we can index multidimensional arrays  by row and column. For example, if we want all the values in the 3rd column, we can index as follows: array[:,3]. Define the function extract() to return the ith column of X_train as x and the ith column of Y_train as y.\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"bV_5eSPdaRtS","executionInfo":{"status":"ok","timestamp":1691304562210,"user_tz":420,"elapsed":144,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["# Extract a single example.\n","def extract(X_train, Y_train, example):\n","\n","\n","  # Index into the array.\n","  #x = None\n","  #y = None\n","\n","  x = X_train[:, example]\n","  y = Y_train[:, example]\n","  x = x.reshape(x.shape[0], 1)\n","\n","\n","  # Reshape\n","  x = x.reshape(x.shape[0], 1)\n","  return x, y"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"QZT4qwujPAyT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691304565349,"user_tz":420,"elapsed":151,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"db28e56a-dec3-4517-9c3e-661160e3d477"},"outputs":[{"output_type":"stream","name":"stdout","text":["x:  [[0.41568627]\n"," [0.59607843]\n"," [0.97647059]\n"," [0.51372549]\n"," [0.72156863]\n"," [0.78431373]\n"," [0.        ]\n"," [0.08235294]\n"," [0.99215686]\n"," [0.57647059]\n"," [0.79215686]\n"," [0.41960784]\n"," [0.97647059]\n"," [0.6627451 ]\n"," [0.54117647]\n"," [0.58431373]\n"," [0.46666667]\n"," [0.65098039]\n"," [0.87843137]\n"," [0.58039216]\n"," [0.6745098 ]\n"," [0.36470588]\n"," [0.65490196]\n"," [0.55686275]\n"," [0.97254902]\n"," [0.10196078]\n"," [0.31764706]\n"," [0.85490196]\n"," [0.58823529]\n"," [0.25882353]\n"," [0.00784314]\n"," [0.74901961]\n"," [0.23529412]\n"," [0.50588235]\n"," [0.70196078]\n"," [0.35294118]\n"," [0.27058824]\n"," [0.40784314]\n"," [0.43137255]\n"," [0.38039216]\n"," [0.61568627]\n"," [0.41568627]\n"," [0.59607843]\n"," [0.98431373]\n"," [0.24313725]\n"," [0.02745098]\n"," [0.97254902]\n"," [0.67058824]\n"," [0.12941176]\n"," [0.48235294]\n"," [0.30980392]\n"," [0.69019608]\n"," [0.14509804]\n"," [0.07843137]\n"," [0.36862745]\n"," [0.19215686]\n"," [0.58431373]\n"," [0.49803922]\n"," [0.80784314]\n"," [0.95686275]\n"," [0.10980392]\n"," [0.46666667]\n"," [0.21176471]\n"," [0.        ]\n"," [0.75294118]\n"," [0.91372549]\n"," [0.07058824]\n"," [0.45490196]\n"," [0.74901961]\n"," [0.64705882]\n"," [0.72156863]\n"," [0.72156863]\n"," [0.78039216]\n"," [0.64705882]\n"," [0.99215686]\n"," [0.68235294]\n"," [0.12941176]\n"," [0.44313725]\n"," [0.00392157]\n"," [0.33333333]\n"," [0.45882353]\n"," [0.46666667]\n"," [0.79215686]\n"," [0.92156863]\n"," [0.48627451]\n"," [0.89411765]\n"," [0.38823529]\n"," [0.85882353]\n"," [0.56470588]\n"," [0.31372549]\n"," [0.62745098]\n"," [0.56470588]\n"," [0.94901961]\n"," [0.07058824]\n"," [0.29411765]\n"," [0.71764706]\n"," [0.37647059]\n"," [0.8745098 ]\n"," [0.55294118]\n"," [0.64705882]\n"," [0.61960784]\n"," [0.18823529]\n"," [0.74117647]\n"," [0.12941176]\n"," [0.20392157]\n"," [0.94509804]\n"," [0.98823529]\n"," [0.94901961]\n"," [0.00784314]\n"," [0.61176471]\n"," [0.14117647]\n"," [0.35294118]\n"," [0.90980392]\n"," [0.6745098 ]\n"," [0.69019608]\n"," [0.23137255]\n"," [0.79215686]\n"," [0.71372549]\n"," [0.94117647]\n"," [0.89411765]\n"," [0.35686275]\n"," [0.94509804]\n"," [0.08235294]\n"," [0.44313725]\n"," [0.21960784]\n"," [0.40784314]\n"," [0.15294118]\n"," [0.11372549]\n"," [0.62745098]\n"," [0.69019608]\n"," [0.03529412]\n"," [0.12941176]\n"," [0.9372549 ]\n"," [0.96862745]\n"," [0.7372549 ]\n"," [0.84705882]\n"," [0.21568627]\n"," [0.04313725]\n"," [0.83137255]\n"," [0.54117647]\n"," [0.43921569]\n"," [0.81568627]\n"," [0.29803922]\n"," [0.26666667]\n"," [0.17254902]\n"," [0.17254902]\n"," [0.98431373]\n"," [0.0745098 ]\n"," [0.56470588]\n"," [0.47058824]\n"," [0.88627451]\n"," [0.65490196]\n"," [0.69803922]\n"," [0.75686275]\n"," [0.1372549 ]\n"," [0.67843137]\n"," [0.94901961]\n"," [0.20392157]\n"," [0.00392157]\n"," [0.07058824]\n"," [0.74901961]\n"," [0.00784314]\n"," [0.84313725]\n"," [0.89019608]\n"," [0.58039216]\n"," [0.24313725]\n"," [0.81960784]\n"," [0.08627451]\n"," [0.8627451 ]\n"," [0.92156863]\n"," [0.52941176]\n"," [0.7254902 ]\n"," [0.88235294]\n"," [0.67058824]\n"," [0.1254902 ]\n"," [0.98823529]\n"," [0.36078431]\n"," [0.23529412]\n"," [0.56470588]\n"," [0.10588235]\n"," [0.20784314]\n"," [0.41960784]\n"," [0.03137255]\n"," [0.07843137]\n"," [0.03137255]\n"," [0.59607843]\n"," [0.80784314]\n"," [0.29803922]\n"," [0.0745098 ]\n"," [0.39215686]\n"," [0.5372549 ]\n"," [0.10196078]\n"," [0.69803922]\n"," [0.23529412]\n"," [0.52156863]\n"," [0.83137255]\n"," [0.96862745]\n"," [0.37254902]\n"," [0.91372549]\n"," [0.41568627]\n"," [0.31764706]\n"," [0.03529412]\n"," [0.1254902 ]\n"," [0.55294118]\n"," [0.99607843]\n"," [0.17254902]\n"," [0.27843137]\n"," [0.36078431]\n"," [0.14117647]\n"," [0.49019608]\n"," [0.66666667]\n"," [0.78431373]\n"," [0.98823529]\n"," [0.10196078]\n"," [0.59607843]\n"," [0.58039216]\n"," [0.24313725]\n"," [0.6745098 ]\n"," [0.61960784]\n"," [0.56862745]\n"," [0.45490196]\n"," [0.1254902 ]\n"," [0.70980392]\n"," [0.3372549 ]\n"," [0.15294118]\n"," [0.23529412]\n"," [0.37254902]\n"," [0.1254902 ]\n"," [0.84313725]\n"," [0.34509804]\n"," [0.63137255]\n"," [0.48627451]\n"," [0.33333333]\n"," [0.2       ]\n"," [0.35294118]\n"," [0.81176471]\n"," [0.36078431]\n"," [0.04313725]\n"," [0.96470588]\n"," [0.91372549]\n"," [0.98823529]\n"," [0.06666667]\n"," [0.70196078]\n"," [0.27843137]\n"," [0.05098039]\n"," [0.37254902]\n"," [0.76862745]\n"," [0.87058824]\n"," [0.24705882]\n"," [0.16078431]\n"," [0.05098039]\n"," [0.80392157]\n"," [0.37647059]\n"," [0.84705882]\n"," [0.23529412]\n"," [0.68627451]\n"," [0.64313725]\n"," [0.71764706]\n"," [0.91764706]\n"," [0.72156863]\n"," [0.83529412]\n"," [0.77647059]\n"," [0.34117647]\n"," [0.8627451 ]\n"," [0.04705882]\n"," [0.61176471]\n"," [0.43529412]\n"," [0.43137255]\n"," [0.54117647]\n"," [0.54509804]\n"," [0.1372549 ]\n"," [0.83529412]\n"," [0.8627451 ]\n"," [0.19215686]\n"," [0.28235294]\n"," [0.85098039]\n"," [0.13333333]\n"," [0.04313725]\n"," [0.34901961]\n"," [0.97254902]\n"," [0.85098039]\n"," [0.73333333]\n"," [0.51764706]\n"," [0.33333333]\n"," [0.86666667]\n"," [0.        ]\n"," [0.72156863]\n"," [0.36862745]\n"," [0.67843137]\n"," [0.78039216]\n"," [0.38039216]\n"," [0.88627451]\n"," [0.58431373]\n"," [0.63529412]\n"," [0.98039216]\n"," [0.93333333]\n"," [0.11372549]\n"," [0.66666667]\n"," [0.02745098]\n"," [0.87058824]\n"," [0.61960784]\n"," [0.29803922]\n"," [0.79607843]\n"," [0.96470588]\n"," [0.65490196]\n"," [0.22352941]\n"," [0.69411765]\n"," [0.96078431]\n"," [0.90588235]\n"," [0.42352941]\n"," [0.6745098 ]\n"," [0.08235294]\n"," [0.36862745]\n"," [0.16078431]\n"," [0.78039216]\n"," [0.8627451 ]\n"," [0.13333333]\n"," [0.56078431]\n"," [0.08627451]\n"," [0.17647059]\n"," [0.14117647]\n"," [0.65882353]\n"," [0.35294118]\n"," [0.20392157]\n"," [0.03529412]\n"," [0.57647059]\n"," [0.90980392]\n"," [0.27058824]\n"," [0.71372549]\n"," [0.54901961]\n"," [0.10196078]\n"," [0.63921569]\n"," [0.59607843]\n"," [0.70588235]\n"," [0.88627451]\n"," [0.16862745]\n"," [0.46666667]\n"," [0.59215686]\n"," [0.69803922]\n"," [0.07058824]\n"," [0.25098039]\n"," [0.78039216]\n"," [0.85490196]\n"," [0.70980392]\n"," [0.62745098]\n"," [0.30588235]\n"," [0.47843137]\n"," [0.82745098]\n"," [0.10196078]\n"," [0.01176471]\n"," [0.64705882]\n"," [0.08235294]\n"," [0.43137255]\n"," [0.95686275]\n"," [0.47843137]\n"," [0.59215686]\n"," [0.92941176]\n"," [0.29803922]\n"," [0.07058824]\n"," [0.13333333]\n"," [0.36078431]\n"," [0.02352941]\n"," [0.77647059]\n"," [0.57647059]\n"," [0.23529412]\n"," [0.80392157]\n"," [0.4627451 ]\n"," [0.90588235]\n"," [0.42745098]\n"," [0.15294118]\n"," [0.25098039]\n"," [0.23529412]\n"," [0.54901961]\n"," [0.65882353]\n"," [0.4745098 ]\n"," [0.01568627]\n"," [0.16470588]\n"," [0.96078431]\n"," [0.89411765]\n"," [0.49019608]\n"," [0.22745098]\n"," [0.87058824]\n"," [0.58431373]\n"," [0.9254902 ]\n"," [0.        ]\n"," [0.1372549 ]\n"," [0.30196078]\n"," [0.08235294]\n"," [0.26666667]\n"," [0.88627451]\n"," [0.36470588]\n"," [0.27058824]\n"," [0.49019608]\n"," [0.44313725]\n"," [0.90588235]\n"," [0.6627451 ]\n"," [0.77647059]\n"," [0.85882353]\n"," [0.17647059]\n"," [0.62745098]\n"," [0.25490196]\n"," [0.8745098 ]\n"," [0.78039216]\n"," [0.87843137]\n"," [0.76470588]\n"," [0.84313725]\n"," [0.99215686]\n"," [0.09803922]\n"," [0.45098039]\n"," [0.10196078]\n"," [0.80784314]\n"," [0.89803922]\n"," [0.58823529]\n"," [0.29803922]\n"," [0.04313725]\n"," [0.11764706]\n"," [0.88235294]\n"," [0.30980392]\n"," [0.96078431]\n"," [0.51372549]\n"," [0.2745098 ]\n"," [0.18039216]\n"," [0.18431373]\n"," [0.97254902]\n"," [0.55686275]\n"," [0.29411765]\n"," [0.01960784]\n"," [0.83921569]\n"," [0.81176471]\n"," [0.46666667]\n"," [0.81568627]\n"," [0.7372549 ]\n"," [0.63529412]\n"," [0.75686275]\n"," [0.14901961]\n"," [0.19607843]\n"," [0.73333333]\n"," [0.07058824]\n"," [0.11372549]\n"," [0.74901961]\n"," [0.08235294]\n"," [0.60392157]\n"," [0.54901961]\n"," [0.09019608]\n"," [0.06666667]\n"," [0.61960784]\n"," [0.14117647]\n"," [0.02745098]\n"," [0.29019608]\n"," [0.09803922]\n"," [0.38823529]\n"," [0.44313725]\n"," [0.54117647]\n"," [0.88627451]\n"," [0.70196078]\n"," [0.00392157]\n"," [0.96078431]\n"," [0.32941176]\n"," [0.43137255]\n"," [0.20784314]\n"," [0.93333333]\n"," [0.47843137]\n"," [0.32156863]\n"," [0.54901961]\n"," [0.76862745]\n"," [0.44313725]\n"," [0.01176471]\n"," [0.74509804]\n"," [0.13333333]\n"," [0.42352941]\n"," [0.19215686]\n"," [0.16470588]\n"," [0.7372549 ]\n"," [0.01568627]\n"," [0.58431373]\n"," [0.28235294]\n"," [0.0745098 ]\n"," [0.00392157]\n"," [0.57254902]\n"," [0.90196078]\n"," [0.27058824]\n"," [0.71764706]\n"," [0.69411765]\n"," [0.85882353]\n"," [0.29411765]\n"," [0.01176471]\n"," [0.79215686]\n"," [0.34901961]\n"," [0.0745098 ]\n"," [0.83529412]\n"," [0.94117647]\n"," [0.65490196]\n"," [0.38431373]\n"," [0.6745098 ]\n"," [0.02352941]\n"," [0.36862745]\n"," [0.24313725]\n"," [0.60392157]\n"," [0.94117647]\n"," [0.48235294]\n"," [0.8745098 ]\n"," [0.38039216]\n"," [0.50196078]\n"," [0.14117647]\n"," [0.21176471]\n"," [0.30588235]\n"," [0.64705882]\n"," [0.90196078]\n"," [0.91764706]\n"," [0.64313725]\n"," [0.34901961]\n"," [0.09019608]\n"," [0.42745098]\n"," [0.87843137]\n"," [0.32941176]\n"," [0.57254902]\n"," [0.79215686]\n"," [0.97254902]\n"," [0.34901961]\n"," [0.17254902]\n"," [0.12156863]\n"," [0.85882353]\n"," [0.8       ]\n"," [0.40784314]\n"," [0.54509804]\n"," [0.44705882]\n"," [0.9372549 ]\n"," [0.09411765]\n"," [0.99215686]\n"," [0.35294118]\n"," [0.34509804]\n"," [0.60392157]\n"," [0.01568627]\n"," [0.36078431]\n"," [0.21960784]\n"," [0.50196078]\n"," [0.07843137]\n"," [0.21568627]\n"," [0.54117647]\n"," [0.41176471]\n"," [0.01960784]\n"," [0.23137255]\n"," [0.43137255]\n"," [0.36078431]\n"," [0.03529412]\n"," [0.71372549]\n"," [0.74117647]\n"," [0.32941176]\n"," [0.65098039]\n"," [0.9254902 ]\n"," [0.48235294]\n"," [0.82745098]\n"," [0.37647059]\n"," [0.4627451 ]\n"," [0.3254902 ]\n"," [0.12941176]\n"," [0.76862745]\n"," [0.79607843]\n"," [0.8745098 ]\n"," [0.18039216]\n"," [0.68235294]\n"," [0.94509804]\n"," [0.58431373]\n"," [0.19215686]\n"," [0.53333333]\n"," [0.09803922]\n"," [0.89019608]\n"," [0.37647059]\n"," [0.04705882]\n"," [0.25490196]\n"," [0.83137255]\n"," [0.40784314]\n"," [0.79607843]\n"," [0.45882353]\n"," [0.42352941]\n"," [0.58039216]]\n","y:  [1.]\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=3)"]},"metadata":{},"execution_count":16}],"source":["# Test for correct output.\n","\n","x, y = extract(X_train, Y_train, 0)\n","print (\"x: \", x)\n","print (\"y: \", y)\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(x[17], 3))\n","  [0.651]\n","  >>> print(np.round(x[50], 3))\n","  [0.31]\n","  >>> print(np.round(y, 1))\n","  [1.]\n","\"\"\"\n","\n","doctest.testmod()\n"]},{"cell_type":"markdown","metadata":{"id":"dIhPFY7QOUVk"},"source":["### Forward Propagation\n","\n","**Step 1-3** :\n","Now let’s compute the forward propagation steps. Let’s begin by defining a helper functions to compute activation. We’ll be using the sigmoid activation function for this model. Define a function which return the sigmoid of the input. You can see the formula for the sigmoid function on the slide.\n","\n","Refer back to the pseudocode guide to remind yourself of where this feedforward function should go. We will be performing the feedforward step for a single example. That means we need to calculate the activation for that single example, which is a column vector of features we have extracted. First, calculate z by multiplying each weight by its associated feature in the example, and then adding the bias. An efficient way to do this is by calculating the dot product of the weights vector and the example feature vector, utilizing numpy (in a preview of the vectorization methods we will use later!).  The dot product of two vectors [a, b, c] and [x, y, z] is defined as (ax + by + cz). Notice that this is exactly what we want to achieve. Numpy’s np.dot() function computes matrix multiplication for 2D arrays. Right now, we have two 2D arrays, both as column vectors: w, of shape (576, 1) and x, of shape (576, 1). How can we use matrix multiplication to return the dot product of these two vectors?\n","\n","It turns out there is nice relationship between the dot product and matrix multiplication. We can rewrite a.b as a*b.T. That is, the dot product of a and b is the same as a multiplied by b transpose. Recall that transposing a vector means to switch its dimensions (i.e. a 2x3 vector becomes a 3x2 vector). So, let’s go ahead and calculate the dot product of the weights w and the example x by passing w.T and x as arguments to np.dot. Recall from the unit on linear algebra that the order of matrix multiplication is very important. Np.dot(w.T, x) will give you a different result to np.dot(x, w.T). Think carefully about what exactly we want to achieve, and try writing out an example if you are still stuck on which order to use. Don’t forget to add the bias to the result of the dot product! Finally, pass z to sigmoid_activation to compute the activation for this example. Overall, the feedforward function should take in w, b, and x as arguments and return the activation a.\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"57sUz2sRyCCJ","executionInfo":{"status":"ok","timestamp":1691304570659,"user_tz":420,"elapsed":144,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["# Helper function for activation.\n","def sigmoid_activation(z):\n","\n","\n","  #  activation = 1/(1+e^-z)\n","  #a =  None\n","\n","  a = 1 / (1 + np.exp(-z))\n","\n","\n","  return a"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"mk_XhMFtPSGR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691304572356,"user_tz":420,"elapsed":142,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"117d17cb-14b2-44b5-aed9-aa96c9b2a40c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=3)"]},"metadata":{},"execution_count":18}],"source":["# Test for correct output.\n","\n","a = sigmoid_activation(0)\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(sigmoid_activation(0), 2))\n","  0.5\n","  >>> print(np.round(sigmoid_activation(.7), 2))\n","  0.67\n","  >>> print(np.round(sigmoid_activation(-3), 2))\n","  0.05\n","\"\"\"\n","\n","doctest.testmod()"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"N3jO2okfLXAx","executionInfo":{"status":"ok","timestamp":1691304576598,"user_tz":420,"elapsed":134,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["def feedforward(w, b, x):\n","\n","  ### EBGIN CODE HERE\n","\n","  # Forward propagation.\n","  #z = None\n","  #a = None\n","\n","  #z = np.dot(w, x) + b\n","  z = (w * x) + b\n","\n","  a = sigmoid_activation(z)\n","\n","  ### END CODE HERE\n","\n","  return a"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t4h0hK_scIhw","executionInfo":{"status":"ok","timestamp":1691304578322,"user_tz":420,"elapsed":157,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"afb2a489-0927-4847-ba2a-97a51805c1a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.5\n"]}],"source":["print(a)"]},{"cell_type":"markdown","metadata":{"id":"g1FDakN_UtIa"},"source":["### Cost Computation\n","\n","**Step 1-4** :\n","\n","Next, let’s see how to compute the cost. Because we are implementing unvectorized gradient descent, you will find that it is easier not to write a function to calculate the cost. Nonetheless, let’s see how the cost is calculated. First, notice that we have declared epsilon to be a very small value (1e-8). We need this to prevent division by zero in the cost calculation. Since we are performing binary classification, we will want to use the binary cross-entropy loss function.\n","$$J=\\sum [ylog(a) + (1-y)log(1-a)] $$\n","We then call np.squeeze on J to give the cost for broadcasting purposes – more on this later when we vectorize!\n","https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"EAr7AHbaL2db","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691304580502,"user_tz":420,"elapsed":146,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"c5424758-fd4f-4ca7-b55f-5808af4055d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["-0.17554456059597978\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=1)"]},"metadata":{},"execution_count":21}],"source":["# Initializations for test output.\n","# Do not change - for grading purposes\n","J = 0\n","a = np.array([0.839])\n","y = np.array([1.])\n","\n","# Calculate cost.\n","epsilon = 1e-8\n","\n","#J += None\n","#cost = None   #call np.squeeze on J\n","\n","J += -y * np.log(a + epsilon) - (1 - y) * np.log(1 - a + epsilon)\n","\n","cost = np.squeeze(-J)  #call np.squeeze on J\n","\n","print(cost)\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(cost, 3))\n","  -0.176\n","\"\"\"\n","\n","doctest.testmod()"]},{"cell_type":"markdown","metadata":{"id":"-Vkc0U5DVIz2"},"source":["### Derivatives and Backpropagation\n","\n","**Step 1-5** :\n","\n","Finally, let’s consider the backpropagation and weight update steps. Similar to cost computation, we are mostly going tp avoid explicit functions here, since the backpropagation steps are awkwardly divided across for loops in unvectorized descent. The one function we will define, however, is a simple one which takes in the activation a and the corresponding label y for our chosen example, and calculates the derivative of the cost with respect to z. We won’t go into the details of the derivation here, but it turns out that dz = a – y.\n","\n","We can calculate the derivative of J with respect to the ith weight dwi by calculating dwi = ith feature of x * dz.\n","\n","We’ll complete weight update within the algorithm itself.\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"KwIqCi4wMkuF","executionInfo":{"status":"ok","timestamp":1691304584009,"user_tz":420,"elapsed":155,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["# Calculate derivative of the cost with respect to z.\n","def calc_dz(a, y):\n","\n","  #dz = None\n","\n","  dz = a - y\n","\n","  return dz"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"NsBZj9nmQiUf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691304585850,"user_tz":420,"elapsed":136,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"93a336c7-8b62-4bdb-ce22-a1e6448bd931"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=1)"]},"metadata":{},"execution_count":23}],"source":["# Test output.\n","\n","dz = calc_dz(a, y)\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(dz, 3))\n","  [-0.161]\n","\"\"\"\n","\n","doctest.testmod()"]},{"cell_type":"markdown","metadata":{"id":"0NoHVtWtZNnN"},"source":["You will complete the rest of the backpropagation steps inside the descent algorithm itself. Typically, we would create functions to perform backpropagation for us; however, since we are implementing unvectorized descent, there is no clean way to do this."]},{"cell_type":"markdown","metadata":{"id":"HUbnB9DyWV28"},"source":["### Full Algorithm\n","**Step 1-6** :\n","\n","We’re now ready to put everything together. You will want to refer back to the pseudocode guide regularly. First, let’s extract the number of examples and the number of features per example from X_train. We can do this by indexing into the shape of X_train. Refer back to the output of the flatten() function to see the shape of X_train after preprocessing.. Alternatively, make a new cell and run X_train.shape to work out how to index into it. Next, let’s initialize our parameters w and b using the function we built earlier. Remember that we need to pass a variable containing the number of features in the train set to this function – luckily, we have just extracted this! Now go ahead and initialize an empty list to which we can append the cost per iteration (this will be useful to keep track of how the cost changes during training).\n","\n","We will now enter the first for loop which is for the number of iterations of descent. Inside this loop, we are going to implement one iteration of unvectorized descent. First, initialize the gradients. In code, the convention is to use dw1 for the derivative of the cost with respect to w1 (and so on). Initialize a gradients vector of zeroes using np.zeros as dw.  The ith component in this vector will be the derivative of the cost with respect to the ith weight. Remember to also initialize J, our cost accumulator, to 0.\n","\n","Now we can enter the for loop over the examples (this should iterate from 0 to the number of examples). The first thing we need to do is extract a single example and its corresponding label. We can do this by calling our extract() function. Next, calculate the activation a by calling the feedforward() function.  Then, calculate the cost for this example using the code you wrote earlier. Don’t forget to include the small epsilon value and to call np.squeeze on J. Next, begin the process of backpropagation by using your calc_dz() function to calculate dz for this example.  Now we need to calculate the gradients with respect to the weights. First, loop over the features. Inside this loop, you will want to accumulate the values of dw1,dw2, and so on. We are accumulating here because, similarly to the cost computation, we are going to repeat this process for each example and thus calculate a cumulative total for all examples. Remember that dw1 = x1*dz. You should be able to use the for loop to iteratively calculate each derivative. Lastly, remember to accumulate db – however, remember to do this outside the for loop over features, but inside the for loop over examples. This is because we only want to calculate db once per example.\n","\n","Now you will want to exit out of the for loop over examples too. The code you have written inside this loop will calculate the total cost for all examples, as well as the total gradient with respect to each parameter for all examples. That is why you must now divide each accumulator by the number of examples to get the average cost per example and average gradient with respect to each parameter. Lastly, we need to update the weights by looping over them and updating each weight with w = w – learning_rate*dw. Don’t forget to also update the bias!\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"frw7-BJGcIik","executionInfo":{"status":"ok","timestamp":1691304589780,"user_tz":420,"elapsed":131,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"70ab7a6b-5eda-473b-9098-63400bde965f"},"outputs":[{"output_type":"stream","name":"stdout","text":["(576, 80)\n"]}],"source":["print(X_train.shape)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XHqXncP3cIim","executionInfo":{"status":"ok","timestamp":1691304591084,"user_tz":420,"elapsed":141,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"382151c1-f178-44ea-e430-76703184db9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 80)\n"]}],"source":["print(Y_train.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"MPyMC31acIin","executionInfo":{"status":"ok","timestamp":1691185674031,"user_tz":420,"elapsed":17,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"13332a11-1461-421a-e7cc-f4b6a572a31e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef unvectorized_descent(X_train, Y_train, num_iterations, learning_rate):\\n\\n  # Extract number of examples and number of features.\\n  m = None\\n  num_features = None\\n\\n  # Initialize parameters.\\n  w, b = None\\n\\n  # Initialize  list of costs.\\n  costs = None\\n\\n  for iteration in range(None, None):\\n\\n    # Initialize cost and gradients.\\n    J = None\\n    dw = None\\n    db = None\\n\\n    # Iterate over examples.\\n    for None in range(None, None):\\n\\n      # Extract a single example.\\n      x, y = None\\n\\n      # Compute forward propagation for a single example.\\n      a = None\\n\\n      # Compute the cost for a single example.\\n      epsilon = None\\n      J += None\\n      cost = None\\n\\n      # Backpropagation for a single example.\\n      # Calculate derivative.\\n      dz = None\\n\\n      # Iterate over features.\\n      for None in range(None):\\n\\n        # Calculate gradient.\\n        dw[None] += None\\n\\n      # Calculate derivative of cost with respect to the bias.\\n      db += None\\n\\n    # Divide by m.\\n    J = None\\n    dw = None\\n    db = None\\n\\n    # Update parameters by looping over weights and derivatives.\\n    for None in range(None):\\n      w[None] = None\\n\\n    b = None\\n\\n\\n    # Record the costs\\n    if iteration % 10 == 0:\\n        costs.append(cost)\\n\\n    # Print the cost every 100 training iterations\\n    if iteration == 0:\\n      print (\"Cost after iteration %i: %f\" %(iteration, cost))\\n\\n    elif iteration % 10 == 0:\\n      print (\"Cost after iteration %i: %f\" %(iteration, cost))\\n\\n    # Print the cost every 100 training iterations\\n    if iteration == num_iterations:\\n      print (\"Cost after iteration %i: %f\" %(iteration, cost))\\n\\n  return w, b, costs\\n\\n  '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}],"source":["# Skeleton of the unvectorized algorithm.\n","\"\"\"\n","def unvectorized_descent(X_train, Y_train, num_iterations, learning_rate):\n","\n","  # Extract number of examples and number of features.\n","  m = None\n","  num_features = None\n","\n","  # Initialize parameters.\n","  w, b = None\n","\n","  # Initialize  list of costs.\n","  costs = None\n","\n","  for iteration in range(None, None):\n","\n","    # Initialize cost and gradients.\n","    J = None\n","    dw = None\n","    db = None\n","\n","    # Iterate over examples.\n","    for None in range(None, None):\n","\n","      # Extract a single example.\n","      x, y = None\n","\n","      # Compute forward propagation for a single example.\n","      a = None\n","\n","      # Compute the cost for a single example.\n","      epsilon = None\n","      J += None\n","      cost = None\n","\n","      # Backpropagation for a single example.\n","      # Calculate derivative.\n","      dz = None\n","\n","      # Iterate over features.\n","      for None in range(None):\n","\n","        # Calculate gradient.\n","        dw[None] += None\n","\n","      # Calculate derivative of cost with respect to the bias.\n","      db += None\n","\n","    # Divide by m.\n","    J = None\n","    dw = None\n","    db = None\n","\n","    # Update parameters by looping over weights and derivatives.\n","    for None in range(None):\n","      w[None] = None\n","\n","    b = None\n","\n","\n","    # Record the costs\n","    if iteration % 10 == 0:\n","        costs.append(cost)\n","\n","    # Print the cost every 100 training iterations\n","    if iteration == 0:\n","      print (\"Cost after iteration %i: %f\" %(iteration, cost))\n","\n","    elif iteration % 10 == 0:\n","      print (\"Cost after iteration %i: %f\" %(iteration, cost))\n","\n","    # Print the cost every 100 training iterations\n","    if iteration == num_iterations:\n","      print (\"Cost after iteration %i: %f\" %(iteration, cost))\n","\n","  return w, b, costs\n","\n","  \"\"\""]},{"cell_type":"code","source":["# Skeleton of the unvectorized algorithm.\n","\n","def unvectorized_descent(X_train, Y_train, num_iterations, learning_rate):\n","\n","  # Extract number of examples and number of features.\n","  #m = None\n","  #num_features = None\n","  m, num_features = X_train.shape[1], X_train.shape[0]\n","\n","  # Initialize parameters.\n","  w, b = initialize_parameters(num_features)\n","\n","  # Initialize  list of costs.\n","  costs = []\n","\n","  for iteration in range(0, num_iterations+1):\n","\n","    # Initialize cost and gradients.\n","    J = 0\n","    dw = np.zeros((num_features, 1))\n","    db = 0\n","\n","\n","    # Iterate over examples.\n","    for i in range(0, m):\n","\n","      # Extract a single example.\n","      x, y = extract(X_train, Y_train, 0)\n","\n","      # Compute forward propagation for a single example.\n","      a = feedforward(w, b, x)\n","\n","      # Compute the cost for a single example.\n","      epsilon = 1e-15\n","      J += -y * np.log(a + epsilon) - (1 - y) * np.log(1 - a + epsilon)\n","      cost = np.squeeze(J)\n","\n","\n","      # Backpropagation for a single example.\n","      # Calculate derivative.\n","      dz = calc_dz(a, y)\n","\n","      # Iterate over features.\n","      for j in range(num_features):\n","        # Calculate gradient.\n","        dw[j] += x[j] * dz[0]  # Adjusted here\n","\n","      # Calculate derivative of cost with respect to the bias.\n","      db += dz[0]\n","\n","    # Divide by m.\n","    J = J/m\n","    dw = dw/m\n","    db = db/m\n","\n","\n","    # Update parameters by looping over weights and derivatives.\n","    for i in range(num_features):\n","      w[i] = w[i] - learning_rate * dw[i]\n","\n","    b = b - learning_rate * db\n","\n","\n","  # Record the costs\n","    if iteration % 10 == 0:\n","        costs.append(cost[0])\n","\n","    # Print the cost every 100 training iterations\n","    if iteration % 100 == 0:\n","      print(\"Cost after iteration %i: %f\" %(iteration, cost[0]))\n","  return w, b, costs\n","\n"],"metadata":{"id":"hDFjnwQqf3Ds","executionInfo":{"status":"ok","timestamp":1691306915534,"user_tz":420,"elapsed":149,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["# Call your function.\n","\n","#w, b, costs = None\n","\n","w, b, costs = unvectorized_descent(X_train, Y_train, 1001, 0.1)\n","\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(costs[0], 3))\n","  55.748\n","  >>> print(np.round(costs[59], 3))\n","  2.667\n","  >>> print(np.round(w[17], 3))\n","  [-0.053]\n","  >>> print(np.round(b, 3))\n","  [0.014]\n","\"\"\"\n","\n","doctest.testmod()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YMzUJsw-ncAS","executionInfo":{"status":"ok","timestamp":1691307031076,"user_tz":420,"elapsed":112449,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"fec427d5-e5a0-4b68-a5e5-3e14d7e20864"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["Cost after iteration 0: 55.154924\n","Cost after iteration 100: 7.281169\n","Cost after iteration 200: 3.629862\n","Cost after iteration 300: 2.397365\n","Cost after iteration 400: 1.785042\n","Cost after iteration 500: 1.420263\n","Cost after iteration 600: 1.178574\n","Cost after iteration 700: 1.006830\n","Cost after iteration 800: 0.878577\n","Cost after iteration 900: 0.779190\n","Cost after iteration 1000: 0.699929\n","**********************************************************************\n","File \"__main__\", line 3, in __main__\n","Failed example:\n","    print(np.round(costs[0], 3))\n","Expected:\n","    55.748\n","Got:\n","    55.155\n","**********************************************************************\n","File \"__main__\", line 5, in __main__\n","Failed example:\n","    print(np.round(costs[59], 3))\n","Expected:\n","    2.667\n","Got:\n","    1.199\n","**********************************************************************\n","File \"__main__\", line 7, in __main__\n","Failed example:\n","    print(np.round(w[17], 3))\n","Expected:\n","    [-0.053]\n","Got:\n","    [2.609]\n","**********************************************************************\n","File \"__main__\", line 9, in __main__\n","Failed example:\n","    print(np.round(b, 3))\n","Expected:\n","    [0.014]\n","Got:\n","    [4.032]\n","**********************************************************************\n","1 items had failures:\n","   4 of   4 in __main__\n","***Test Failed*** 4 failures.\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=4, attempted=4)"]},"metadata":{},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"rTKWVRBm9c_J"},"source":["Now call the unvectorized_descent() function. Pass the X_train, Y_train, number of iterations (set to 1001) and a learning rate of 0.1. You should see the cost decrease as the number of iterations increase.\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"cbH57YWxcIip","executionInfo":{"status":"ok","timestamp":1691304606138,"user_tz":420,"elapsed":167,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["def unvectorized_descent2(X_train, Y_train, num_iterations, learning_rate):\n","  # Extract number of examples and number of features.\n","  m, num_features = X_train.shape[1], X_train.shape[0]\n","\n","  # Initialize parameters.\n","  w = np.zeros((num_features, 1))\n","  b = 0\n","  #w,b=intialize_parameters(num_features)\n","\n","  # Initialize list of costs.\n","  costs = []\n","\n","  for iteration in range(0, num_iterations+1):\n","\n","    # Initialize cost and gradients.\n","    J = 0\n","    dw = np.zeros((num_features, 1))\n","    db = 0\n","\n","    # Iterate over examples.\n","    for i in range(0, m):\n","\n","      # Extract a single example.\n","      x = X_train[:, i].reshape(num_features, 1)\n","      y = Y_train[:, i]\n","\n","      # Compute forward propagation for a single example.\n","      a = 1 / (1 + np.exp(-(np.dot(w.T, x) + b)))\n","\n","      # Compute the cost for a single example.\n","      epsilon = 1e-15\n","      J += -(y * np.log(a + epsilon) + (1 - y) * np.log(1 - a + epsilon))\n","      cost = J / m\n","\n","      # Backpropagation for a single example.\n","      # Calculate derivative.\n","      dz = a - y\n","\n","      # Iterate over features.\n","      for j in range(num_features):\n","        # Calculate gradient.\n","        dw[j] += x[j] * dz[0]  # Adjusted here\n","\n","      # Calculate derivative of cost with respect to the bias.\n","      db += dz[0]  # Adjusted here\n","\n","    # Divide by m.\n","    J = J / m\n","    dw = dw / m\n","    db = db / m\n","\n","    # Update parameters by looping over weights and derivatives.\n","    for i in range(num_features):\n","      w[i] = w[i] - learning_rate * dw[i]\n","\n","    b = b - learning_rate * db\n","\n","    # Record the costs\n","    if iteration % 10 == 0:\n","        costs.append(cost[0])\n","\n","    # Print the cost every 100 training iterations\n","    if iteration % 100 == 0:\n","      print(\"Cost after iteration %i: %f\" %(iteration, cost[0]))\n","\n","  return w, b, costs\n"]},{"cell_type":"code","source":["# Call your function.\n","\n","#w, b, costs = None\n","\n","w, b, costs = unvectorized_descent2(X_train, Y_train, 1001, 0.1)\n","\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(costs[0], 3))\n","  55.748\n","  >>> print(np.round(costs[59], 3))\n","  2.667\n","  >>> print(np.round(w[17], 3))\n","  [-0.053]\n","  >>> print(np.round(b, 3))\n","  [0.014]\n","\"\"\"\n","\n","doctest.testmod()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zaq_re7zmY14","executionInfo":{"status":"ok","timestamp":1691304729192,"user_tz":420,"elapsed":116574,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"274fefe3-b900-4fc1-a7c9-dc129456e609"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Cost after iteration 0: 0.693147\n","Cost after iteration 100: 0.148567\n","Cost after iteration 200: 0.088970\n","Cost after iteration 300: 0.062738\n","Cost after iteration 400: 0.048217\n","Cost after iteration 500: 0.039061\n","Cost after iteration 600: 0.032783\n","Cost after iteration 700: 0.028221\n","Cost after iteration 800: 0.024760\n","Cost after iteration 900: 0.022047\n","Cost after iteration 1000: 0.019865\n","**********************************************************************\n","File \"__main__\", line 3, in __main__\n","Failed example:\n","    print(np.round(costs[0], 3))\n","Expected:\n","    55.748\n","Got:\n","    [0.693]\n","**********************************************************************\n","File \"__main__\", line 5, in __main__\n","Failed example:\n","    print(np.round(costs[59], 3))\n","Expected:\n","    2.667\n","Got:\n","    [0.033]\n","**********************************************************************\n","File \"__main__\", line 7, in __main__\n","Failed example:\n","    print(np.round(w[17], 3))\n","Expected:\n","    [-0.053]\n","Got:\n","    [-0.041]\n","**********************************************************************\n","1 items had failures:\n","   3 of   4 in __main__\n","***Test Failed*** 3 failures.\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=3, attempted=4)"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kmb14i7scIiq","executionInfo":{"status":"ok","timestamp":1691185682705,"user_tz":420,"elapsed":282,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"a2b80d1c-9750-45d7-803b-0813aba9970b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 1.78862847e-02]\n"," [ 4.36509851e-03]\n"," [ 9.64974681e-04]\n"," [-1.86349270e-02]\n"," [-2.77388203e-03]\n"," [-3.54758979e-03]\n"," [-8.27414815e-04]\n"," [-6.27000677e-03]\n"," [-4.38181690e-04]\n"," [-4.77218030e-03]\n"," [-1.31386475e-02]\n"," [ 8.84622380e-03]\n"," [ 8.81318042e-03]\n"," [ 1.70957306e-02]\n"," [ 5.00336422e-04]\n"," [-4.04677415e-03]\n"," [-5.45359948e-03]\n"," [-1.54647732e-02]\n"," [ 9.82367434e-03]\n"," [-1.10106763e-02]\n"," [-1.18504653e-02]\n"," [-2.05649899e-03]\n"," [ 1.48614836e-02]\n"," [ 2.36716267e-03]\n"," [-1.02378514e-02]\n"," [-7.12993200e-03]\n"," [ 6.25244966e-03]\n"," [-1.60513363e-03]\n"," [-7.68836350e-03]\n"," [-2.30030722e-03]\n"," [ 7.45056266e-03]\n"," [ 1.97611078e-02]\n"," [-1.24412333e-02]\n"," [-6.26416911e-03]\n"," [-8.03766095e-03]\n"," [-2.41908317e-02]\n"," [-9.23792022e-03]\n"," [-1.02387576e-02]\n"," [ 1.12397796e-02]\n"," [-1.31914233e-03]\n"," [-1.62328545e-02]\n"," [ 6.46675452e-03]\n"," [-3.56270759e-03]\n"," [-1.74314104e-02]\n"," [-5.96649642e-03]\n"," [-5.88594380e-03]\n"," [-8.73882298e-03]\n"," [ 2.97138154e-04]\n"," [-2.24825777e-02]\n"," [-2.67761865e-03]\n"," [ 1.01318344e-02]\n"," [ 8.52797841e-03]\n"," [ 1.10818750e-02]\n"," [ 1.11939066e-02]\n"," [ 1.48754313e-02]\n"," [-1.11830068e-02]\n"," [ 8.45833407e-03]\n"," [-1.86088953e-02]\n"," [-6.02885104e-03]\n"," [-1.91447204e-02]\n"," [ 1.04814751e-02]\n"," [ 1.33373782e-02]\n"," [-1.97414679e-03]\n"," [ 1.77464503e-02]\n"," [-6.74727510e-03]\n"," [ 1.50616865e-03]\n"," [ 1.52945703e-03]\n"," [-1.06419527e-02]\n"," [ 4.37946611e-03]\n"," [ 1.93897846e-02]\n"," [-1.02493087e-02]\n"," [ 8.99338446e-03]\n"," [-1.54506852e-03]\n"," [ 1.76962730e-02]\n"," [ 4.83788348e-03]\n"," [ 6.76216400e-03]\n"," [ 6.43163281e-03]\n"," [ 2.49086707e-03]\n"," [-1.39576350e-02]\n"," [ 1.39166291e-02]\n"," [-1.37066901e-02]\n"," [ 2.38563192e-03]\n"," [ 6.14077088e-03]\n"," [-8.37912273e-03]\n"," [ 1.45063214e-03]\n"," [ 1.16788229e-02]\n"," [-2.41044701e-04]\n"," [-8.88657418e-03]\n"," [-2.91573775e-02]\n"," [-9.71840503e-03]\n"," [-5.91078738e-03]\n"," [-5.16417368e-03]\n"," [-9.59996180e-03]\n"," [ 3.77295234e-03]\n"," [-5.74708420e-03]\n"," [-1.09454334e-03]\n"," [ 6.79071600e-03]\n"," [-8.55437169e-03]\n"," [-3.00206075e-03]\n"," [ 2.15814934e-02]\n"," [ 8.74285723e-03]\n"," [-1.29353663e-02]\n"," [-7.97409382e-04]\n"," [ 5.64485518e-03]\n"," [ 1.23347104e-02]\n"," [ 1.48986395e-03]\n"," [-5.30582144e-03]\n"," [-7.30526644e-03]\n"," [ 6.45061985e-03]\n"," [ 3.13060374e-03]\n"," [-5.16647925e-03]\n"," [-1.89071666e-03]\n"," [-4.16198015e-03]\n"," [ 7.24657658e-03]\n"," [-6.89960677e-03]\n"," [ 4.86414475e-03]\n"," [ 8.51518950e-03]\n"," [ 4.86249326e-03]\n"," [-8.34239851e-03]\n"," [ 1.34499246e-02]\n"," [-6.78212679e-03]\n"," [ 4.26435074e-03]\n"," [-7.53334794e-03]\n"," [-1.74411025e-02]\n"," [ 2.25750266e-03]\n"," [ 2.87035165e-03]\n"," [-7.74409606e-04]\n"," [ 2.76068497e-03]\n"," [-6.48410888e-03]\n"," [-7.37464837e-03]\n"," [-1.68090099e-03]\n"," [ 1.90927681e-02]\n"," [ 8.14814541e-03]\n"," [-5.19991754e-03]\n"," [ 5.58713205e-03]\n"," [-4.78364660e-03]\n"," [-4.57260787e-03]\n"," [ 8.59284008e-03]\n"," [-5.25264645e-03]\n"," [-1.67563463e-02]\n"," [-9.06494701e-03]\n"," [ 8.84152062e-04]\n"," [ 1.28007821e-03]\n"," [ 1.24161652e-02]\n"," [-7.16025798e-03]\n"," [ 7.31465736e-03]\n"," [ 4.25966750e-03]\n"," [-1.49013772e-03]\n"," [ 8.35843902e-03]\n"," [ 4.92118903e-03]\n"," [-8.62308487e-03]\n"," [ 1.07168393e-02]\n"," [-1.22090192e-02]\n"," [ 5.96154331e-04]\n"," [ 2.44416199e-05]\n"," [ 4.24635721e-03]\n"," [-7.25433480e-03]\n"," [-3.49433856e-04]\n"," [-1.40620027e-03]\n"," [ 9.97088367e-03]\n"," [-7.95914693e-03]\n"," [ 7.27455292e-04]\n"," [-2.61240485e-03]\n"," [-1.29804664e-02]\n"," [ 2.67611247e-02]\n"," [-7.12190272e-04]\n"," [-1.48665807e-02]\n"," [ 1.40862696e-02]\n"," [-1.07058550e-02]\n"," [ 3.70869972e-03]\n"," [ 8.62832095e-03]\n"," [-6.48432023e-03]\n"," [-4.30890055e-03]\n"," [-5.40270264e-03]\n"," [-1.29361010e-03]\n"," [-1.62246117e-02]\n"," [-1.23563662e-02]\n"," [-1.40786442e-03]\n"," [ 1.03895212e-02]\n"," [ 6.31744177e-03]\n"," [ 1.72941743e-02]\n"," [ 6.94052272e-03]\n"," [-5.11128993e-03]\n"," [-1.22843406e-03]\n"," [-2.03039355e-02]\n"," [-9.60775110e-03]\n"," [-1.02035928e-02]\n"," [ 2.70593425e-03]\n"," [ 6.47829797e-03]\n"," [-5.60373419e-03]\n"," [-5.88501620e-03]\n"," [-1.54655582e-02]\n"," [-1.27762058e-03]\n"," [ 2.48168027e-03]\n"," [ 4.45780959e-03]\n"," [-7.82709043e-03]\n"," [ 1.98848968e-02]\n"," [ 1.19505834e-02]\n"," [-9.52375987e-04]\n"," [-5.27187779e-03]\n"," [-3.21584693e-03]\n"," [ 1.51130372e-03]\n"," [-1.86277156e-04]\n"," [ 4.83528787e-03]\n"," [ 7.68965158e-03]\n"," [ 1.36624284e-02]\n"," [ 1.14726479e-02]\n"," [-1.10229155e-03]\n"," [ 3.88250414e-03]\n"," [-3.87127181e-03]\n"," [-5.87220312e-03]\n"," [ 1.91082685e-02]\n"," [-4.59846150e-03]\n"," [ 1.99073781e-02]\n"," [-3.49035393e-03]\n"," [ 2.52825091e-03]\n"," [ 1.08940955e-02]\n"," [ 2.39220218e-04]\n"," [ 3.93125281e-03]\n"," [-2.41384800e-03]\n"," [-4.75524858e-03]\n"," [-1.65777023e-03]\n"," [-6.49717421e-03]\n"," [ 1.63138295e-02]\n"," [-1.67698603e-03]\n"," [ 1.72266920e-02]\n"," [-2.68510868e-02]\n"," [ 1.84207947e-04]\n"," [ 5.61951672e-03]\n"," [-2.93821238e-03]\n"," [ 1.09465308e-02]\n"," [ 6.39692355e-03]\n"," [-2.74600120e-03]\n"," [ 4.35009258e-03]\n"," [ 2.81187838e-02]\n"," [ 2.51995127e-03]\n"," [ 2.99502331e-03]\n"," [-4.39991311e-03]\n"," [ 1.33497041e-03]\n"," [-1.28926119e-02]\n"," [-1.98290257e-03]\n"," [ 2.45758763e-02]\n"," [ 1.06721555e-02]\n"," [ 6.41420656e-03]\n"," [ 1.10392166e-02]\n"," [ 1.88175499e-02]\n"," [ 5.93588115e-03]\n"," [ 2.07087853e-02]\n"," [ 1.06979836e-02]\n"," [ 1.66519510e-03]\n"," [ 1.71947656e-02]\n"," [-2.35921474e-02]\n"," [-5.71348962e-03]\n"," [ 2.65787047e-03]\n"," [-9.12090913e-03]\n"," [-1.56058403e-03]\n"," [-6.38790896e-03]\n"," [-6.54415212e-03]\n"," [ 2.71192633e-02]\n"," [ 6.27473889e-03]\n"," [-5.39478492e-04]\n"," [ 1.31516722e-02]\n"," [-2.37337476e-03]\n"," [ 8.85339675e-03]\n"," [ 3.50815841e-03]\n"," [ 1.62657360e-02]\n"," [-1.41987131e-02]\n"," [ 7.65721072e-03]\n"," [ 1.22249745e-03]\n"," [-1.15738338e-02]\n"," [ 1.06542018e-02]\n"," [-8.72375661e-03]\n"," [ 1.61923848e-02]\n"," [ 5.13093008e-03]\n"," [ 6.95483983e-03]\n"," [ 8.04573882e-04]\n"," [ 9.04528487e-03]\n"," [-1.86565117e-02]\n"," [ 7.47277939e-04]\n"," [-6.28250807e-03]\n"," [ 2.82660307e-03]\n"," [-4.71569067e-04]\n"," [ 6.16577883e-03]\n"," [-8.37631447e-03]\n"," [ 1.83915189e-02]\n"," [ 2.31585860e-02]\n"," [-2.08283418e-03]\n"," [-1.49731858e-04]\n"," [ 2.87557650e-03]\n"," [ 1.26408575e-02]\n"," [ 1.89690096e-02]\n"," [-1.20580279e-02]\n"," [-6.15108564e-03]\n"," [-1.06215612e-02]\n"," [-1.11278115e-02]\n"," [-1.63929625e-02]\n"," [ 3.62803486e-03]\n"," [-1.15903647e-02]\n"," [ 1.50326195e-02]\n"," [ 9.08318716e-03]\n"," [-1.02970953e-02]\n"," [-1.03020862e-02]\n"," [-6.12384418e-03]\n"," [ 1.39996550e-02]\n"," [-8.49607360e-03]\n"," [-1.49354967e-02]\n"," [-4.94238853e-04]\n"," [ 3.73389944e-03]\n"," [-6.57257485e-03]\n"," [ 1.61943081e-02]\n"," [ 2.40456780e-03]\n"," [ 4.53037787e-03]\n"," [-8.55532721e-03]\n"," [-3.97298386e-04]\n"," [-1.56518814e-03]\n"," [-2.27224411e-02]\n"," [ 2.87657229e-03]\n"," [-1.79480704e-02]\n"," [-2.87856456e-04]\n"," [-1.47394283e-02]\n"," [ 2.01965064e-02]\n"," [ 3.26262273e-03]\n"," [ 8.61357614e-03]\n"," [ 9.19008131e-03]\n"," [-1.32448870e-02]\n"," [-2.28179231e-02]\n"," [-3.29578153e-03]\n"," [ 8.97149747e-03]\n"," [ 9.10345116e-04]\n"," [ 7.85432247e-03]\n"," [ 9.36632573e-03]\n"," [-1.49239710e-02]\n"," [ 2.87683724e-03]\n"," [ 1.96647463e-02]\n"," [-5.70532295e-03]\n"," [-2.02907213e-02]\n"," [-2.31904553e-03]\n"," [-4.64668721e-03]\n"," [ 4.18381023e-03]\n"," [-8.92406335e-03]\n"," [ 9.07167730e-04]\n"," [-2.21741761e-02]\n"," [ 8.53963372e-03]\n"," [ 1.58686654e-02]\n"," [ 1.29787761e-02]\n"," [-1.51521566e-02]\n"," [ 3.19196176e-03]\n"," [-2.98397020e-02]\n"," [ 2.83185922e-03]\n"," [-6.43601719e-04]\n"," [-9.95794244e-03]\n"," [ 3.43794518e-03]\n"," [ 1.38014655e-03]\n"," [ 9.39502854e-03]\n"," [ 1.27308524e-03]\n"," [ 2.35027013e-03]\n"," [-1.94507226e-02]\n"," [-1.15972295e-02]\n"," [-4.75899387e-03]\n"," [ 2.96844531e-03]\n"," [-6.29877896e-05]\n"," [ 1.50089036e-02]\n"," [-8.70159190e-03]\n"," [-2.39632073e-03]\n"," [ 2.55080695e-03]\n"," [-2.31497410e-03]\n"," [ 4.95580395e-03]\n"," [-5.70562824e-03]\n"," [ 1.42034231e-02]\n"," [-3.19590141e-03]\n"," [ 1.11594518e-02]\n"," [-3.03033672e-04]\n"," [ 1.49152235e-02]\n"," [-1.39811601e-02]\n"," [ 5.16797554e-03]\n"," [-4.32576376e-03]\n"," [ 2.31400761e-03]\n"," [ 1.19280517e-02]\n"," [-1.13909680e-02]\n"," [-1.32202973e-02]\n"," [-9.98366879e-03]\n"," [ 2.54387574e-03]\n"," [-1.88683693e-02]\n"," [ 9.65937344e-04]\n"," [-1.28622933e-02]\n"," [-1.14372183e-02]\n"," [-3.69177745e-03]\n"," [ 3.80597585e-03]\n"," [-6.26414503e-03]\n"," [-4.92043893e-03]\n"," [-4.18442282e-04]\n"," [-2.72735536e-03]\n"," [-2.67652137e-02]\n"," [-4.30100607e-03]\n"," [ 8.49640578e-04]\n"," [ 1.09777949e-02]\n"," [ 2.04633278e-02]\n"," [ 6.66987850e-03]\n"," [ 7.90921927e-04]\n"," [-9.64763467e-03]\n"," [ 8.90533708e-04]\n"," [ 7.78896902e-03]\n"," [ 1.26464491e-02]\n"," [-8.80511334e-03]\n"," [ 2.36405596e-03]\n"," [ 8.15604466e-03]\n"," [ 1.86081167e-02]\n"," [ 2.55590486e-03]\n"," [-5.41503716e-03]\n"," [-6.89599657e-03]\n"," [-3.57440729e-03]\n"," [-6.51920204e-03]\n"," [ 8.26535848e-03]\n"," [ 1.06930572e-02]\n"," [ 7.24856822e-03]\n"," [ 1.19218624e-02]\n"," [-4.53768538e-03]\n"," [ 3.80335059e-03]\n"," [-3.84663176e-03]\n"," [ 4.36586883e-04]\n"," [ 1.22498574e-02]\n"," [-2.97353118e-04]\n"," [-1.86480580e-02]\n"," [-2.52815988e-03]\n"," [-7.12849796e-03]\n"," [-1.50891712e-02]\n"," [-7.90365689e-03]\n"," [ 9.60624803e-03]\n"," [ 1.68091065e-02]\n"," [-4.89006042e-03]\n"," [ 1.00253584e-02]\n"," [ 1.17822210e-02]\n"," [-1.15979227e-02]\n"," [-3.93627047e-04]\n"," [-4.44601709e-04]\n"," [ 1.72385679e-03]\n"," [-1.59375081e-02]\n"," [-3.49142239e-03]\n"," [ 1.05782121e-02]\n"," [ 1.26220320e-02]\n"," [ 1.83136208e-02]\n"," [-3.37509046e-03]\n"," [ 1.86950756e-02]\n"," [ 6.65905112e-03]\n"," [-1.35920117e-02]\n"," [ 7.61609282e-03]\n"," [-3.52280034e-03]\n"," [ 5.19076264e-03]\n"," [-1.02523943e-03]\n"," [ 1.20823864e-02]\n"," [ 2.56560159e-03]\n"," [-2.82505018e-03]\n"," [ 9.64965768e-03]\n"," [ 2.56221776e-03]\n"," [-4.12956401e-03]\n"," [ 1.27727436e-02]\n"," [-4.08345242e-03]\n"," [-6.37134863e-03]\n"," [-5.39574612e-03]\n"," [-1.46547209e-02]\n"," [-5.53207169e-03]\n"," [ 1.86087769e-02]\n"," [-9.08283935e-03]\n"," [ 8.41890374e-05]\n"," [-1.10818335e-02]\n"," [-6.11353152e-03]\n"," [ 1.51869216e-02]\n"," [ 8.96355736e-03]\n"," [-6.10250350e-03]\n"," [ 6.22011608e-05]\n"," [-8.26003962e-03]\n"," [-7.84205352e-03]\n"," [-9.14822198e-03]\n"," [-8.97139777e-03]\n"," [ 3.25949273e-03]\n"," [ 5.96793167e-03]\n"," [ 4.88242493e-03]\n"," [-1.69436857e-03]\n"," [-1.35804520e-02]\n"," [-6.71104716e-04]\n"," [-9.24293742e-03]\n"," [ 8.81301133e-03]\n"," [ 5.56442945e-03]\n"," [ 7.46891531e-03]\n"," [-3.48368316e-03]\n"," [-1.81101917e-02]\n"," [ 9.57896394e-03]\n"," [ 1.22632133e-02]\n"," [-1.48331661e-02]\n"," [ 1.37916044e-03]\n"," [ 1.19325258e-02]\n"," [-1.07572355e-02]\n"," [ 1.76768278e-02]\n"," [-3.49032457e-03]\n"," [-1.07539011e-02]\n"," [ 1.37165804e-02]\n"," [ 2.94532137e-03]\n"," [-6.53138162e-03]\n"," [ 8.79787953e-03]\n"," [-1.95576713e-02]\n"," [ 1.81530638e-02]\n"," [-8.16250060e-03]\n"," [ 2.85984273e-03]\n"," [-3.98616170e-03]\n"," [ 4.36232115e-03]\n"," [ 8.87925723e-03]\n"," [-8.23886105e-03]\n"," [ 7.86362918e-03]\n"," [-2.94033248e-03]\n"," [-6.87859510e-03]\n"," [-2.24043607e-03]\n"," [-1.00460861e-02]\n"," [-7.95667573e-03]\n"," [ 9.49133840e-03]\n"," [-1.50823042e-03]\n"," [ 1.14153310e-02]\n"," [ 4.31756994e-03]\n"," [-1.02596830e-02]\n"," [-1.89787143e-02]\n"," [-6.87241818e-03]\n"," [ 2.64126489e-03]\n"," [ 8.81291991e-05]\n"," [-8.30095621e-03]\n"," [ 1.01281230e-02]\n"," [-1.74572350e-02]\n"," [-1.25864659e-02]\n"," [ 1.27295102e-03]\n"," [ 2.19376315e-02]\n"," [ 4.13934128e-03]\n"," [-1.00523966e-02]\n"," [ 5.92657140e-03]\n"," [ 6.83396574e-03]\n"," [ 9.37856928e-03]\n"," [-4.09864534e-03]\n"," [-7.99726439e-03]\n"," [ 1.18217279e-03]\n"," [ 4.15274879e-03]\n"," [ 1.59823525e-02]\n"," [ 2.08964887e-02]\n"," [-7.33069934e-03]\n"," [-5.22971627e-03]\n"," [-6.35730806e-04]\n"," [-2.49772107e-03]\n"," [ 4.85790177e-03]\n"," [ 4.85737205e-03]\n"," [ 2.57896984e-03]\n"," [ 1.34407129e-02]\n"," [-1.58510924e-03]\n"," [-1.05080794e-02]\n"," [-1.56060123e-02]\n"," [-1.02813987e-02]\n"," [-1.04425792e-02]\n"," [-1.88342383e-02]\n"," [ 6.63187770e-03]\n"," [ 1.85267835e-03]\n"," [-4.22792727e-03]\n"," [-8.89888979e-03]\n"," [-1.58094538e-02]\n"," [-9.19719597e-03]\n"," [ 1.84790906e-02]\n"," [-2.21079164e-02]\n"," [ 1.41810517e-03]\n"," [ 7.43636971e-03]\n"," [ 1.36132230e-03]\n"," [-3.16804921e-03]\n"," [ 7.68073943e-03]\n"," [-1.10332826e-02]\n"," [ 5.55544995e-03]\n"," [ 1.43069737e-02]\n"," [-1.04137439e-02]\n"," [-8.19833406e-03]\n"," [ 1.35940375e-03]\n"," [ 9.89572796e-03]\n"," [ 1.33914830e-02]\n"," [ 6.80626954e-03]\n"," [-6.69502774e-03]]\n","0\n"]}],"source":["m, num_features = X_train.shape[1], X_train.shape[0]\n","\n","w, b = initialize_parameters(num_features)\n","\n","print(w)\n","print(b)"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"bIwzcQWXcIir","executionInfo":{"status":"ok","timestamp":1691307066180,"user_tz":420,"elapsed":146,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["def unvectorized_descent(X_train, Y_train, num_iterations, learning_rate):\n","  # Extract number of examples and number of features.\n","\n","  m, num_features = X_train.shape[1], X_train.shape[0]\n","\n","  # Initialize parameters.\n","\n","  np.random.seed(3) #do not change - for grading purposes\n","\n","  #w = None\n","  #b = None\n","  w = np.random.randn(num_features, 1) * 0.01\n","  b = 0\n","\n","  # Initialize list of costs.\n","  costs = []\n","\n","  for iteration in range(0, num_iterations+1):\n","\n","    # Initialize cost and gradients.\n","    J = 0\n","\n","    dw = np.zeros((num_features, 1))\n","\n","\n","    db = 0\n","\n","    # Iterate over examples.\n","    for i in range(0, m):\n","\n","      # Extract a single example.\n","      x = X_train[:, i].reshape(num_features, 1)\n","      y = Y_train[:, i]\n","\n","      # Compute forward propagation for a single example.\n","      a = 1 / (1 + np.exp(-(np.dot(w.T, x) + b)))\n","\n","      # Compute the cost for a single example.\n","      epsilon = 1e-8\n","      J += -(y * np.log(a + epsilon) + (1 - y) * np.log(1 - a + epsilon))\n","      cost = J / m\n","\n","      # Backpropagation for a single example.\n","      # Calculate derivative.\n","      dz = a - y\n","\n","      # Iterate over features.\n","      for j in range(num_features):\n","        # Calculate gradient.\n","        dw[j] += x[j] * dz[0]  # Adjusted here\n","\n","      # Calculate derivative of cost with respect to the bias.\n","      db += dz[0]  # Adjusted here\n","\n","    # Divide by m.\n","    J = J / m\n","    dw = dw / m\n","    db = db / m\n","\n","    # Update parameters by looping over weights and derivatives.\n","    for i in range(num_features):\n","      w[i] = w[i] - learning_rate * dw[i]\n","\n","    b = b - learning_rate * db\n","\n","    # Record the costs\n","    if iteration % 10 == 0:\n","        costs.append(cost[0])\n","\n","    # Print the cost every 100 training iterations\n","    if iteration % 100 == 0:\n","      print(\"Cost after iteration %i: %f\" %(iteration, cost[0]))\n","\n","  return w, b, costs\n"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"Q_n-clo-cApV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691307179346,"user_tz":420,"elapsed":111126,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"50f07eea-f4a3-46bf-8e06-04960ad48078"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cost after iteration 0: 0.696855\n","Cost after iteration 100: 0.148962\n","Cost after iteration 200: 0.089114\n","Cost after iteration 300: 0.062813\n","Cost after iteration 400: 0.048263\n","Cost after iteration 500: 0.039092\n","Cost after iteration 600: 0.032805\n","Cost after iteration 700: 0.028238\n","Cost after iteration 800: 0.024773\n","Cost after iteration 900: 0.022058\n","Cost after iteration 1000: 0.019873\n","**********************************************************************\n","File \"__main__\", line 3, in __main__\n","Failed example:\n","    print(np.round(costs[0], 3))\n","Expected:\n","    55.748\n","Got:\n","    [0.697]\n","**********************************************************************\n","File \"__main__\", line 5, in __main__\n","Failed example:\n","    print(np.round(costs[59], 3))\n","Expected:\n","    2.667\n","Got:\n","    [0.033]\n","**********************************************************************\n","1 items had failures:\n","   2 of   4 in __main__\n","***Test Failed*** 2 failures.\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=2, attempted=4)"]},"metadata":{},"execution_count":72}],"source":["# Call your function.\n","\n","#w, b, costs = None\n","\n","w, b, costs = unvectorized_descent(X_train, Y_train, 1001, 0.1)\n","\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(np.round(costs[0], 3))\n","  55.748\n","  >>> print(np.round(costs[59], 3))\n","  2.667\n","  >>> print(np.round(w[17], 3))\n","  [-0.053]\n","  >>> print(np.round(b, 3))\n","  [0.014]\n","\"\"\"\n","\n","doctest.testmod()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B4lbfvhQcIiu","executionInfo":{"status":"ok","timestamp":1691185852634,"user_tz":420,"elapsed":87,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"32199e61-32cd-4ce9-e6a3-c2339d6bca7d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.03334324])"]},"metadata":{},"execution_count":31}],"source":["costs[59]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZZnUtYmFcIiv","executionInfo":{"status":"ok","timestamp":1691185852637,"user_tz":420,"elapsed":77,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"74d94bc2-7033-4177-ce04-11e20d0ae925"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.69685521])"]},"metadata":{},"execution_count":32}],"source":["costs[0]"]},{"cell_type":"markdown","metadata":{"id":"Jcs6AYZeOFD4"},"source":["### Unvectorized Predictions\n","\n","\n","**Step 1-7** :\n","\n","Now let’s build a function to make predictions on the dataset. First, extract the number of examples again and initialize an array of zeroes to the shape (1, number of examples). This will be our array Y_hat, which is the array of our predictions. It is this shape because we will make a single prediction (0 or 1) for each example in the set. Now, loop over the examples in the set, and utilize the extract() function again to extract a single column x. We don’t need y this time, so you can use the blank identifier “_” to “throw away” the value of y when calling extract(), since it returns both x and y. Next, call feedforward() using your optimized parameters to calculate the activation for this example. Index into Y_hat using the multidimensional indexing technique you used in extract() and set the ith element of Y_hat (corresponding to the ith element of the test set), equal to a.\n","\n","When this for loop completes, you will have an array Y_hat of shape (1, number of examples), where each element is the activation for the corresponding example. The final step is to turn these activations into predicted labels. Recall that the sigmoid function outputs a number between 0 and 1. You will need to decide how close the activation needs to be to 1 for it to count as a predicted label of 1. Since the model is perforing binary classification, you can set a threshold of 0.5, above which all activations are rounded up to 1,a nd below which they are rounded down to 0. There are many ways to do this, including an if/else block, or using np.where() for a single line implementation. Finally, return Y_hat.  You can now go ahead and call your prediction function on both X_train and X_test.\n","\n","Lastly, you’ll want to write an evaluate() function which calculates the accuracy of your predictions. The general idea is that you should compare your predictions array with the ground truth label array. There are a few ways to do this. See if you can use  vector addition with the np.mean and np.abs functions to see how many predicted labels matched the ground truth labels, and return the accuracy as a percentage.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzEyX5CZcIix"},"outputs":[],"source":["def unvectorized_prediction(w, b, X):\n","    m = X.shape[1]\n","    Y_hat = np.zeros((1, m))\n","\n","    for i in range(m):\n","        x = X[:, i].reshape(w.shape[0], 1)\n","        a = 1 / (1 + np.exp(-(np.dot(w.T, x) + b)))\n","\n","        # Set the right element of Y_hat to a\n","        # The right element can be accessed using [:, example]\n","        Y_hat[:, i] = a\n","\n","    # Change all elements to 0 or 1 using np.where()\n","    Y_hat = np.where(Y_hat > 0.5, 1, 0)\n","\n","    return Y_hat\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BvmAexw1BXwU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691185852640,"user_tz":420,"elapsed":68,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"9d103653-bde3-47a8-f37e-096c4dcd9ba9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=1)"]},"metadata":{},"execution_count":34}],"source":["# Check your prediction function is working properly.\n","\n","\n","#predictions_train = None\n","#predictions_test = None\n","\n","\n","predictions_train = unvectorized_prediction(w, b, X_train)\n","predictions_test = unvectorized_prediction(w, b, X_test)\n","\n","#print (predictions_test)\n","\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print(predictions_test)\n","  [[1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0]]\n","\"\"\"\n","\n","doctest.testmod()"]},{"cell_type":"markdown","metadata":{"id":"qu70xQV4BT-s"},"source":["### Evaluation\n","\n","**Step 1-8** :\n","\n","Finally, call the evaluate() function. You should see that your network performs extremely well on the training data, at around 100%.  Recall that a high train score but lower test score would be a sign of overfitting.  Check your model by evaluating the train and test sets.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_LkXWyZXARxe"},"outputs":[],"source":["def evaluate(w, b, Y_hat, Y):\n","\n","  #accuracy = None\n","  accuracy = np.mean(Y_hat == Y) * 100\n","  return accuracy\n","\n","  print(\"Accuracy: {} %\".format(accuracy))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcmAJFidA9Nc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691185852642,"user_tz":420,"elapsed":55,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"7280d290-9de0-4153-c1b1-5e214bd9bc2d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=1)"]},"metadata":{},"execution_count":36}],"source":["# Calculate train accuracy.\n","\n","#train_accuracy = None\n","\n","train_accuracy = evaluate(w, b, predictions_train, Y_train)\n","\n","import doctest\n","\n","\"\"\"\n","  >>> print('Train accuracy:', train_accuracy)\n","  Train accuracy: 100.0\n","\"\"\"\n","\n","doctest.testmod()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NS9SX7aYA9RH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691185852643,"user_tz":420,"elapsed":46,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"dc5d11a0-380d-47d0-8fd7-1f44d8b820d7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=1)"]},"metadata":{},"execution_count":37}],"source":["# Calculate test accuracy.\n","\n","#test_accuracy = None\n","\n","test_accuracy = evaluate(w, b, predictions_train, Y_train)\n","\n","\"\"\"\n","  >>> print('Test accuracy:', test_accuracy)\n","  Test accuracy: 100.0\n","\"\"\"\n","\n","doctest.testmod()"]},{"cell_type":"markdown","metadata":{"id":"VqPtsdOQ-8AJ"},"source":["Congratulations! You have now implemented unvectorized descent to test and classify on a synthetic dataset. As you probably realized, the algorithm was fairly complicated, despite the dataset being simplistic.\n","\n","In Part 2, we will use vectorization to make our algorithm much more efficient, allowing us to apply it to a real dataset."]}],"metadata":{"colab":{"provenance":[{"file_id":"1cBlg5vx08jxvaAPLAUdFSvPLu4VFnCg3","timestamp":1691302348877},{"file_id":"1fPsULeAui5fH7-OTPW7ot5kSdVpg1FAH","timestamp":1691185512470}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":0}