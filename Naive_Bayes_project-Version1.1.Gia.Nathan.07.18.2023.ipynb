{"cells":[{"cell_type":"markdown","metadata":{"id":"AdLjCMeS5O4P"},"source":["**Copyright: © NexStream Technical Education, LLC**.\n","All rights reserved"]},{"cell_type":"markdown","metadata":{"id":"tdXJjvNtdeKy"},"source":["#Bayes Theorem - Part 1\n","\n","Create a Colab script which calculates P(A|B) using Bayes theorem for the example provided in the lecture; that is, if you test positive how probable is it that you have cancer?\n","- Create a Python script to implement the bayes_theorem function\n","- Create a runner to call your function and output the result, P(A|B) for the following\n","Cases:\n","-  Case 1: Cancer occurs in 1% of the people your age and the test is 99% reliable\n","-  Case 2: Cancer occurs in 3% of the people your age and the test is 90% reliable\n","-  Repeat the tests 3 times each (using your previous P(A|B) result in the\n","subsequent run) and record P(A|B) for each repeated test. How did your results\n","improve with each additional test run?\n","\n","Summary of the lecture example:\n","- P(A):  probability you have cancer prior to getting tested (called the 'belief')\n","- P(B):  probability you test positive  (called the 'evidence')\n","- P(A|B):  probability you have cancer given the test is positive\n","\n","- Bayes Theorem:\n","$$P(A|B)=\\frac{P(B|A)\\cdot P(A)}{P(B)}$$\n","\n","<br>\n","\n","**def bayes_theorem(p_a, p_b_given_a):**\n","- Inputs:\n","  - *p_a* = probability of event 'a', e.g., you have cancer prior to getting tested\n","  - *p_b_given_a* = probability of event 'b' given event 'a' has occurred, e.g. the probability that you test positive given you have cancer.\n","- return *p_a_given_b* = probability of event 'a' given event 'b', e.g. the probability that you have cancer given you test positive, *p_b* (for test purposes).\n","\n","<br>\n","\n","Follow the steps as outlined in the following code cell to implement the functions.  Make sure your code passes the embedded doctests.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"CwNgUJLCtnO5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689824883172,"user_tz":420,"elapsed":281,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"outputId":"abc871f2-478f-494a-a7b4-89a40dd5a90e"},"outputs":[{"output_type":"stream","name":"stderr","text":["\n","PYDEV DEBUGGER WARNING:\n","sys.settrace() should not be used when the debugger is being used.\n","This may cause the debugger to stop working correctly.\n","If this is needed, please check: \n","http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n","to see how to restore the debug tracing back correctly.\n","Call Location:\n","  File \"/usr/lib/python3.10/doctest.py\", line 1501, in run\n","    sys.settrace(save_trace)\n","\n"]},{"output_type":"stream","name":"stdout","text":["**********************************************************************\n","File \"__main__\", line 7, in __main__\n","Failed example:\n","    print(np.round(pagb_2, 4))\n","Expected:\n","    [0.2177 0.7147 0.9575]\n","Got:\n","    [0.7357 0.996  1.    ]\n","**********************************************************************\n","File \"__main__\", line 9, in __main__\n","Failed example:\n","    print(np.round(pb_2, 4))\n","Expected:\n","    [0.124  0.2742 0.6718]\n","Got:\n","    [0.0367 0.6648 0.8965]\n","**********************************************************************\n","1 items had failures:\n","   2 of   4 in __main__\n","***Test Failed*** 2 failures.\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=2, attempted=4)"]},"metadata":{},"execution_count":1}],"source":["#In-class assignment in Bayes lecture\n","#If you test positive how probable is it that you have cancer?\n","import numpy as np\n","\n","\n","#Calculate P(A|B) given P(A), P(B|A)\n","\"\"\"def bayes_theorem(p_a, p_b_given_a):\n","  p_not_a = None\n","  p_b_given_not_a = None\n","  p_b = None\n","  p_a_given_b = None\n","\n","  return p_b, p_a_given_b\"\"\"\n","\n","\n","def bayes_theorem(p_a, p_b_given_a):\n","    # probability of not having the disease\n","    p_not_a = 1 - p_a\n","\n","    # probability of testing positive given that you don't have the disease (false positive)\n","    p_b_given_not_a = 0.01 # assuming 1% false positive rate\n","\n","    # total probability of testing positive\n","    p_b = p_b_given_a * p_a + p_b_given_not_a * p_not_a\n","\n","    # probability of having the disease given that you tested positive\n","    p_a_given_b = (p_b_given_a * p_a) / p_b\n","\n","    return p_b, p_a_given_b\n","\n","#-------------------------------------------------------------------------------------------------\n","#Test with the following doctest test vectors.\n","#DO NOT EDIT THE TEST CODE!!!!\n","\n","#Scenario and definitions\n","#   P(A):  probability that event (e.g. cancer) occurs in % of population (belief)\n","#   P(B):  probability test is positive (evidence)\n","#   P(B|A):  probability test is positive if event occurs, i.e. reliability of test\n","#            when multiply by P(A) then get the rate of true positives\n","#   P(B|not A): probability test is positive if event does not occur\n","#            when multiply by P(not A) then get the rate of false positives\n","\n","#Case 1.  Cancer occurs in 1% of the people your age and the test is 99% reliable\n","#Case 2.  Cancer occurs in 3% of the people your age and the test is 90% reliable\n","\n","\"\"\"def bayes_runner(p_a, p_b_given_a, iterations):\n","  pagb_list = []\n","  pb_list = []\n","  for i in range(iterations):\n","    p_b, p_a_given_b = bayes_theorem(p_a, p_b_given_a)\n","    p_a = p_a_given_b\n","    pagb_list.append(p_a_given_b)\n","    pb_list.append(p_b)\n","  return pb_list, pagb_list\"\"\"\n","\n","def bayes_runner(p_a, p_b_given_a, iterations):\n","    pagb_list = []\n","    pb_list = []\n","\n","    for i in range(iterations):\n","        p_b, p_a_given_b = bayes_theorem(p_a, p_b_given_a)\n","\n","        # updating prior belief with new evidence\n","        p_a = p_a_given_b\n","\n","        pagb_list.append(p_a_given_b)\n","        pb_list.append(p_b)\n","\n","    return pb_list, pagb_list\n","\n","\n","#Case 1.  Cancer occurs in 1% of the people your age and the test is 99% reliable\n","pb_1, pagb_1 = bayes_runner(0.01, 0.99, 3)\n","\n","#Case 2.  Cancer occurs in 3% of the people your age and the test is 90% reliable\n","pb_2, pagb_2 = bayes_runner(0.03, 0.90, 3)\n","\n","import doctest\n","\"\"\"\n","   >>> print(np.round(pagb_1, 4))\n","   [0.5    0.99   0.9999]\n","   >>> print(np.round(pb_1, 4))\n","   [0.0198 0.5    0.9802]\n","   >>> print(np.round(pagb_2, 4))\n","   [0.2177 0.7147 0.9575]\n","   >>> print(np.round(pb_2, 4))\n","   [0.124  0.2742 0.6718]\n","\"\"\"\n","doctest.testmod()\n"]},{"cell_type":"markdown","metadata":{"id":"c4ApW3n5qS3J"},"source":["# Naive Bayes Monomial Classifier - Part 2\n","\n","Create a Colab script which uses a Naive Bayes classifier to detect email spam messages. Note, you may NOT use any machine learning libraries (e.g. sklearn) for this section.\n","- Use the message 'subject' line (you can use the body but it may be much more complex)\n","- Create a training set of Wanted and Unwanted messages\n","- Create your 'model'\n"," - Parse the words in the emails and create conditional probabilities for the Wanted\n","and Unwanted messages\n","- Test your model with other messages not used in the training set\n","- Record and reflect on your results, i.e. evaluate the performance, and compare against\n","typical performance of the algorithm (do some searches to find the average performance\n","of a Naive Bayes based spam filter).\n","- A training dataset has been provided as materials for this assignment. You may find another, or scrape your own emails if you prefer - just describe what dataset you used or created in your reflections.\n","\n","\n","<br>\n","\n","- Follow the steps as outlined in the following code cells to implement the functions.\n","- Record your comments and reflections for this part in a text cell at the end of this section.\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TnSLWP7PrVhJ","outputId":"fd1e83d5-0dd8-4aa7-8000-c23500113ac2","executionInfo":{"status":"ok","timestamp":1689824885318,"user_tz":420,"elapsed":2158,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","/content/drive/MyDrive/Colab Notebooks\n"]}],"source":["#Mount your google drive and copy the dataset to the current working directory (!cp),\n","#or change the working directory to the folder (%cd).\n","#from google.colab import drive\n","#drive.mount('/content/drive')\n"," # Mount drive\n","from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)\n","# cd (change directory) to the folder which contains the dataset\n","# YOUR CODE HERE...\n","%cd /content/drive/MyDrive/Colab Notebooks/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"WzByC_duqUEf","outputId":"7cf7e82e-0e4c-4185-f472-fa1af42272a4","executionInfo":{"status":"ok","timestamp":1689824887073,"user_tz":420,"elapsed":1765,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Unnamed: 0 label                                               text  \\\n","0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n","1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n","2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n","3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n","4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n","\n","   label_num  \n","0          0  \n","1          0  \n","2          0  \n","3          1  \n","4          0  "],"text/html":["\n","\n","  <div id=\"df-48f44d25-ec87-4780-9747-e4fa6dc3518c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>label</th>\n","      <th>text</th>\n","      <th>label_num</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>605</td>\n","      <td>ham</td>\n","      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2349</td>\n","      <td>ham</td>\n","      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3624</td>\n","      <td>ham</td>\n","      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4685</td>\n","      <td>spam</td>\n","      <td>Subject: photoshop , windows , office . cheap ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2030</td>\n","      <td>ham</td>\n","      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48f44d25-ec87-4780-9747-e4fa6dc3518c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-95aa4635-04c5-4032-82b5-0c2acecf1e69\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-95aa4635-04c5-4032-82b5-0c2acecf1e69')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-95aa4635-04c5-4032-82b5-0c2acecf1e69 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-48f44d25-ec87-4780-9747-e4fa6dc3518c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-48f44d25-ec87-4780-9747-e4fa6dc3518c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":3}],"source":["#Read the dataset as a Pandas dataframe.\n","#and examine the head of the file.\n","#An example is shown for the provided dataset.\n","#This code assumes the dataset csv file is in the working directory.\n","\n","import pandas as pd\n","\n","df = pd.read_csv('./spam_training_dataset.csv')\n","df.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"slQzZjnqsbcO","executionInfo":{"status":"ok","timestamp":1689824887076,"user_tz":420,"elapsed":35,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a47cf5e7-d1e0-488f-e84b-c8ea83cfc376"},"outputs":[{"output_type":"stream","name":"stdout","text":["0              Subject: enron methanol ; meter # : 988291\n","1                   Subject: hpl nom for january 9 , 2001\n","2                                   Subject: neon retreat\n","3       Subject: photoshop , windows , office . cheap ...\n","4                            Subject: re : indian springs\n","                              ...                        \n","5166                        Subject: put the 10 on the ft\n","5167             Subject: 3 / 4 / 2000 and following noms\n","5168                Subject: calpine daily gas nomination\n","5169    Subject: industrial worksheets for august 2000...\n","5170              Subject: important online banking alert\n","Name: Subject, Length: 5171, dtype: object\n"]}],"source":["#Split each row on the line return '\\r'\n","#An example is shown for the provided dataset.\n","df['Subject'] = df.text.str.split('\\r').str.get(0)\n","print(df['Subject'])"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"wnGM8R3YuFHs","executionInfo":{"status":"ok","timestamp":1689824887256,"user_tz":420,"elapsed":204,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/","height":423},"outputId":"5fed0e10-7efb-42c2-9d94-4b5cb57344c6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Unnamed: 0 label                                               text  \\\n","0           3017   ham  Subject: 3 - urgent - to avoid loss of informa...   \n","1            451   ham  Subject: nominations for eastrans reciept for ...   \n","2            307   ham  Subject: change in operating companies\\r\\nplea...   \n","3           3752  spam  Subject: we deliver to your door within 24 hou...   \n","4           2612   ham  Subject: natural gas nomination for 03 / 01\\r\\...   \n","...          ...   ...                                                ...   \n","1030        1518   ham  Subject: put the 10 on the ft\\r\\nthe transport...   \n","1031         404   ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...   \n","1032        2933   ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...   \n","1033        1409   ham  Subject: industrial worksheets for august 2000...   \n","1034        4807  spam  Subject: important online banking alert\\r\\ndea...   \n","\n","      label_num                                            Subject  \n","0             0  Subject: 3 - urgent - to avoid loss of informa...  \n","1             0  Subject: nominations for eastrans reciept for ...  \n","2             0             Subject: change in operating companies  \n","3             1  Subject: we deliver to your door within 24 hou...  \n","4             0        Subject: natural gas nomination for 03 / 01  \n","...         ...                                                ...  \n","1030          0                      Subject: put the 10 on the ft  \n","1031          0           Subject: 3 / 4 / 2000 and following noms  \n","1032          0              Subject: calpine daily gas nomination  \n","1033          0  Subject: industrial worksheets for august 2000...  \n","1034          1            Subject: important online banking alert  \n","\n","[1035 rows x 5 columns]"],"text/html":["\n","\n","  <div id=\"df-9f6f66cd-4a72-4613-9b33-bc15f2444c0c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>label</th>\n","      <th>text</th>\n","      <th>label_num</th>\n","      <th>Subject</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3017</td>\n","      <td>ham</td>\n","      <td>Subject: 3 - urgent - to avoid loss of informa...</td>\n","      <td>0</td>\n","      <td>Subject: 3 - urgent - to avoid loss of informa...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>451</td>\n","      <td>ham</td>\n","      <td>Subject: nominations for eastrans reciept for ...</td>\n","      <td>0</td>\n","      <td>Subject: nominations for eastrans reciept for ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>307</td>\n","      <td>ham</td>\n","      <td>Subject: change in operating companies\\r\\nplea...</td>\n","      <td>0</td>\n","      <td>Subject: change in operating companies</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3752</td>\n","      <td>spam</td>\n","      <td>Subject: we deliver to your door within 24 hou...</td>\n","      <td>1</td>\n","      <td>Subject: we deliver to your door within 24 hou...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2612</td>\n","      <td>ham</td>\n","      <td>Subject: natural gas nomination for 03 / 01\\r\\...</td>\n","      <td>0</td>\n","      <td>Subject: natural gas nomination for 03 / 01</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1030</th>\n","      <td>1518</td>\n","      <td>ham</td>\n","      <td>Subject: put the 10 on the ft\\r\\nthe transport...</td>\n","      <td>0</td>\n","      <td>Subject: put the 10 on the ft</td>\n","    </tr>\n","    <tr>\n","      <th>1031</th>\n","      <td>404</td>\n","      <td>ham</td>\n","      <td>Subject: 3 / 4 / 2000 and following noms\\r\\nhp...</td>\n","      <td>0</td>\n","      <td>Subject: 3 / 4 / 2000 and following noms</td>\n","    </tr>\n","    <tr>\n","      <th>1032</th>\n","      <td>2933</td>\n","      <td>ham</td>\n","      <td>Subject: calpine daily gas nomination\\r\\n&gt;\\r\\n...</td>\n","      <td>0</td>\n","      <td>Subject: calpine daily gas nomination</td>\n","    </tr>\n","    <tr>\n","      <th>1033</th>\n","      <td>1409</td>\n","      <td>ham</td>\n","      <td>Subject: industrial worksheets for august 2000...</td>\n","      <td>0</td>\n","      <td>Subject: industrial worksheets for august 2000...</td>\n","    </tr>\n","    <tr>\n","      <th>1034</th>\n","      <td>4807</td>\n","      <td>spam</td>\n","      <td>Subject: important online banking alert\\r\\ndea...</td>\n","      <td>1</td>\n","      <td>Subject: important online banking alert</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1035 rows × 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f6f66cd-4a72-4613-9b33-bc15f2444c0c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-c65d7b20-fd06-4833-bd30-ef288c1d3c90\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c65d7b20-fd06-4833-bd30-ef288c1d3c90')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-c65d7b20-fd06-4833-bd30-ef288c1d3c90 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9f6f66cd-4a72-4613-9b33-bc15f2444c0c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9f6f66cd-4a72-4613-9b33-bc15f2444c0c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":5}],"source":["#Setup dataframes with 80% train and 20% test\n","#An example is shown for the provided dataset.\n","cutoff = int(len(df)*0.8)\n","train = df[:cutoff]\n","test = df[cutoff:]\n","\n","#Reset the indices of the test rows\n","test.reset_index(drop=True)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"S28NAQX_ttMG","executionInfo":{"status":"ok","timestamp":1689824888203,"user_tz":420,"elapsed":953,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c515a68b-e728-45a9-cffc-ed4a4242b630"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'Subject:': 0.00033818058843422386, 'occidental': 0.00033818058843422386, 'battleground': 0.00033818058843422386, 'meter': 0.00033818058843422386, '1485': 0.00033818058843422386, 'october': 0.00033818058843422386, '2000': 0.00033818058843422386, 'thank': 0.00033818058843422386}\n","{}\n"]}],"source":["#Train the model\n","\n","#Create empty dictionaries of ham (wanted), and spam (unwanted) messages.\n","#This will create the histogram bins needed for ham and spam messages.\n","#The dicts will set keys = email word, value = word frequency (count)\n","#ham = None\n","#spam = None\n","\n","ham = {}\n","spam = {}\n","\n","\n","\n","#Loop over length of the training set\n","\"\"\"for i in range(len(train)):\n","\n","  #Read an example (X) == row from the training set\n","  #Hint:  https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html\n","  X = None\n","\n","  #If the exammple is ham (can get this from the 'label' or 'label_num' column)\n","  #then set a reference to the ham dictionary, else set reference to spam dict.\n","  if X.label == None\n","    toAppend = None\n","  else:\n","    toAppend = None\"\"\"\n","\n","# Iterate over training set\n","for i in range(len(train)):\n","    # Extract the i-th row in the training set\n","    X = train.loc[i]\n","\n","    # Determine which dictionary to append to\n","    if X['label'] == 'ham':\n","        toAppend = ham\n","    else:\n","        toAppend = spam\n","\n","\n","\n","\n","  #Loop over the words in the example.\n","  #Hint: https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html\n","  #Hint: In this case str is the X.text field in the df\n","\"\"\"  for word in None:\n","\n","    #Check for 'stopwords' by ignoring any words with length < 4\n","    if None > 3:\n","      #Check if word is in the dictionary and increment count if so\n","      #otherwise add the word to the dictionary and set the count to 1\n","      if word in None:\n","        None\n","      else:\n","        None\"\"\"\n","\n","# Loop over words in the text\n","for word in X['text'].split():\n","        # Ignore 'stopwords' shorter than 4 characters\n","        if len(word) > 3:\n","            # Increment word count if already in dictionary, else add to dictionary\n","            if word in toAppend:\n","                toAppend[word] += 1\n","            else:\n","                toAppend[word] = 1\n","\n","\n","#Set the lengths for each of the ham and spam entries\n","#so can calculate the probability of each word in the ham and spam histograms.\n","lenHam = len(train[train.label == \"ham\"])\n","lenSpam = len(train[train.label == \"spam\"])\n","\n","#Loop over each word and frequency in the dicts and calculate their weightings.\n","#Hint:  https://docs.python.org/3/tutorial/datastructures.html#dictionaries (Looping Techniques)\n","#Ham histogram probabilities\n","\"\"\"for None, None in None\n","  ham[None] = None\n","#Spam histogram probabilities\n","for None, None in None\n","  spam[None] = None\"\"\"\n","\n","# Loop over each word and frequency in the dictionaries and calculate their weightings\n","for word, count in ham.items():\n","    ham[word] = count / lenHam\n","for word, count in spam.items():\n","    spam[word] = count / lenSpam\n","\n","\n","#Finally sort the dictionaries by values in decending order (highest weighting first)\n","#and keep the items with the top 25 weightings.\n","#Note, you should experiment with keeping different amounts of items.\n","ham = dict(sorted(ham.items(), key=lambda item: item[1], reverse=True)[:25])\n","print(ham)\n","spam = dict(sorted(spam.items(), key=lambda item: item[1], reverse=True)[:25])\n","print(spam)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ZMEkZLt34NaV","executionInfo":{"status":"ok","timestamp":1689824888205,"user_tz":420,"elapsed":9,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["#Make predictions\n","#def predict(text):\n","#Input: test message text\n","#Return: 'ham' or 'spam' label\n","\n","# Loop over words in test message\n","#   if not stopword (<4 char) then accumulate wanted or unwanted probabilities\n","# Note, for this implementation, if the word is not in the ham or spam dictionary,\n","# just ignore it, i.e. don't multiply in a 0 probability to avoid the zero frequency problem.\n","# If the wanted score > unwanted score then return 'ham', else return 'spam'\n","\n","#YOUR CODE HERE\n","\n","\"\"\"def predict(text):\n","  w_score = None\n","  u_score = None\n","  for word in None\n","    if None > 3:\n","      if word in None:\n","        w_score = None\n","      if word in None:\n","        u_score = None\n","\n","  #Compare w_score against u_score and return 'ham' or 'spam' label\n","  if None > None:\n","    return None\n","  else:\n","    return None\"\"\"\n","\n","\n","def predict(text):\n","    w_score = 1\n","    u_score = 1\n","    # Loop over words in test message\n","    for word in text.split():\n","        # Ignore 'stopwords' shorter than 4 characters\n","        if len(word) > 3:\n","            # If word in ham dictionary, multiply its probability into the wanted score\n","            if word in ham:\n","                w_score *= ham[word]\n","            # If word in spam dictionary, multiply its probability into the unwanted score\n","            if word in spam:\n","                u_score *= spam[word]\n","    # Compare wanted score against unwanted score and return label\n","    if w_score > u_score:\n","        return 'ham'\n","    else:\n","        return 'spam'\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"QvZCIvJ96Xt2","executionInfo":{"status":"ok","timestamp":1689824888641,"user_tz":420,"elapsed":444,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"89a8c4d8-f05d-46a9-e52f-bc560e0b9b1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.30917874396135264\n"]}],"source":["#Test the model\n","#Loop over the test dataset\n","#  extract an example (row) and pass the Subject field to the predict function\n","#  if the returned label matches the example label then increment the correct\n","#Calculate a percent correct\n","\n","#YOUR CODE HERE\n","\n","#correct = 0\n","\n","\"\"\"for i in range(len(test)):\n","  email = None\n","  if predict(None) == None:\n","    correct = None\"\"\"\n","\n","# Test the model\n","correct = 0\n","for i in range(len(test)):\n","    email = test.iloc[i]\n","    if predict(email.text) == email.label:\n","        correct += 1\n","\n","print(correct/len(test))"]},{"cell_type":"markdown","metadata":{"id":"c3J2GOPWYrs5"},"source":["### Complete Program for Clarity"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"dXQ_iAc-Yrs6","executionInfo":{"status":"ok","timestamp":1689824891279,"user_tz":420,"elapsed":2647,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"724cd304-8d4c-460f-aa94-8b821241dd4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.8289855072463768\n"]}],"source":["import pandas as pd\n","\n","# Read the dataset as a Pandas dataframe.\n","df2 = pd.read_csv('./spam_training_dataset.csv')\n","\n","# Split each row on the line return '\\r'\n","df2['Subject'] = df2.text.str.split('\\r').str.get(0)\n","\n","# Setup dataframes with 80% train and 20% test\n","cutoff = int(len(df2)*0.8)\n","train = df2[:cutoff]\n","test = df2[cutoff:]\n","\n","# Reset the indices of the test rows\n","test = test.reset_index(drop=True)\n","\n","# Create empty dictionaries of ham (wanted), and spam (unwanted) messages.\n","ham = {}\n","spam = {}\n","\n","# Loop over length of the training set\n","for i in range(len(train)):\n","    X = train.iloc[i] # Read an example (X) == row from the training set\n","\n","    # If the example is ham then set a reference to the ham dictionary, else set reference to spam dict.\n","    if X.label == \"ham\":\n","        toAppend = ham\n","    else:\n","        toAppend = spam\n","\n","    # Loop over the words in the example.\n","    for word in X.text.split(' '):\n","        # Check for 'stopwords' by ignoring any words with length < 4\n","        if len(word) > 3:\n","            # Check if word is in the dictionary and increment count if so\n","            # otherwise add the word to the dictionary and set the count to 1\n","            if word in toAppend:\n","                toAppend[word] += 1\n","            else:\n","                toAppend[word] = 1\n","\n","# Set the lengths for each of the ham and spam entries\n","lenHam = len(train[train.label == \"ham\"])\n","lenSpam = len(train[train.label == \"spam\"])\n","\n","# Loop over each word and frequency in the dicts and calculate their weightings.\n","# Ham histogram probabilities\n","for word, count in ham.items():\n","    ham[word] = count/lenHam\n","# Spam histogram probabilities\n","for word, count in spam.items():\n","    spam[word] = count/lenSpam\n","\n","# Finally sort the dictionaries by values in decending order (highest weighting first)\n","# and keep the items with the top 25 weightings.\n","ham = dict(sorted(ham.items(), key=lambda item: item[1], reverse=True)[:25])\n","spam = dict(sorted(spam.items(), key=lambda item: item[1], reverse=True)[:25])\n","\n","# Make predictions\n","def predict(text):\n","    w_score = 0\n","    u_score = 0\n","    for word in text.split(' '):\n","        if len(word) > 3:\n","            if word in ham:\n","                w_score += ham[word]\n","            if word in spam:\n","                u_score += spam[word]\n","\n","    # Compare w_score against u_score and return 'ham' or 'spam' label\n","    if w_score > u_score:\n","        return 'ham'\n","    else:\n","        return 'spam'\n","\n","# Test the model\n","correct = 0\n","for i in range(len(test)):\n","    email = test.iloc[i]\n","    if predict(email.text) == email.label:\n","        correct += 1\n","\n","print(correct/len(test))\n"]},{"cell_type":"markdown","metadata":{"id":"FJkhJPEGFU3k"},"source":["**Enter your model performance and reflections here.**\n",".\n",".\n",".\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"18fu0xueE5YS"},"source":["# Naive Bayes Gaussian Classifier (from scratch) - Part 3\n","\n","Create a Colab script which uses a Naive Bayes classifier to predict one of the 3 species of flowers based on input attributes from the Iris dataset.\n","Note, you may NOT use any machine learning libraries (e.g. sklearn) for this section.\n","- Create an 80/20 split of the dataset (provided)\n","- Generate the Gaussian distributions for each of your attributes in each of the classes (species) for your training data\n","- Use the test data to predict the species\n","- Calculate and record the accuracy of your classifier\n","- The Iris dataset has been provided as materials for this assignment, however for testing purposes, please run the provided cell which accesses the dataset directly and performs a train/test split on the data (this is the only use of a sklearn function in this section).\n","\n","Summary of lecture notes:\n","- Generate an initial guess (prior probabilities) for each of the classes based on the number of examples for each class, e.g.\n"," - P(Setosa) = 50/150\n"," - P(Versicolor) = 50/150\n"," - P(Virginica) = 50/150\n","Note, depending on your train/test split, the initial guess probabilities may be different\n","- Calculate the score for each of the classes, e.g.\n","\n","*SetosaScore =  P(Setosa) * g(SepalLength | Setosa) * g(SepalWidth | Setosa) * g(PetalLength | Setosa) * g(PetalWidth | Setosa)*\n","etc.\n","\n","where:\n","$$g(x)= \\frac{1}{\\sigma \\sqrt(2\\pi)}exp(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}) $$\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"muBA6nPdi_kT","executionInfo":{"status":"ok","timestamp":1689824932610,"user_tz":420,"elapsed":487,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"aa7bc728-f95b-417e-c3a6-783e29f31b9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["     sepal_length  sepal_width  petal_length  petal_width\n","91            6.1          3.0           4.6          1.4\n","135           7.7          3.0           6.1          2.3\n","69            5.6          2.5           3.9          1.1\n","128           6.4          2.8           5.6          2.1\n","114           5.8          2.8           5.1          2.4\n","..            ...          ...           ...          ...\n","133           6.3          2.8           5.1          1.5\n","137           6.4          3.1           5.5          1.8\n","72            6.3          2.5           4.9          1.5\n","140           6.7          3.1           5.6          2.4\n","37            4.9          3.1           1.5          0.1\n","\n","[120 rows x 4 columns]\n","91     Iris-versicolor\n","135     Iris-virginica\n","69     Iris-versicolor\n","128     Iris-virginica\n","114     Iris-virginica\n","            ...       \n","133     Iris-virginica\n","137     Iris-virginica\n","72     Iris-versicolor\n","140     Iris-virginica\n","37         Iris-setosa\n","Name: species, Length: 120, dtype: object\n"]}],"source":["#Load the dataset into a Pandas dataframe and split the data.\n","#The cell has been provided to set up a random seed for testing purposes.\n","#Please do NOT alter the code.\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n","df = pd.read_csv(csv_url, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n","#print(df.head())\n","\n","\n","feature_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n","for col in feature_cols:\n","  df[col] = pd.to_numeric(df[col])\n","\n","X_train, X_test, y_train, y_test = train_test_split(df[feature_cols], df['species'], test_size=0.2, random_state=1)\n","print(X_train)\n","print(y_train)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"6CJsKxG2U5Kc","executionInfo":{"status":"ok","timestamp":1689824932890,"user_tz":420,"elapsed":286,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a6cb899a-bb87-4af9-cd18-612bf87fd21e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=4)"]},"metadata":{},"execution_count":18}],"source":["#Calculate the mean and standard deviation of the training set\n","#Hint: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html\n","#Hint: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mean.html\n","\n","\n","#train_mean = None\n","#train_std = None\n","\n","X_train['species'] = y_train\n","train_mean = X_train.groupby('species').mean()\n","train_std = X_train.groupby('species').std()\n","\n","#-------------------------------------------------------------------------------------------------\n","#Test with the following doctest test vectors.\n","#DO NOT EDIT THE TEST CODE!!!!\n","\n","\n","import doctest\n","\"\"\"\n","   >>> print(np.round(train_mean['petal_length']['Iris-setosa'], 4))\n","   1.4692\n","   >>> print(np.round(train_std['petal_length']['Iris-setosa'], 4))\n","   0.1608\n","   >>> print(np.round(train_mean['sepal_width']['Iris-virginica'], 4))\n","   2.9523\n","   >>> print(np.round(train_std['sepal_width']['Iris-virginica'], 4))\n","   0.3069\n","\"\"\"\n","doctest.testmod()\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"qKAB6-feSvQB","executionInfo":{"status":"ok","timestamp":1689824932892,"user_tz":420,"elapsed":13,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}}},"outputs":[],"source":["#Prediction\n","#  def predict(example):\n","#    init best_score and best_species\n","#    loop over each of the species\n","#       set initial species_score=0\n","#       loop over the feature_cols (defined in the provided cell)\n","#           extract the mean and std for the species feature\n","#           calculate the species_score (accumulate products)\n","#       if species_score > best_score\n","#           update best_score and best_species\n","#     return best_species\n","\n","import math\n","\n","\"\"\"def predict(example):\n","  best_score = None\n","  best_species = None\n","  for species in None:\n","    species_score = None\n","    for col in None:\n","      s = None\n","      m = None\n","      species_score = None\n","    if None > None:\n","      best_species = None\n","      best_score = None\n","  return best_species\"\"\"\n","\n","def predict(example):\n","  best_score = -math.inf\n","  best_species = None\n","  for species in y_train.unique():\n","    species_score = 0\n","    for col in feature_cols:\n","      x = example[col]\n","      mean = train_mean[col][species]\n","      std = train_std[col][species]\n","      exponent = math.exp(-((x-mean)**2 / (2 * std**2 )))\n","      species_score += math.log((1 / (math.sqrt(2 * math.pi) * std)) * exponent)\n","    if species_score > best_score:\n","      best_species = species\n","      best_score = species_score\n","  return best_species\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"mjuPHOBGS5a_","executionInfo":{"status":"ok","timestamp":1689824932893,"user_tz":420,"elapsed":12,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6e2f069-70e7-474b-92a7-e2a936e11233"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=1)"]},"metadata":{},"execution_count":20}],"source":["#Test the classifier\n","\n","#-------------------------------------------------------------------------------------------------\n","#Test with the following doctest test vectors.\n","#DO NOT EDIT THE TEST CODE!!!!\n","\n","t = 0\n","for i in range(len(X_test)):\n","  test_example = X_test.iloc[i]\n","  #print(test_example)\n","  pred = predict(test_example)\n","  actual = y_test.iloc[i]\n","  if pred == actual:\n","    t+=1\n","\n","\n","import doctest\n","\"\"\"\n","   >>> print(np.round(t/len(X_test), 4))\n","   0.9667\n","\"\"\"\n","\n","doctest.testmod()\n"]},{"cell_type":"markdown","metadata":{"id":"wu1VHo_KYrtC"},"source":["### Part3: Code all in one place for clarity"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"cydjVNJ6YrtD","executionInfo":{"status":"ok","timestamp":1689824933093,"user_tz":420,"elapsed":210,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5cf4d4c2-d6dd-4983-c84a-60dee2b0f414"},"outputs":[{"output_type":"stream","name":"stdout","text":["                 sepal_length  sepal_width  petal_length  petal_width\n","species                                                              \n","Iris-setosa          4.961538     3.353846      1.469231     0.230769\n","Iris-versicolor      5.945946     2.732432      4.229730     1.305405\n","Iris-virginica       6.525000     2.952273      5.534091     2.020455\n","                 sepal_length  sepal_width  petal_length  petal_width\n","species                                                              \n","Iris-setosa          0.332150     0.366237      0.160843     0.107981\n","Iris-versicolor      0.531557     0.330869      0.468369     0.201310\n","Iris-virginica       0.628814     0.306889      0.556962     0.284138\n","0.9667\n"]},{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=5)"]},"metadata":{},"execution_count":21}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import math\n","\n","csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n","df = pd.read_csv(csv_url, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n","\n","feature_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n","for col in feature_cols:\n","  df[col] = pd.to_numeric(df[col])\n","\n","X_train, X_test, y_train, y_test = train_test_split(df[feature_cols], df['species'], test_size=0.2, random_state=1)\n","\n","#Calculate the mean and standard deviation of the training set\n","X_train['species'] = y_train\n","train_mean = X_train.groupby('species').mean()\n","train_std = X_train.groupby('species').std()\n","\n","#Print train_mean and train_std for doctest\n","print(train_mean)\n","print(train_std)\n","\n","#Prediction\n","def predict(example):\n","  best_score = -math.inf\n","  best_species = None\n","  for species in y_train.unique():\n","    species_score = 0\n","    for col in feature_cols:\n","      x = example[col]\n","      mean = train_mean[col][species]\n","      std = train_std[col][species]\n","      exponent = math.exp(-((x-mean)**2 / (2 * std**2 )))\n","      species_score += math.log((1 / (math.sqrt(2 * math.pi) * std)) * exponent)\n","    if species_score > best_score:\n","      best_species = species\n","      best_score = species_score\n","  return best_species\n","\n","#Test the classifier\n","t = 0\n","for i in range(len(X_test)):\n","  test_example = X_test.iloc[i]\n","  pred = predict(test_example)\n","  actual = y_test.iloc[i]\n","  if pred == actual:\n","    t+=1\n","\n","print(np.round(t/len(X_test), 4))\n","\n","\n","#-------------------------------------------------------------------------------------------------\n","#Test with the following doctest test vectors.\n","#DO NOT EDIT THE TEST CODE!!!!\n","\n","\n","import doctest\n","\"\"\"\n","   >>> print(np.round(train_mean['petal_length']['Iris-setosa'], 4))\n","   1.4692\n","   >>> print(np.round(train_std['petal_length']['Iris-setosa'], 4))\n","   0.1608\n","   >>> print(np.round(train_mean['sepal_width']['Iris-virginica'], 4))\n","   2.9523\n","   >>> print(np.round(train_std['sepal_width']['Iris-virginica'], 4))\n","   0.3069\n","   >>> print(np.round(t/len(X_test), 4))\n","   0.9667\n","\"\"\"\n","\n","doctest.testmod()"]},{"cell_type":"markdown","metadata":{"id":"o2pDknYtAqcY"},"source":["# Naive Bayes Gaussian Classifier (sklearn) - Part 4\n","\n","Repeat Part 3 using Scikit-learn functions:\n","\n","Hint: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Qm4iQ9PjYrtF","executionInfo":{"status":"ok","timestamp":1689824895206,"user_tz":420,"elapsed":246,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e59076f9-ed4f-4677-a551-b07699873216"},"outputs":[{"output_type":"stream","name":"stdout","text":["     sepal_length  sepal_width  petal_length  petal_width\n","91            6.1          3.0           4.6          1.4\n","135           7.7          3.0           6.1          2.3\n","69            5.6          2.5           3.9          1.1\n","128           6.4          2.8           5.6          2.1\n","114           5.8          2.8           5.1          2.4\n","..            ...          ...           ...          ...\n","133           6.3          2.8           5.1          1.5\n","137           6.4          3.1           5.5          1.8\n","72            6.3          2.5           4.9          1.5\n","140           6.7          3.1           5.6          2.4\n","37            4.9          3.1           1.5          0.1\n","\n","[120 rows x 4 columns]\n","91     Iris-versicolor\n","135     Iris-virginica\n","69     Iris-versicolor\n","128     Iris-virginica\n","114     Iris-virginica\n","            ...       \n","133     Iris-virginica\n","137     Iris-virginica\n","72     Iris-versicolor\n","140     Iris-virginica\n","37         Iris-setosa\n","Name: species, Length: 120, dtype: object\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n","df = pd.read_csv(csv_url, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n","#print(df.head())\n","\n","\n","feature_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n","for col in feature_cols:\n","  df[col] = pd.to_numeric(df[col])\n","\n","X_train, X_test, y_train, y_test = train_test_split(df[feature_cols], df['species'], test_size=0.2, random_state=1)\n","print(X_train)\n","print(y_train)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"caDHaC1MS7fj","executionInfo":{"status":"ok","timestamp":1689824895207,"user_tz":420,"elapsed":9,"user":{"displayName":"Gia Nathan","userId":"13214133616904326521"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4a248a64-80fa-4a30-cdef-cfc1312e620c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["TestResults(failed=0, attempted=1)"]},"metadata":{},"execution_count":16}],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","\n","\n","#Note, use the X_train, X_test, y_train, y_test are from the provided\n","#cell in Part 3\n","\n","#Instantiate the Naive Bayes classifier\n","\n","clf = GaussianNB()\n","\n","#Fit the model\n","\n","clf.fit(X_train, y_train)\n","\n","#Run the predictions\n","\n","y_pred = clf.predict(X_test)\n","\n","#Calculate the score\n","\n","\"\"\"clf = None\n","clf.None\n","y_pred = clf.None\n","\"\"\"\n","score = clf.score(X_test, y_test)\n","\n","\n","#-------------------------------------------------------------------------------------------------\n","#Test with the following doctest test vectors.\n","#DO NOT EDIT THE TEST CODE!!!!\n","\n","import doctest\n","\"\"\"\n","   >>> print(np.round(clf.score(X_test, y_pred), 4))\n","   1.0\n","\"\"\"\n","\n","doctest.testmod()\n"]}],"metadata":{"colab":{"collapsed_sections":["18fu0xueE5YS","o2pDknYtAqcY"],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":0}