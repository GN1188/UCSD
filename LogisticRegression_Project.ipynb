{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcEaN-n1aDaB"
   },
   "source": [
    "**Copyright: Â© NexStream Technical Education, LLC**.  \n",
    "All rights reserved "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f_nSXKqMaF_"
   },
   "source": [
    "#**Project:  Logistic Regression in Binary Classification**\n",
    "#Part 1:  Introduction   \n",
    "In this project, you will evaluate the features of a dataset to train logistic regression models.  \n",
    "Please reference the video lecture on Logistic Regression for a description of the algorithm.\n",
    "Please complete the following steps in your Colab Script.  The reference script below provides template code and hints to help with each step.  \n",
    "\n",
    "Consider the following situations in which a machine learning algorithm might be applied:\n",
    "\n",
    "**(A)**   Determining whether tumors are malignant or benign based on their features.  \n",
    "**(B)**   Determining whether emails are legitimate or spam.  \n",
    "**(C)**   Determining if online transactions are fraudulent or not.  \n",
    "\n",
    "In all three examples, we want to know if something fits some criteria or doesn't. This is an example of *binary classification*, where the output is False/True (or 0/1). \n",
    "\n",
    "$y \\in \\{0, 1\\}$\n",
    "\n",
    "What this means is that the output of our algorithm should only take on 2 distinct values, where 1 represents the \"positive class\", and 0 represents \"not positive\" or \"negative\" class. For the example of a tumor classifying algorithm above, 1 might indicate that a tumor is malignant and a 0 might indicate that it is not malignant (or benign). \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "One approach might be to use a linear regression model along with a threshold to classify whether a tumor is malignant or not. For example, a highly simplified model might use tumor size as the only feature to determine if a tumor is malignant. \n",
    "\n",
    "![alt text](https://docs.google.com/uc?export=download&id=1Vk_D3Wb9zHQPmkooMBMdZl78qrFzgMCp)\n",
    "\n",
    "The dashed line in this image represents the \"decision boundary\", or the boundary between examples the algorithm would flag as malignant and those it would not. \n",
    "\n",
    "![alt text](https://docs.google.com/uc?export=download&id=1_IV9f9W1UKw_pMksdRWx5zoUYFTKFzLx)\n",
    "\n",
    "One problem with this approach can be seen in the image above. Linear regression is highly susceptible to outliers, and so the decision boundary can be changed to an undesirable one simply by a few anomalous examples in your training data. Overall, linear regression is simply not the right tool for the job here.  Further, linear regression is typically reserved for predicting numbers, not categorical or binary classifications.\n",
    "\n",
    "Instead, we can use the logistic function in order to model our hypothesis. The logistic function is defined as \n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "This equation is referred to as the sigmoid function. This function has the the property that its range is strictly between 0 and 1, which is useful for us. The logistic function (or variants of it) have many other uses as well, with one example being populations of animal populations with limited resources undergoing logistic growth.  \n",
    "\n",
    "---\n",
    "**Cost function**\n",
    "\n",
    "Recall that for linear regression, we used the mean squared error (MSE) as the cost function, where  \n",
    "\n",
    "$$ J = \\frac{1}{m}\\sum (h_{\\theta}(x) - y)^2$$\n",
    "\n",
    "Also recall that for logistic regression we could not use the MSE since the distance calculation used in MSE resulted in +inf..-inf when we translated the sigmoid function in the log domain to the linear domain.  Thus the MSE doesn't work as well for logistic regression and instead we use what is called the log-likelihood.  Please see the lecture video for more introductory details on logistic regression and its cost function.\n",
    "\n",
    "To summarize, the cost function becomes  \n",
    "\n",
    "$$ J = \\begin{cases} \n",
    "          -\\log(h_{\\theta}(x)) & y = 1 \\\\\n",
    "          - \\log(1 - h_{\\theta}(x)) & y = 0\n",
    "       \\end{cases}\n",
    "$$  \n",
    "\n",
    "A plot of the functions for y=1 and y=0 should look something like this, where the blue graph is for a true value of y=1 and the red is for  a true value of y=0:\n",
    " \n",
    "![alt text](https://docs.google.com/uc?export=download&id=14SavBhWaeei2dybla0GMmryen5iW2E5I)\n",
    "\n",
    "\n",
    "The log function is advantageous to use because it severely penalizes the prediction if it is far off from the true value.  For example if our true case is y=1 and our predicted value is small (say 0.1) then the error (Loss) is high as shown in the blue graph.\n",
    "Alternatively, if our true case is y=0 and our predicted value is high (say 0.9) then the error (Loss) is high as shown in the red graph.\n",
    "\n",
    "\n",
    "\n",
    "The cost function can actually be simplified into a more compact (but less readable form).  Please see the lecture video for more details.\n",
    "$$ cost = -y\\log(h_{\\theta}(x)) - (1 - y)\\log(1-h_{\\theta}(x))$$\n",
    "\n",
    "Since y can only take on the values of 0 and 1, you should convince yourself that it is equivalent to the piecewise definition above. \n",
    "\n",
    "Over the entire set of training examples, the cost function is\n",
    "$$ J = \\frac{1}{m}\\sum_{i=1}^{m}[-y\\log(h_{\\theta}(x)) - (1 - y)\\log(1-h_{\\theta}(x))]$$\n",
    "\n",
    "In Part 1, you will familiarize yourself with the sigmoid and cross-entropy loss functions.  Please complete the following steps as described in the following code cell.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJ8egurvI-aF"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Step 1:  Plot the sigmoid function\n",
    "#         Hint:  see the equation in the project description notes.\n",
    "#         Take a screenshot of your code and output figure, you will need to upload it.\n",
    "# YOUR CODE HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xt7zcwoiKRjl"
   },
   "outputs": [],
   "source": [
    "#Step 2:  Plot the cost function for y=1 and y=0 on the same graph.\n",
    "#         See the equation in the text cell.\n",
    "#         For y = 1 --> (-log(h(x))) then create a numpy sequance from 0.0001 to 1 with 1000 points.\n",
    "#         For y = 0 --> (-log(1-h(x))) then create a numpy sequance from 0 to 0.9999 with 1000 points.\n",
    "#         Hint:  https://numpy.org/doc/stable/reference/generated/numpy.linspace.html \n",
    "#         Take a screenshot of your code and output figure, you will need to upload it.\n",
    "\n",
    "# YOUR CODE HERE..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIghameM6ch6"
   },
   "source": [
    "#Part 2:  Implementation of Logistic Regression\n",
    "\n",
    "In this part you will write the functions required to implement Logistic Regression.  Implement the following steps in your colab script.  Use the function application programmer interface (API) as defined below.  See the Outline below and in the template script provided in the following code cell. Also, please reference the lecture video for more details on the algorithm.\n",
    "\n",
    "- **Step 1:**  Upload the dataset to your Google drive and mount the drive.\n",
    "- **Step 2:**  Write a function which loads a CSV file and converts the values to float.\n",
    "           Input:  filename\n",
    "           Output: return the dataset \n",
    "           def load_csv_file(filename):\n",
    "\n",
    "- **Step 3:**  Write a function which normalizes the dataset.  In this step you should find the min and max of each of the independent variables (min/max of the X1 column and min/max of the X2 column), then use the equation shown in the code cell below to normalize each of the values in the column.\n",
    "           Input:  dataset\n",
    "           Output: return normalized dataset\n",
    "           def normalize_dataset(dataset):\n",
    "\n",
    "- **Step 4:** Write a function which implements the sigmoid function:\n",
    "          Input: z\n",
    "          Output:\treturn 1.0 / (1.0 + exp(-z))\n",
    "          def sigmoid(z):\n",
    "\n",
    "\n",
    "- **Step 5:** Write a function which trains the logistic regression model.  In this step you will estimate the logistic regression coefficients using stochastic gradient descent.  Note, in stochastic gradient descent we upate the prediction estimate after EACH example.\n",
    "          Input: training dataset, learning rate, number of iterations (epochs)\n",
    "          Output: Python list of coefficients\n",
    "          def logRegression_train(train, lr, numIterations):\n",
    "\n",
    "  Step 5 Hints:  \n",
    " - The following calculations are done over each sample in the training set.\n",
    " - Calc the model equation: z = b0 + b1X1 + b2X2.  \n",
    " - Apply the sigmoid function to the model equation.  \n",
    " - Calc the error.   \n",
    " - Update the coefficients: new coef = old coef + (learning rate)(error)(input).  \n",
    ">_Init b0 = 0_  \n",
    ">_loop over number of iterations_   \n",
    ">>_loop over number of training samples_  \n",
    ">>>_yhat = signmod(b0 + b1X1 + b2X2 + ... bnXn)_  \n",
    ">>>_error = training result - yhat_  \n",
    ">>>_b0 = b0 + (lr)(error)(1 - yhat)_  \n",
    ">>>_b1 = b1 + (lr)(error)(yhat)(X1)_   \n",
    ">>>_b2 = b2 + (lr)(error)(yhat)(X2)_   \n",
    ">>>_bn = bn + (lr)(error)(yhat)(Xn)_   \n",
    ">>>_append b0..bn to list_\n",
    "\n",
    "    where lr = learning rate, epochs = number of iterations.  Note, for the dataset in this part, we have 2 independent variables exam score 1, exam score 2 (X1, X2) and one dependent variable, admission result (y).     \n",
    "\n",
    "\n",
    "- **Step 6:** Write a function which tests the logistic regression model.\n",
    "         Input: Model coeffs, test dataset, learning rate, iterations (epochs)\n",
    "         Output: Python list of predictions\n",
    "         def logRegression_test(coef, test, lr, numIterations):\n",
    "\n",
    "  Step 6 Hints:  \n",
    " - The following calculations are done over each sample in the test set.\n",
    " - Calc the model equation: z = b0 + b1X1 + b2X2.  \n",
    " - Apply the sigmoid function to the model equation.  \n",
    " - Round the output of the sigmoid function to map to a binary class.\n",
    ">_loop over number of test samples_   \n",
    ">>_yhat = round(sigmoid(b0 + b1X1 + b2X2 + ... bnXn))_  \n",
    ">>_append prediction = yhat to list_   \n",
    "\n",
    "- **Step 7:**  Write a function which runs the logistic_regression algorithm and calculates the accuracy.\n",
    "            Inputs: dataset, learning rate, number of iterations (epochs)\n",
    "            Outputs: accuracy score (as %), coeffs in this ordera\n",
    "            def run_logRegression(dataset, lr, numIterations):\n",
    "\n",
    "  Main function hints:\n",
    "  - In this implementation you should use the same data in the dataset for both training and testing. (you will implement a split in the next part)\n",
    "  - Create 'train_set' from the input dataset feature & results columns (i.e. all columns)\n",
    "  - Create 'test_set' from just the feature columns (not the results column) \n",
    "  - Generate model coefficients from function logRegression_train (train_set, learningRate, iterations)         \n",
    "  - Generate prediction scores from function logRegression_test (coefs from model, test_set, learningRate, iterations)\n",
    "  - Calculate the accuracy.\n",
    "  Accuracy calc hints:\n",
    "  - Count the number of data points where y == y_hat\n",
    "  - Compute the percentage of correct/#points\n",
    "\n",
    "- **Step 8:**  Run the doctest module to verify your outputs with the test dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYUKp3EPb7Ck"
   },
   "outputs": [],
   "source": [
    "#Step 1: Upload the dataset to your Google drive and mount the drive.\n",
    "#        Hint:  Upload the dataset file file to your Google Drive.\n",
    "#        Hint:  Mount your drive and change to the folder. \n",
    "#Note if get ERROR:root:Internal Python error in the inspect module error when trying to mount drive, restart runtime\n",
    "\n",
    "# Mount drive \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)\n",
    "# cd (change directory) to the folder which contains the dataset\n",
    "# YOUR CODE HERE...\n",
    "%cd /content/drive/My\\ Drive/Colab\\YOUR_DRIVE_PATH_HERE...\n",
    "\n",
    "\n",
    "# Step 2:   Load a CSV file and convert the values to float.\n",
    "#           Input:  filename\n",
    "#           Output: return the dataset\n",
    "#           Hint: https://docs.python.org/3/library/functions.html#open\n",
    "#           Hint: https://docs.python.org/3/library/functions.html#float\n",
    "#           Hint: https://docs.python.org/3/library/stdtypes.html \n",
    "def load_csv_file(filename):\n",
    "# YOUR CODE HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqVV_aMHKvvH"
   },
   "outputs": [],
   "source": [
    "# Steps 3 - 8: \n",
    "# Logistic Regression implementation\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import exp\n",
    "\n",
    "\n",
    "#Step 3:  Normalize the dataset\n",
    "#         Input:  dataset\n",
    "#         Output: return normalized dataset\n",
    "#         Hint:  https://docs.python.org/3/library/functions.html#min\n",
    "#         Hint:  https://docs.python.org/3/library/functions.html#max \n",
    "#         Hint:  Normalize the values (x) in each column:  x_normalized = (x - column min)/(column max - column min)\n",
    "#         Hint:  Normalize each column separately (find the min/max for each column, then normalize the values in that column)\n",
    "def normalize_dataset(dataset):\n",
    "# YOUR CODE HERE...\n",
    " \n",
    "\n",
    "# Step 4: Implement sigmoid function:\n",
    "#         Input: z\n",
    "#         Output:\treturn sigmoid(z)\n",
    "def sigmoid(z):\n",
    "# YOUR CODE HERE...\n",
    "\n",
    "\n",
    "# Step 5: Train the logistic regression model\n",
    "#         Estimate the logistic regression coefficients using stochastic gradient descent\n",
    "#         Input: training dataset, learning rate, number of iterations (epochs)\n",
    "#         Output: Python list of coefficients\n",
    "#         Hints:\n",
    "#          - initialize coef list to 0 for the number of dataset training samples (= #rows in dataset)\n",
    "#          - loop over numIterations (this is the number of training epochs)\n",
    "#          - loop over number of training samples\n",
    "#          - init z = b0, then calc the following:\n",
    "#             - z = z + b1x1 + b2x2 + ... bnXn\n",
    "#             - yhat = signmoid(z) \n",
    "#             - error = result (last column from the training dataset) - yhat\n",
    "#          - update the coefficients:\n",
    "#             - b0 = b0 + lr*error*yhat*(1-yhat)\n",
    "#             - bn = bn + lr*error*yhat*(1-yhat)*xn (do this for all coeffs, b1, b2, ...)\n",
    "#          - return Python list of coefficients \n",
    "def logRegression_train(train, lr, numIterations):\n",
    "# YOUR CODE HERE...\n",
    "\n",
    "\n",
    "# Step 6: Test the logistic regression model\n",
    "#         Input: Trained model coefficients, test dataset, learning rate, number of iterations (epochs)\n",
    "#         Output: Python list of predictions\n",
    "#         Hints:\n",
    "#          - loop over independent features in each row in the dataset\n",
    "#             - initialize z to b0\n",
    "#             - loop over all samples in the feature (all row samples in the column)\n",
    "#                 - z = z + coef[i + 1] * sample[i]\n",
    "#             - calculate yhat by applying sigmoid function:  yhat = sigmoid(z)  \n",
    "#             - map yhat with threshold 0.5, i.e. map 0 if < 0.5, map to 1 of >=0.5 (use round)\n",
    "#             - append prediction to Python list\n",
    "#          - return prediction list\n",
    "def logRegression_test(coef, test, lr, numIterations):\n",
    "# YOUR CODE HERE...\n",
    "\n",
    "\n",
    "# Step 7:   Run the logistic_regression algorithm and calculate the accuracy\n",
    "#           Inputs: dataset, learning rate, number of iterations (epochs)\n",
    "#           Outputs: accuracy score (as %), coeffs in this order\n",
    "#           Main function hints:\n",
    "#            - In this implementation you should use the same data in the dataset for both training and testing.\n",
    "#              (you will implement a split in the next part)\n",
    "#            - Create 'train_set' from the input dataset feature & results columns (i.e. all columns)\n",
    "#            - Create 'test_set' from just the feature columns (not the results column) \n",
    "#            - Generate model coefficients from function logRegression_train (train_set, learningRate, iterations)         \n",
    "#            - Generate prediction scores from function logRegression_test (coefs from model, test_set, learningRate, iterations)\n",
    "#            - Calculate the accuracy.\n",
    "#           Accuracy calc hints:\n",
    "#            - Count the number of data points where y == y_hat\n",
    "#            - Compute the percentage of correct/#points\n",
    "def run_logRegression(dataset, lr, numIterations):\n",
    "# YOUR CODE HERE...\n",
    "\n",
    "  #Train the model\n",
    "  #Test the model (Note copy train data to test in this example (ie. we test with the training set))\n",
    "  #Calculate the accuracy, return the score and coeff list\n",
    "\n",
    "\n",
    "#Step 8:  Run the doctest module.  DO NOT modify any code below this line!\n",
    "\n",
    "import doctest\n",
    "filename = 'TestAdmission.csv'\n",
    "dataset = load_csv_file(filename)\n",
    "normalize_dataset(dataset)\n",
    "train_set = list(dataset)\n",
    "coef = logRegression_train(train_set, 0.1, 1000)\n",
    "test = list()\n",
    "for row in dataset:\n",
    "  row_copy = list(row[0:2])\n",
    "  test.append(row_copy)\n",
    "scores, coeff = run_logRegression(dataset, 0.1, 1000)\n",
    "\n",
    "\"\"\"\n",
    "#Step 2-3 test:  Load and prepare data\n",
    "  >>> print(dataset[0:3])\n",
    "  [[0.06542783850894099, 0.6946548757450327, 0.0], [0.0032663246723157206, 0.1947045484083302, 0.0], [0.08296784231261667, 0.6196177878273813, 0.0]]\n",
    "  >>> print(sigmoid(0.934))\n",
    "  0.7178860936679435\n",
    "  >>> print(sigmoid(0.123))\n",
    "  0.5307112905000478\n",
    "\n",
    "#Step 4 test:  Sigmoid function\n",
    "  >>> print(sigmoid(0.95))\n",
    "  0.7211151780228631\n",
    "  >>> print(sigmoid(0.05))\n",
    "  0.5124973964842103\n",
    "\n",
    "#Step 5 test:  Train the logistic regression model\n",
    "  >>> print(logRegression_train(train_set, 0.1, 1000))\n",
    "  [-8.464325024665897, 9.992512529024514, 8.978643616724892]\n",
    " \n",
    "# Step 6: Test the logistic regression model\n",
    "  >>> print(logRegression_test(coef, test, 0.1, 1000)[0:20])\n",
    "  [0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1]\n",
    "\n",
    "# Step 7:   Run the logistic_regression algorithm and calculate the accuracy\n",
    "  >>> print('Scores:', scores, 'Coeffs:', coeff)\n",
    "  Scores: [91.0] Coeffs: [-8.464325024665897, 9.992512529024514, 8.978643616724892]\n",
    "\n",
    "\"\"\"\n",
    "#Test code to set up and run your Logistic Regression implementation and output the \n",
    "#   results (scores), coefficients for the regression model (coeff), and Accuracy\n",
    "filename = 'TestAdmission.csv'\n",
    "dataset = load_csv_file(filename)\n",
    "normalize_dataset(dataset)\n",
    "learningRate = 0.1\n",
    "numIterations = 1000\n",
    "scores, coeff = run_logRegression(dataset, learningRate, numIterations)\n",
    "print('Scores: %s' % scores)\n",
    "print('Coeffs:', coeff)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "\n",
    "\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eo4h0r8_sCe"
   },
   "source": [
    "# Part 3 Display results and predict some results\n",
    "\n",
    "In this part, you will visual your model results.  Follow the steps below and outlined in the following code cell.\n",
    "\n",
    "- **Step 1:** Plot the data with the model fit equation (decision boundary).\n",
    "    - Read the TestAdmission csv file.  This file contains two columns of test scores (Exam1 and Exam2), and an admissions results column (admitted == 1, rejected ==0)\n",
    "    - Plot the data as Exam2 score (y-axis) vs. Exam1 score (x-axis) with color codes for the output.\n",
    "    - Normalize the independent variable data.\n",
    "    - Plot the normalized data as Exam2 score (y-axis) vs. Exam1 score (x-axis) with color codes for the output.\n",
    "    - Plot the decision boundary (i.e. the regression line computed in the previous steps) on the same graph.  \n",
    "      Hint:  create a numpy linspace array between 0 and 1 for the the x-axis values, then for our TestAdmission dataset with 2 independent variables, calculate: \n",
    " \n",
    " $$y = \\frac{-b0 - b1x}{b2}$$\n",
    "\n",
    "\n",
    "Your output should look something like this:\n",
    "\n",
    "![alt text](https://docs.google.com/uc?export=download&id=1p7ohV76w3Ksqtel0ryfQI4yoFwLYi_xq)  \n",
    "\n",
    "- **Step 2:** Predict the admission result for different combination of test exam scores.\n",
    "         - Prompt the user for exam 1 score and exam2 score, where each score is an integer value between 0 and 100.\n",
    "         - Normalize the test scores\n",
    "         - Apply the prediction\n",
    "         - Print the output 'Congrats you are admitted' or 'Sorry you are rejected'\n",
    "\n",
    "\n",
    "Some examples of what your output should look like (see the green 'X' which were input as exam test scores 1, and 2):\n",
    "\n",
    "![alt text](https://docs.google.com/uc?export=download&id=1eODE4iIy7eyBE7L8ZsIkxjTucHgK6I01)\n",
    "\n",
    " \n",
    "![alt text](https://docs.google.com/uc?export=download&id=1FK-Tz1kKMkOuLPwKoIJBY3TMKhrYPGZW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wairZ7wMsU3e"
   },
   "outputs": [],
   "source": [
    "# Step 1: Plot the data with the model fit equation (decision boundary).\n",
    "#         Read the TestAdmission csv file.\n",
    "#         Normalize the independent variable data.\n",
    "#         Plot the normalized data as Exam2 score (y-axis) vs. Exam1 score (x-axis) with color codes for the output.\n",
    "#         Plot the regression line computed in the previous steps on the same graph.\n",
    "#         Confirm your model equation is correct.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# YOUR CODE HERE..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttoF8LUWUX8C"
   },
   "outputs": [],
   "source": [
    "#Step 2:  Prompt the user for test exam scores 1 and 2, then \n",
    "#         calculate \"admission\" predictions based on the exam scores input to the\n",
    "#         Logistic regression model.\n",
    "\n",
    "# YOUR CODE HERE..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zoXP_LqNdP1"
   },
   "source": [
    "#Part 4: Dataset split\n",
    "\n",
    "In the previous step, we used the training set also as the test set.  In this part, you will create a train/test split to provide a more realistic application of the trained model.  Implement a random 80% train / 20% test split in your dataset and repeat Part 2.  Hint, you can do this by calculating a test split size based on an input parameter percentage then loop through the original dataset then randomly create an index into the dataset, copy the row into the test dataset and remove the row.  The remaining rows after creating the test split will form the training set\n",
    "\n",
    "- Rewrite the run_logRegression function and call it run_logRegression_split.  \n",
    "_def run_logRegression_split(dataset, lr, numIterations, testSplitPercent):_\n",
    "- Split the dataset into randomly chosen samples with 80% allocated to training and 20% allocated to test.\n",
    "- Run your function several times and note the differences from run to run.  \n",
    "- Are you results consistent with Part 2?  If not describe the differences you see and why that may be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7l55sdUuOwr5"
   },
   "outputs": [],
   "source": [
    "#Part 3:  Implement a 80/20 split on your dataset.\n",
    "#         Rewrite the run_logRegression function from Part 2, call it run_logRegression_split.\n",
    "#         Input: dataset, learning rate, number of iteration, testSplitPercent\n",
    "#         Output:  Model scores and coefficients\n",
    "#         Hint:  \n",
    "\n",
    "def run_logRegression_split(dataset, lr, numIterations, testSplitPercent):\n",
    "# YOUR CODE HERE...\n",
    "\n",
    "  #Train the model\n",
    "  #Test the model \n",
    "  #Calculate the accuracy\n",
    "\n",
    "\n",
    "# Setup and run the logistic regression algorithm\n",
    "filename = 'TestAdmission.csv'\n",
    "dataset = load_csv_file(filename)\n",
    "normalize_dataset(dataset)\n",
    "#print('normalized:', dataset)\n",
    "learningRate = 0.1\n",
    "numIterations = 1000\n",
    "scores, coeff = run_logRegression_split(dataset, learningRate, numIterations, 20)\n",
    "print('Scores: %s' % scores)\n",
    "print('Coeffs:', coeff)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
